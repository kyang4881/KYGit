{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UZBnRQFZ830"
   },
   "source": [
    "# Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install stable-baselines3\n",
    "# !pip3 install shimmy>=0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "312ce7b4",
    "outputId": "40cbc775-56e7-49c8-ce70-ada76fddb910"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from stable_baselines3 import A2C, DDPG, DQN, PPO, SAC, TD3\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from typing import Any, Dict\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import torch\n",
    "import time\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from gym.wrappers.normalize import NormalizeObservation, NormalizeReward, RunningMeanStd\n",
    "import math\n",
    "import datetime\n",
    "import copy\n",
    "from itertools import product\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "metNBhXrTMeu",
    "outputId": "38965785-d6bc-4935-df9e-e93647d38b3a"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0B9nA0iSWORg"
   },
   "outputs": [],
   "source": [
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "\n",
    "\n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "\n",
    "\n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "\n",
    "\n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "\n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 162\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.radius = np.random.randint(600,1201)\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return self._get_state()\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "\n",
    "\n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "\n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "\n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0 - time_taken\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "\n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "\n",
    "        next_state = self._get_state()\n",
    "        return reward, next_state, self.is_done, velocity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "beYmLX89VmBI"
   },
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bb5f3d6a"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.loaded_model = PPO.load(\"./Model/best_model_PPO.zip\")\n",
    "        self.rescale = True\n",
    "\n",
    "    def rescaler(self, input, low, high):\n",
    "        \"\"\"Rescale the input value to the range [-1, 1].\n",
    "        Args:\n",
    "            input (float): Input value to be rescaled.\n",
    "            low (float): Lower bound of the rescaling range.\n",
    "            high (float): Upper bound of the rescaling range.\n",
    "        Returns:\n",
    "            float: Rescaled value.\n",
    "        \"\"\"\n",
    "        return 2.0 * (input - low) / (high - low) - 1.0\n",
    "\n",
    "    def _convert_observation(self, observation):\n",
    "        \"\"\"Converts the raw observation into a rescaled dictionary of observations.\n",
    "        Args:\n",
    "            observation (list): Raw observation data.\n",
    "        Returns:\n",
    "            dict: Rescaled observation data.\n",
    "        \"\"\"\n",
    "        # Mapping for tire types\n",
    "        tire_type_mapping = {\n",
    "            \"Ultrasoft\": 0, \"Soft\": 1, \"Intermediate\": 2, \"Fullwet\": 3\n",
    "        }\n",
    "        # Mapping for weather conditions\n",
    "        weather_mapping = {\n",
    "            \"Dry\": 0, \"20% Wet\": 1, \"40% Wet\": 2, \"60% Wet\": 3, \"80% Wet\": 4, \"100% Wet\": 5\n",
    "        }\n",
    "\n",
    "        # Map observations\n",
    "        tire_type = np.array([tire_type_mapping[observation[0]]])\n",
    "        weather = np.array([weather_mapping[observation[2]]])\n",
    "\n",
    "        # Rescale observations\n",
    "        if self.rescale:\n",
    "            car_condition = np.array([self.rescaler(observation[1], low=0.0, high=1.0)])\n",
    "            radius = np.array([self.rescaler(float(observation[3]), low=600.0, high=1200.0)])\n",
    "            laps_cleared = np.array([self.rescaler(float(observation[4]), low=0.0, high=162.0)])\n",
    "        else:\n",
    "            car_condition = np.array([observation[1]])\n",
    "            radius = np.array([float(observation[3])])\n",
    "            laps_cleared = np.array([float(observation[4])])\n",
    "\n",
    "        # Return observations as a dictionary\n",
    "        return {'tire_type': tire_type,\n",
    "                'car_condition': car_condition,\n",
    "                'weather': weather,\n",
    "                'radius': radius,\n",
    "                'laps_cleared': laps_cleared\n",
    "        }\n",
    "\n",
    "    def act(self, state):\n",
    "        state = self._convert_observation(state)\n",
    "        action, _ = self.loaded_model.predict(state)\n",
    "        return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZu9gLiqQyic"
   },
   "source": [
    "# Custom Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IoIuSb0_Nj1"
   },
   "source": [
    "The custom environments come in 4 different combinations:\n",
    "1. No rescaling and no reward shaping\n",
    "2. Has rescaling but no reward shaping\n",
    "3. No rescaling but has reward shaping\n",
    "4. Has both rescaling and reward shaping\n",
    "\n",
    "Rescaling observations in Proximal Policy Optimization (PPO) is done to normalize the input data to the range [-1, 1], potentially encouraging the neural network used for policy approximation to learn more effectively across a consistent and stable input range.\n",
    "\n",
    "Reward shaping is employed to modify the immediate rewards in a reinforcement learning task, making the learning process more efficient and guiding the agent towards desirable behavior by providing intermediate rewards that encourage desired actions and discourage undesired ones.\n",
    "\n",
    "The following environments were created to assess the effectiveness of these two methods for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yFdEyH2GW1cT"
   },
   "outputs": [],
   "source": [
    "class CustomRacingEnv(gym.Env):\n",
    "    \"\"\"A custom environment that enables compatibility with the gym environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, car, rescale=False, reward_shaping=False):\n",
    "        super(CustomRacingEnv, self).__init__()\n",
    "        self.track = Track(car)\n",
    "        self.rescale = rescale\n",
    "        self.reward_shaping = reward_shaping\n",
    "        self.radius = None\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(5)  # 0-4 actions\n",
    "\n",
    "        # Define observation space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'tire_type': spaces.Discrete(4),\n",
    "            'car_condition': spaces.Box(low=np.array([-1.0]), high=np.array([1.0]), shape=(1,), dtype=np.float32),\n",
    "            'weather': spaces.Discrete(6),\n",
    "            'radius': spaces.Box(low=np.array([-1.0]), high=np.array([1.0]), shape=(1,), dtype=np.float32),\n",
    "            'laps_cleared': spaces.Box(low=np.array([-1.0]), high=np.array([1.0]), shape=(1,), dtype=np.float32)\n",
    "        })\n",
    "\n",
    "        # Mapping for tire types\n",
    "        self.tire_type_mapping = {\n",
    "            \"Ultrasoft\": 0, \"Soft\": 1, \"Intermediate\": 2, \"Fullwet\": 3\n",
    "        }\n",
    "        # Mapping for weather conditions\n",
    "        self.weather_mapping = {\n",
    "            \"Dry\": 0, \"20% Wet\": 1, \"40% Wet\": 2, \"60% Wet\": 3, \"80% Wet\": 4, \"100% Wet\": 5\n",
    "        }\n",
    "\n",
    "    def _convert_observation(self, observation):\n",
    "        \"\"\"Converts the raw observation into a rescaled dictionary of observations.\n",
    "        Args:\n",
    "            observation (list): Raw observation data.\n",
    "        Returns:\n",
    "            dict: Rescaled observation data.\n",
    "        \"\"\"\n",
    "        # Map observations\n",
    "        tire_type = np.array([self.tire_type_mapping[observation[0]]])\n",
    "        weather = np.array([self.weather_mapping[observation[2]]])\n",
    "\n",
    "        # Rescale observations\n",
    "        if self.rescale:\n",
    "            car_condition = np.array([self.rescaler(observation[1], low=0.0, high=1.0)])\n",
    "            radius = np.array([self.rescaler(float(observation[3]), low=600.0, high=1200.0)])\n",
    "            laps_cleared = np.array([self.rescaler(float(observation[4]), low=0.0, high=162.0)])\n",
    "        else:\n",
    "            car_condition = np.array([observation[1]])\n",
    "            radius = np.array([float(observation[3])])\n",
    "            laps_cleared = np.array([float(observation[4])])\n",
    "\n",
    "        # Return observations as a dictionary\n",
    "        return {'tire_type': tire_type,\n",
    "                'car_condition': car_condition,\n",
    "                'weather': weather,\n",
    "                'radius': radius,\n",
    "                'laps_cleared': laps_cleared\n",
    "        }\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute a step in the environment based on the given action.\n",
    "        Args:\n",
    "            action (int): Action to be taken.\n",
    "        Returns:\n",
    "            tuple: Tuple containing the next observation, reward, done flag, and additional information.\n",
    "        \"\"\"\n",
    "        reward, next_state, done, velocity = self.track.transition(int(action))\n",
    "\n",
    "        if self.reward_shaping:\n",
    "            weather_reward = 0\n",
    "            # Reward for tire condition management\n",
    "            car_condition_reward = 20 * next_state[1]\n",
    "            tire_change_penalty = -25 if action < 4 else 0  # Penalize for unnecessary tire changes\n",
    "\n",
    "            # [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "            # Reward for weather-dependent tire choice\n",
    "            if next_state[2] == \"Dry\" and next_state[0] in [\"Ultrasoft\", \"Soft\"]:\n",
    "                if next_state[1] > 0.8 and action == 4:\n",
    "                    weather_reward += 20  # tyres good condition\n",
    "                if next_state[1] < 0.7 and action == 4:\n",
    "                    if action == 4:\n",
    "                        weather_reward -= 20  # don't change\n",
    "                    elif action == 0:\n",
    "                        weather_reward += 20  # change to the right tyres\n",
    "                    elif action == 1:\n",
    "                        weather_reward += 10  # change to right but not best tyres\n",
    "                    else:\n",
    "                        weather_reward -= 20  # change to the wrong tyres\n",
    "\n",
    "            if next_state[2] in [\"60% Wet\", \"80% Wet\", \"100% Wet\"] and next_state[0] == \"Fullwet\":\n",
    "                if next_state[1] > 0.8 and action == 4:\n",
    "                    weather_reward += 20  # tyres good condition\n",
    "                if next_state[1] < 0.7 and action == 4:  # worn out tyres\n",
    "                    if action == 4:\n",
    "                        weather_reward -= 20  # don't change\n",
    "                    elif action == 3:\n",
    "                        weather_reward += 20  # change to the right tyres\n",
    "                    elif action == 2:\n",
    "                        weather_reward -= 10  # change to wrong but not wrost tyres\n",
    "                    else:\n",
    "                        weather_reward -= 20  # change to the wrong tyres\n",
    "\n",
    "            if next_state[2] in [\"20% Wet\", \"40% Wet\"] and next_state[0] == \"Intermediate\":\n",
    "                if next_state[1] > 0.8 and action == 4:\n",
    "                    weather_reward += 20  # tyres good condition\n",
    "                if next_state[1] < 0.7: # worn out tyres\n",
    "                    if action == 4:\n",
    "                        weather_reward -= 20  # don't change\n",
    "                    elif action == 2:\n",
    "                        weather_reward += 20  # change to the right tyres\n",
    "                    else:\n",
    "                        weather_reward -= 20  # change to the wrong tyres\n",
    "\n",
    "            # Reward for lap time reduction\n",
    "            lap_time_reward = 100 if next_state[4] == 161.75 and action == 4 else -100\n",
    "\n",
    "            # Combine rewards\n",
    "            reward += car_condition_reward + tire_change_penalty + weather_reward + lap_time_reward\n",
    "\n",
    "        obs = self._convert_observation(next_state)\n",
    "\n",
    "        return obs, reward, done, {'velocity': velocity}\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Reset the environment to its initial state.\n",
    "        Returns:\n",
    "            dict: modified observations.\n",
    "        \"\"\"\n",
    "        state = self.track.reset()\n",
    "\n",
    "        if self.radius != None:\n",
    "            self.track.radius = self.radius\n",
    "            state[3] = self.radius\n",
    "\n",
    "        obs = self._convert_observation(state)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def rescaler(self, input, low, high):\n",
    "        \"\"\"Rescale the input value to the range [-1, 1].\n",
    "        Args:\n",
    "            input (float): Input value to be rescaled.\n",
    "            low (float): Lower bound of the rescaling range.\n",
    "            high (float): Upper bound of the rescaling range.\n",
    "        Returns:\n",
    "            float: Rescaled value.\n",
    "        \"\"\"\n",
    "        return 2.0 * (input - low) / (high - low) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SlauAIirmYkj"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, env, iterations=10):\n",
    "    \"\"\"\n",
    "    Evaluate a trained PPO model on an environment with different radii.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): The path to the saved PPO model.\n",
    "        env (gym.Env): The wrapped environment.\n",
    "        iterations (int): The number of iterations to run the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing max, min, and average returns for different radii.\n",
    "    \"\"\"\n",
    "    model = PPO.load(model_path + \"best_model.zip\")\n",
    "    all_g = {\"max\": [], \"min\": [], \"avg\":[]}\n",
    "    print(f'Averaged over {iterations} iterations')\n",
    "    for radius in [600, 700, 800, 900, 1000, 1100, 1200]:\n",
    "        G_list = []\n",
    "        for iteration in range(iterations):\n",
    "            env.radius = radius\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            G = 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(state)\n",
    "                next_state, reward, done, velocity = env.step(action)\n",
    "                # added velocity for sanity check\n",
    "                state = copy.deepcopy(next_state)\n",
    "                G += reward\n",
    "            G_list.append(G)\n",
    "        print(f\"\\nRadius: {radius}, min G: {np.min(G_list)}\")\n",
    "        print(f\"Radius: {radius}, avg G: {np.mean(G_list)}\")\n",
    "        print(f\"Radius: {radius}, max G: {np.max(G_list)}\\n\")\n",
    "        all_g[\"max\"].append(np.max(G_list))\n",
    "        all_g[\"min\"].append(np.min(G_list))\n",
    "        all_g[\"avg\"].append(np.mean(G_list))\n",
    "    return all_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k-2pF1cunp2"
   },
   "source": [
    "### Instantiate Class Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnGC1clwuO2v",
    "outputId": "c306ee8d-d176-4857-932d-745edfa7effe"
   },
   "outputs": [],
   "source": [
    "CustomRaceTrack_1 = CustomRacingEnv(Car(), rescale=False, reward_shaping=False)\n",
    "CustomRaceTrack_2 = CustomRacingEnv(Car(), rescale=True, reward_shaping=False)\n",
    "CustomRaceTrack_3 = CustomRacingEnv(Car(), rescale=False, reward_shaping=True)\n",
    "CustomRaceTrack_4 = CustomRacingEnv(Car(), rescale=True, reward_shaping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI2LlZGOZ9Q5"
   },
   "source": [
    "# Test Case 1. Rescaling [No], Reward Shaping [No]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D8hQg__wJYao",
    "outputId": "f36308c5-9e9a-4d05-a4b1-2bdb7c60ceca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-20080.67 +/- 11.41\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-20093.85 +/- 18.61\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-72675.49 +/- 163.52\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-72546.51 +/- 191.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-20076.64 +/- 68.85\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=6000, episode_reward=-20081.48 +/- 82.88\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-72687.99 +/- 79.39\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-72543.41 +/- 156.63\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-19966.20 +/- 60.74\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-19988.01 +/- 59.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-19984.29 +/- 50.95\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-19999.83 +/- 95.38\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-50788.99 +/- 698.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-51579.39 +/- 427.83\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-20011.22 +/- 38.57\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-20074.17 +/- 55.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-20040.62 +/- 57.39\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-20106.59 +/- 52.10\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-20087.03 +/- 63.75\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-20052.81 +/- 73.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-20086.46 +/- 27.72\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-20063.07 +/- 63.05\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-20092.30 +/- 59.34\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-20108.62 +/- 44.02\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-20013.38 +/- 51.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-20073.46 +/- 20.56\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-20097.09 +/- 21.98\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-20102.83 +/- 20.11\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-20104.15 +/- 22.26\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-20085.25 +/- 15.49\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-20069.06 +/- 19.12\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-20080.09 +/- 19.32\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-20072.95 +/- 15.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-20086.81 +/- 11.64\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-20098.64 +/- 15.82\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-20087.77 +/- 15.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-20099.28 +/- 9.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-20108.97 +/- 22.40\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-20089.10 +/- 11.05\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-20094.31 +/- 13.13\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-20075.89 +/- 17.93\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-20097.83 +/- 17.18\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-20108.68 +/- 15.84\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-20078.22 +/- 20.38\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-20083.54 +/- 20.19\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-20101.90 +/- 9.79\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-20110.97 +/- 14.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-20084.62 +/- 17.23\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-20098.77 +/- 28.78\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-20095.85 +/- 15.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-20101.82 +/- 17.03\n",
      "Episode length: 1296.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x79daca1fe8c0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustomRaceTrack_1.radius = 1200\n",
    "model_file_path_1 = './logs/CustomRacingEnv_1/'\n",
    "\n",
    "model_1 = PPO(\"MultiInputPolicy\", CustomRaceTrack_1)\n",
    "\n",
    "model_1.learn(\n",
    "    total_timesteps=50000,\n",
    "    callback=EvalCallback(\n",
    "        CustomRaceTrack_1, best_model_save_path=model_file_path_1, eval_freq=1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_KmtJ2kZmgvO",
    "outputId": "72521c57-e207-407b-ac81-419d6fd39a68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11963.81748928114\n",
      "Radius: 600, avg G: -11891.476429997958\n",
      "Radius: 600, max G: -11811.492733844183\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13449.925552790352\n",
      "Radius: 700, avg G: -13297.575791065066\n",
      "Radius: 700, max G: -13237.30634584136\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14902.593394532098\n",
      "Radius: 800, avg G: -14700.372646844908\n",
      "Radius: 800, max G: -14580.218147253308\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16172.20631820828\n",
      "Radius: 900, avg G: -16061.3591291005\n",
      "Radius: 900, max G: -15996.498350638232\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17636.171344824288\n",
      "Radius: 1000, avg G: -17415.272037234565\n",
      "Radius: 1000, max G: -17330.76017844753\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18810.947563297646\n",
      "Radius: 1100, avg G: -18720.533726801103\n",
      "Radius: 1100, max G: -18651.07862762241\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20181.869944387116\n",
      "Radius: 1200, avg G: -20063.483338782913\n",
      "Radius: 1200, max G: -19956.202460979952\n"
     ]
    }
   ],
   "source": [
    "result_case1 = evaluate_model(model_path=model_file_path_1, env=CustomRaceTrack_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI6qY3UsexJd"
   },
   "source": [
    "# Test Case 2. Rescaling [Yes], Reward Shaping [No]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFt38qgjZpei",
    "outputId": "f42f9276-dc1a-456a-ccbe-0fb54ab203c6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-20395.99 +/- 207.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-20933.73 +/- 461.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-20110.95 +/- 56.07\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-20085.72 +/- 13.26\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-58975.19 +/- 5726.14\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-59740.62 +/- 6704.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-49680.67 +/- 5118.48\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-52513.77 +/- 7374.10\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-20163.54 +/- 114.46\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-20133.84 +/- 126.59\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-19977.64 +/- 67.75\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=12000, episode_reward=-20039.50 +/- 81.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-19997.08 +/- 61.41\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-20034.24 +/- 53.56\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-20007.20 +/- 81.16\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-19924.04 +/- 67.27\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=17000, episode_reward=-20023.87 +/- 89.04\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-19992.88 +/- 63.97\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-19952.35 +/- 76.80\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-20024.43 +/- 53.11\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-27103.07 +/- 3335.87\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-26397.54 +/- 4409.14\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-22998.64 +/- 1415.37\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-23760.27 +/- 1792.40\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-34565.78 +/- 7964.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-31436.28 +/- 3923.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-25414.51 +/- 1185.54\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-25081.88 +/- 1677.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-24849.05 +/- 1488.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-27010.34 +/- 3638.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-53550.49 +/- 3294.62\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-57814.79 +/- 6921.81\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-72727.14 +/- 107.62\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-72592.78 +/- 171.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-72619.47 +/- 62.75\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-72593.74 +/- 159.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-72608.18 +/- 163.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-72659.72 +/- 147.03\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-72655.95 +/- 162.61\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-72639.16 +/- 173.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-40699.56 +/- 5013.29\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-36805.39 +/- 3748.28\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-45974.20 +/- 8289.81\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-33512.98 +/- 5659.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-31154.66 +/- 4992.73\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-43337.67 +/- 4089.83\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-40761.19 +/- 6451.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-35236.60 +/- 4080.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-38898.96 +/- 3503.32\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-44623.53 +/- 4578.67\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-41852.90 +/- 6781.86\n",
      "Episode length: 1296.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x79daca1ba650>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustomRaceTrack_2.radius = 1200\n",
    "model_file_path_2 = './logs/CustomRacingEnv_2/'\n",
    "\n",
    "model_2 = PPO(\"MultiInputPolicy\", CustomRaceTrack_2)\n",
    "\n",
    "model_2.learn(\n",
    "    total_timesteps=50000,\n",
    "    callback=EvalCallback(\n",
    "        CustomRaceTrack_2, best_model_save_path=model_file_path_2, eval_freq=1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cWc9qXB7m2jG",
    "outputId": "48bb5455-ed5a-41f5-fa6c-659cd39b57ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11931.001451042053\n",
      "Radius: 600, avg G: -11879.16081605907\n",
      "Radius: 600, max G: -11828.23776441517\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13342.286353110281\n",
      "Radius: 700, avg G: -13268.83763748121\n",
      "Radius: 700, max G: -13173.814755173413\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14782.385484078508\n",
      "Radius: 800, avg G: -14681.728017979734\n",
      "Radius: 800, max G: -14584.183326894998\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16105.863005305766\n",
      "Radius: 900, avg G: -16031.439685750685\n",
      "Radius: 900, max G: -15963.033255743825\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17468.229559723135\n",
      "Radius: 1000, avg G: -17369.302391713525\n",
      "Radius: 1000, max G: -17272.291766689268\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18761.586480290076\n",
      "Radius: 1100, avg G: -18694.947290085714\n",
      "Radius: 1100, max G: -18617.57671664353\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20148.917024107584\n",
      "Radius: 1200, avg G: -20026.206924532475\n",
      "Radius: 1200, max G: -19941.712809252105\n"
     ]
    }
   ],
   "source": [
    "result_case2 = evaluate_model(model_path=model_file_path_2, env=CustomRaceTrack_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7l3aRq_e6Si"
   },
   "source": [
    "# Test Case 3. Rescaling [No], Reward Shaping [Yes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTweVq_mewnm",
    "outputId": "62edbe96-b1b9-46d6-90ad-bedf1fa7be50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-162086.31 +/- 448.65\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-162368.16 +/- 805.34\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-208876.47 +/- 3543.35\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-205642.52 +/- 2897.49\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-212396.24 +/- 4088.48\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-210171.94 +/- 2125.46\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-206978.85 +/- 4842.63\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-205986.63 +/- 3032.89\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-209385.13 +/- 4049.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-210179.00 +/- 3317.07\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-208510.76 +/- 1749.29\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-209685.71 +/- 3512.89\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-211841.09 +/- 5900.13\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-208691.66 +/- 5563.65\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-209200.71 +/- 3889.09\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-208914.24 +/- 4413.06\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-206878.56 +/- 3661.12\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-214297.79 +/- 2666.32\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-208133.48 +/- 3660.36\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-207778.25 +/- 3010.39\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-209478.80 +/- 4115.37\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-208887.35 +/- 2557.41\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-205128.57 +/- 2888.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-204959.82 +/- 2420.05\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-210595.06 +/- 5554.77\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-210942.94 +/- 1875.42\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-207960.17 +/- 4971.61\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-206168.33 +/- 3207.77\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-209157.92 +/- 3467.56\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-210782.87 +/- 5611.72\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-211424.17 +/- 4922.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-206234.30 +/- 3836.97\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-206034.99 +/- 3896.14\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-208434.69 +/- 3475.64\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-209951.62 +/- 5408.43\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-208399.95 +/- 4097.28\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-210822.40 +/- 3484.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-208485.19 +/- 2847.38\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-209692.23 +/- 2099.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-205975.70 +/- 2055.81\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-211983.09 +/- 4521.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-209059.24 +/- 7046.03\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-211060.99 +/- 7309.27\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-210134.65 +/- 4763.22\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-211015.64 +/- 4780.15\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-208445.94 +/- 4856.60\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-207722.57 +/- 4800.47\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-209067.07 +/- 4525.67\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-207081.00 +/- 4563.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-209061.09 +/- 4907.03\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-209150.18 +/- 4737.95\n",
      "Episode length: 1296.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x79d996aa0220>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustomRaceTrack_3.radius = 1200\n",
    "model_file_path_3 = './logs/CustomRacingEnv_3/'\n",
    "\n",
    "model_3 = PPO(\"MultiInputPolicy\", CustomRaceTrack_3)\n",
    "\n",
    "model_3.learn(\n",
    "    total_timesteps=50000,\n",
    "    callback=EvalCallback(\n",
    "        CustomRaceTrack_3, best_model_save_path=model_file_path_3, eval_freq=1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DUQz9ZTHnUPS",
    "outputId": "a84582df-c8f0-4fab-e7eb-a0e722682ee3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11945.734666861816\n",
      "Radius: 600, avg G: -11879.576617414787\n",
      "Radius: 600, max G: -11836.319518595572\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13435.01413610137\n",
      "Radius: 700, avg G: -13286.634934776539\n",
      "Radius: 700, max G: -13211.36740991476\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14799.60779664564\n",
      "Radius: 800, avg G: -14679.741931319513\n",
      "Radius: 800, max G: -14603.476983202516\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16334.500469774786\n",
      "Radius: 900, avg G: -16059.260480532539\n",
      "Radius: 900, max G: -15944.01777160026\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17518.453955372075\n",
      "Radius: 1000, avg G: -17414.852668868676\n",
      "Radius: 1000, max G: -17352.28428126192\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18911.933877135445\n",
      "Radius: 1100, avg G: -18713.033228139044\n",
      "Radius: 1100, max G: -18596.440887029574\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20283.034431943222\n",
      "Radius: 1200, avg G: -20059.612601444904\n",
      "Radius: 1200, max G: -19928.531763947205\n"
     ]
    }
   ],
   "source": [
    "result_case3 = evaluate_model(model_path=model_file_path_3, env=CustomRaceTrack_2) # Test in the env without reward shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqDMqGzye9vH"
   },
   "source": [
    "# Test Case 4. Rescaling [Yes], Reward Shaping [Yes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fWu8NP2ewl6",
    "outputId": "bbf02348-f1c5-4020-a1c0-66531bf15f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-158085.39 +/- 105.38\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-158115.15 +/- 47.16\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-209245.57 +/- 5135.37\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-206500.96 +/- 2670.64\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-209922.46 +/- 5737.28\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-210190.44 +/- 3881.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-208838.18 +/- 1705.41\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-210256.55 +/- 5248.94\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-210789.40 +/- 1488.93\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-208562.56 +/- 1700.77\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-203748.16 +/- 3072.64\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-208112.13 +/- 4300.80\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-210233.23 +/- 3799.10\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-210713.58 +/- 2115.79\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-209202.23 +/- 4047.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-205945.30 +/- 2565.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-212952.99 +/- 4669.68\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-205438.49 +/- 4274.27\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-206491.69 +/- 1931.87\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-208120.58 +/- 3704.23\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-207951.62 +/- 4028.40\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-208353.88 +/- 3387.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-211696.50 +/- 3880.46\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-210654.35 +/- 5749.80\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-208392.14 +/- 4400.47\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-209133.04 +/- 4772.82\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-205466.21 +/- 1777.28\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-208060.52 +/- 3155.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-209811.37 +/- 2606.01\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-210239.62 +/- 4013.63\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-208607.58 +/- 5509.16\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-205970.66 +/- 3493.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-212969.46 +/- 1552.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-206755.15 +/- 3318.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-209208.74 +/- 2622.13\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-206153.55 +/- 2939.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-209571.13 +/- 2937.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-204827.99 +/- 4169.96\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-211408.27 +/- 3121.68\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-205916.26 +/- 2956.72\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-206882.12 +/- 4390.37\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-209695.02 +/- 4650.18\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-207054.80 +/- 3620.74\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-210438.52 +/- 3752.08\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-208628.14 +/- 3895.15\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-208133.93 +/- 2224.21\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-210243.71 +/- 3511.48\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-208050.03 +/- 2675.30\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-212961.69 +/- 6766.12\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-207054.29 +/- 4519.77\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-207304.70 +/- 3335.67\n",
      "Episode length: 1296.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x79d996ab6aa0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustomRaceTrack_4.radius = 1200\n",
    "model_file_path_4 = './logs/CustomRacingEnv_4/'\n",
    "\n",
    "model_4 = PPO(\"MultiInputPolicy\", CustomRaceTrack_4)\n",
    "\n",
    "model_4.learn(\n",
    "    total_timesteps=50000,\n",
    "    callback=EvalCallback(\n",
    "        CustomRaceTrack_4, best_model_save_path=model_file_path_4, eval_freq=1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "keu20n7bnjR9",
    "outputId": "abf88f37-0205-45ab-f33f-51431da90bd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11974.111271979184\n",
      "Radius: 600, avg G: -11880.790229842149\n",
      "Radius: 600, max G: -11814.500508924395\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13445.706391072135\n",
      "Radius: 700, avg G: -13289.663575555494\n",
      "Radius: 700, max G: -13197.500293501942\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14742.95178961832\n",
      "Radius: 800, avg G: -14664.608381396194\n",
      "Radius: 800, max G: -14584.50262645518\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16153.464937443314\n",
      "Radius: 900, avg G: -16056.175715649431\n",
      "Radius: 900, max G: -15983.096786393948\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17467.249638904537\n",
      "Radius: 1000, avg G: -17391.07800669071\n",
      "Radius: 1000, max G: -17305.4616838973\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18856.63682756249\n",
      "Radius: 1100, avg G: -18711.88334350833\n",
      "Radius: 1100, max G: -18642.911296560404\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20150.881303404807\n",
      "Radius: 1200, avg G: -20065.14048029581\n",
      "Radius: 1200, max G: -19992.78154895319\n"
     ]
    }
   ],
   "source": [
    "result_case4 = evaluate_model(model_path=model_file_path_4, env=CustomRaceTrack_2) # Test in the env without reward shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veFdHVljagTZ"
   },
   "source": [
    "# Best PPO Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rchgD2EC-Esr"
   },
   "source": [
    "### The best PPO Model was trained with the **CustomRaceTrack_2** environment (has observation rescaling but no reward shaping) for 500,000 time steps. The evaluation is based on the average of 50 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mDgbXbfPHjR",
    "outputId": "b7dad0ce-2087-4f54-cb84-093490d1074e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "track_1 = Track(Car())\n",
    "agent_1 = Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IeGTP1q6QIQR",
    "outputId": "3fba8699-dae7-4725-802a-52f18b9abc9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 50 iterations\n",
      "\n",
      "Radius: 600, min G: -11515.42940674654\n",
      "Radius: 600, avg G: -11427.458324256384\n",
      "Radius: 600, max G: -11324.827457730746\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13029.995195140511\n",
      "Radius: 700, avg G: -12896.844209935829\n",
      "Radius: 700, max G: -12795.80924254445\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14509.360370120225\n",
      "Radius: 800, avg G: -14354.51442999304\n",
      "Radius: 800, max G: -14244.756504406323\n",
      "\n",
      "\n",
      "Radius: 900, min G: -15856.911567195033\n",
      "Radius: 900, avg G: -15754.76077429582\n",
      "Radius: 900, max G: -15635.922004030079\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17277.104104319882\n",
      "Radius: 1000, avg G: -17126.53087897182\n",
      "Radius: 1000, max G: -16902.999820235065\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18616.035504542226\n",
      "Radius: 1100, avg G: -18469.245668024967\n",
      "Radius: 1100, max G: -18295.22673160177\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -19907.26610179147\n",
      "Radius: 1200, avg G: -19800.259304864536\n",
      "Radius: 1200, max G: -19641.266174732362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max': [-11324.827457730746,\n",
       "  -12795.80924254445,\n",
       "  -14244.756504406323,\n",
       "  -15635.922004030079,\n",
       "  -16902.999820235065,\n",
       "  -18295.22673160177,\n",
       "  -19641.266174732362],\n",
       " 'min': [-11515.42940674654,\n",
       "  -13029.995195140511,\n",
       "  -14509.360370120225,\n",
       "  -15856.911567195033,\n",
       "  -17277.104104319882,\n",
       "  -18616.035504542226,\n",
       "  -19907.26610179147],\n",
       " 'avg': [-11427.458324256384,\n",
       "  -12896.844209935829,\n",
       "  -14354.51442999304,\n",
       "  -15754.76077429582,\n",
       "  -17126.53087897182,\n",
       "  -18469.245668024967,\n",
       "  -19800.259304864536]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterations = 50\n",
    "result_opt = {\"max\": [], \"min\": [], \"avg\":[]}\n",
    "print(f'Averaged over {iterations} iterations')\n",
    "for radius in [600, 700, 800, 900, 1000, 1100, 1200]:\n",
    "    G_list = []\n",
    "    for iteration in range(iterations):\n",
    "        track_1.reset()\n",
    "        track_1.radius = radius\n",
    "        state = track_1._get_state()\n",
    "        done = False\n",
    "        G = 0\n",
    "        while not done:\n",
    "            action = agent_1.act(state)\n",
    "            reward, next_state, done, velocity = track_1.transition(action)\n",
    "            # added velocity for sanity check\n",
    "            state = copy.deepcopy(next_state)\n",
    "            G += reward\n",
    "        G_list.append(G)\n",
    "    print(f\"\\nRadius: {radius}, min G: {np.min(G_list)}\")\n",
    "    print(f\"Radius: {radius}, avg G: {np.mean(G_list)}\")\n",
    "    print(f\"Radius: {radius}, max G: {np.max(G_list)}\\n\")\n",
    "    result_opt[\"max\"].append(np.max(G_list))\n",
    "    result_opt[\"min\"].append(np.min(G_list))\n",
    "    result_opt[\"avg\"].append(np.mean(G_list))\n",
    "result_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1x2_56HSLEt"
   },
   "source": [
    "# 5. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ufVYttq9vyv"
   },
   "source": [
    "### Generate a set of all possible hyperparameter combinations as defined in the grid then randomly select 10 combinations to run through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wR7uWIf1mK8G",
    "outputId": "0bd019d2-1e28-4f8c-b4b8-f7a84436079a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 4096, 'batch_size': 2048, 'n_epochs': 512, 'gamma': 0.99, 'gae_lambda': 0.95, 'clip_range': 0.8, 'ent_coef': 0.1, 'vf_coef': 0.5, 'target_kl': 0.05, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-145618.48 +/- 2486.66\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-142970.42 +/- 1228.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-142239.18 +/- 4038.27\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-145148.30 +/- 3027.81\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-208528.40 +/- 4400.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-208745.94 +/- 4676.31\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-205335.60 +/- 2178.53\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-208831.54 +/- 7042.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-208375.64 +/- 5553.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-210987.72 +/- 3617.58\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-205821.10 +/- 2833.89\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-210377.24 +/- 2452.34\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-155193.43 +/- 13015.37\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-155084.05 +/- 10842.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-150539.93 +/- 4248.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-152072.55 +/- 8921.51\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-139265.13 +/- 4072.71\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=18000, episode_reward=-140309.55 +/- 4465.09\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-137093.28 +/- 5251.24\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-140614.31 +/- 6878.62\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -12100.719127926404\n",
      "Radius: 600, avg G: -11815.488489111247\n",
      "Radius: 600, max G: -11637.173361451456\n",
      "\n",
      "\n",
      "Radius: 700, min G: -14030.6149693272\n",
      "Radius: 700, avg G: -13341.813798932411\n",
      "Radius: 700, max G: -13136.134208689044\n",
      "\n",
      "\n",
      "Radius: 800, min G: -15230.573596802416\n",
      "Radius: 800, avg G: -14841.322601767868\n",
      "Radius: 800, max G: -14557.99290150672\n",
      "\n",
      "\n",
      "Radius: 900, min G: -17289.65322720771\n",
      "Radius: 900, avg G: -16523.483967775177\n",
      "Radius: 900, max G: -15988.070980567933\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -20103.326745461127\n",
      "Radius: 1000, avg G: -18320.555499243135\n",
      "Radius: 1000, max G: -17342.654784735634\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -21629.770105860956\n",
      "Radius: 1100, avg G: -19755.256163055506\n",
      "Radius: 1100, max G: -19012.618150403683\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -22752.498663867893\n",
      "Radius: 1200, avg G: -21576.247592890017\n",
      "Radius: 1200, max G: -20789.686939057916\n",
      "\n",
      "\n",
      "Mean reward: -16751.221708363875\n",
      "\n",
      "\n",
      "Saved a new model\n",
      "\n",
      "\n",
      "Elapsed time: 244.44 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.003, 'n_steps': 512, 'batch_size': 2048, 'n_epochs': 1024, 'gamma': 0.8, 'gae_lambda': 0.8, 'clip_range': 0.8, 'ent_coef': 0.0, 'vf_coef': 0.5, 'target_kl': 0.05, 'device': 'auto'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 2048, but because the `RolloutBuffer` is of size `n_steps * n_envs = 512`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 512\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=512 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-210065.04 +/- 1925.65\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-207971.45 +/- 5509.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-207585.50 +/- 5302.90\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-204705.85 +/- 5043.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-209511.13 +/- 7810.87\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-208820.25 +/- 4663.43\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-208411.70 +/- 5529.47\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-168170.05 +/- 7136.36\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=9000, episode_reward=-141383.44 +/- 2863.96\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=10000, episode_reward=-163524.69 +/- 1412.87\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-164933.07 +/- 2409.03\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-167414.31 +/- 5126.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-178306.70 +/- 7014.15\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-175437.71 +/- 5161.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-172450.11 +/- 3858.72\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-173731.75 +/- 6826.93\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-165916.31 +/- 5861.06\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-159501.01 +/- 3030.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-159522.92 +/- 3291.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-154364.75 +/- 2445.82\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -12728.47794506997\n",
      "Radius: 600, avg G: -12351.880056079708\n",
      "Radius: 600, max G: -12083.500010885457\n",
      "\n",
      "\n",
      "Radius: 700, min G: -15174.833847466085\n",
      "Radius: 700, avg G: -14456.874464131237\n",
      "Radius: 700, max G: -14020.82611066189\n",
      "\n",
      "\n",
      "Radius: 800, min G: -17508.32847897993\n",
      "Radius: 800, avg G: -17063.110924521672\n",
      "Radius: 800, max G: -16363.050706317219\n",
      "\n",
      "\n",
      "Radius: 900, min G: -20343.793016272262\n",
      "Radius: 900, avg G: -18669.397471595403\n",
      "Radius: 900, max G: -17037.227700249055\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -23211.66236223928\n",
      "Radius: 1000, avg G: -21327.716693101916\n",
      "Radius: 1000, max G: -20313.01058359016\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -24893.360135863648\n",
      "Radius: 1100, avg G: -23838.584880914685\n",
      "Radius: 1100, max G: -22455.53133317618\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -28222.49255644324\n",
      "Radius: 1200, avg G: -26108.90500539517\n",
      "Radius: 1200, max G: -22905.390543600188\n",
      "\n",
      "\n",
      "Mean reward: -19098.950229835922\n",
      "\n",
      "\n",
      "Elapsed time: 260.35 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 3e-05, 'n_steps': 512, 'batch_size': 2048, 'n_epochs': 128, 'gamma': 0.3, 'gae_lambda': 0.8, 'clip_range': 0.4, 'ent_coef': 0.1, 'vf_coef': 0.5, 'target_kl': 0.01, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-203341.99 +/- 4622.78\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-212726.29 +/- 4580.30\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-206095.97 +/- 5419.73\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-205926.77 +/- 2564.05\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-211355.25 +/- 4416.23\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-207971.63 +/- 2136.31\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-205964.97 +/- 2847.80\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-211118.09 +/- 3749.32\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-207432.40 +/- 4101.02\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-207484.69 +/- 3865.71\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-207307.64 +/- 3610.96\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-211706.64 +/- 3957.74\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-210424.83 +/- 2604.59\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-212059.51 +/- 5493.66\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-209967.64 +/- 4331.08\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-208343.33 +/- 4891.61\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-210124.58 +/- 5005.83\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-207689.15 +/- 3675.85\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-208504.01 +/- 3863.20\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-206216.89 +/- 1933.15\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11938.468482660164\n",
      "Radius: 600, avg G: -11872.72463664486\n",
      "Radius: 600, max G: -11803.438581396573\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13356.438057615433\n",
      "Radius: 700, avg G: -13281.478968313091\n",
      "Radius: 700, max G: -13207.435038722202\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14732.625875670536\n",
      "Radius: 800, avg G: -14663.222623790436\n",
      "Radius: 800, max G: -14605.31871899592\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16190.744692045266\n",
      "Radius: 900, avg G: -16058.778668517058\n",
      "Radius: 900, max G: -15962.216940786348\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17478.244205535753\n",
      "Radius: 1000, avg G: -17377.617787171133\n",
      "Radius: 1000, max G: -17305.621693306577\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18817.20967798935\n",
      "Radius: 1100, avg G: -18733.903883159495\n",
      "Radius: 1100, max G: -18631.523809873797\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20142.329934116726\n",
      "Radius: 1200, avg G: -20058.491392856806\n",
      "Radius: 1200, max G: -19910.56319819362\n",
      "\n",
      "\n",
      "Mean reward: -16006.11413654101\n",
      "\n",
      "\n",
      "Saved a new model\n",
      "\n",
      "\n",
      "Elapsed time: 266.80 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 3e-05, 'n_steps': 2048, 'batch_size': 256, 'n_epochs': 512, 'gamma': 0.8, 'gae_lambda': 0.8, 'clip_range': 0.4, 'ent_coef': 0.1, 'vf_coef': 0.7, 'target_kl': 0.01, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-156464.01 +/- 1116.91\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-158501.31 +/- 2275.66\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-208587.50 +/- 3621.43\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-208393.40 +/- 6185.35\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-208781.73 +/- 3335.90\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-208033.70 +/- 1584.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-208774.58 +/- 3999.01\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-209294.03 +/- 5609.97\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-213165.84 +/- 2721.60\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-208554.23 +/- 3712.62\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-210729.09 +/- 5498.54\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-209846.99 +/- 3845.25\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-205754.08 +/- 2402.68\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-207079.25 +/- 2070.95\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-211004.89 +/- 4956.95\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-208699.69 +/- 5524.74\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-208700.77 +/- 4451.07\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-209432.51 +/- 4297.48\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-208924.68 +/- 3146.58\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-208760.28 +/- 4113.09\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11956.495714202834\n",
      "Radius: 600, avg G: -11882.870167756733\n",
      "Radius: 600, max G: -11788.537748902658\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13416.073065629196\n",
      "Radius: 700, avg G: -13301.192479806585\n",
      "Radius: 700, max G: -13221.694454309703\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14771.88120688903\n",
      "Radius: 800, avg G: -14683.013808733414\n",
      "Radius: 800, max G: -14611.248527934385\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16104.265066931788\n",
      "Radius: 900, avg G: -16031.545895427538\n",
      "Radius: 900, max G: -15984.086660600851\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17449.351428884303\n",
      "Radius: 1000, avg G: -17380.180903204364\n",
      "Radius: 1000, max G: -17254.125028838153\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18783.878073595504\n",
      "Radius: 1100, avg G: -18680.308979569112\n",
      "Radius: 1100, max G: -18599.915574388626\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20153.747389365108\n",
      "Radius: 1200, avg G: -19999.04214563097\n",
      "Radius: 1200, max G: -19927.65313985421\n",
      "\n",
      "\n",
      "Mean reward: -15999.100355259767\n",
      "\n",
      "\n",
      "Saved a new model\n",
      "\n",
      "\n",
      "Elapsed time: 300.41 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 4096, 'batch_size': 256, 'n_epochs': 512, 'gamma': 0.8, 'gae_lambda': 0.8, 'clip_range': 0.4, 'ent_coef': 0.0, 'vf_coef': 0.5, 'target_kl': 0.05, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-152637.06 +/- 2870.98\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-151718.02 +/- 3951.22\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-151789.85 +/- 5822.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-147640.97 +/- 4370.96\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-212989.44 +/- 4678.34\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-207539.01 +/- 3390.54\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-208433.44 +/- 4241.58\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-205918.87 +/- 2622.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-208073.38 +/- 3642.62\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-207617.70 +/- 2336.10\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-205712.16 +/- 3292.27\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-209381.80 +/- 3384.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-208128.47 +/- 3884.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-207699.20 +/- 2206.90\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-207768.42 +/- 3029.14\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-211378.94 +/- 3879.94\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-206782.21 +/- 4957.96\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-208186.08 +/- 4125.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-204892.25 +/- 3899.53\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-208094.40 +/- 1849.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11969.229582482365\n",
      "Radius: 600, avg G: -11873.142124184673\n",
      "Radius: 600, max G: -11803.29695563527\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13362.706669659512\n",
      "Radius: 700, avg G: -13290.33419212597\n",
      "Radius: 700, max G: -13208.364812569838\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14719.179822426995\n",
      "Radius: 800, avg G: -14663.69575697213\n",
      "Radius: 800, max G: -14576.723896648036\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16181.670288386567\n",
      "Radius: 900, avg G: -16067.013435562374\n",
      "Radius: 900, max G: -15981.71821444761\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17439.21069482819\n",
      "Radius: 1000, avg G: -17375.811504564866\n",
      "Radius: 1000, max G: -17301.212580214105\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18804.16088919361\n",
      "Radius: 1100, avg G: -18706.25914909795\n",
      "Radius: 1100, max G: -18638.599623934595\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20190.556079636593\n",
      "Radius: 1200, avg G: -20062.566049880894\n",
      "Radius: 1200, max G: -19958.555610343505\n",
      "\n",
      "\n",
      "Mean reward: -16008.28609203789\n",
      "\n",
      "\n",
      "Elapsed time: 235.41 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 4096, 'batch_size': 256, 'n_epochs': 1024, 'gamma': 0.3, 'gae_lambda': 0.8, 'clip_range': 0.8, 'ent_coef': 0.01, 'vf_coef': 0.7, 'target_kl': 0.05, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-157906.92 +/- 61.35\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-158041.64 +/- 95.38\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-158016.23 +/- 100.52\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-157979.56 +/- 68.41\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-210425.08 +/- 4820.96\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-213727.68 +/- 4072.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-209527.00 +/- 4736.74\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-207865.54 +/- 4196.39\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-210435.58 +/- 3973.68\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-208206.04 +/- 3362.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-208026.59 +/- 2398.26\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-208496.97 +/- 3581.06\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-208545.65 +/- 2104.70\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-210341.13 +/- 2867.24\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-209631.66 +/- 4795.13\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-209998.15 +/- 3036.49\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-207974.71 +/- 3675.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-207653.89 +/- 2808.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-207135.21 +/- 2294.63\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-208219.89 +/- 4044.49\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -12082.270527135803\n",
      "Radius: 600, avg G: -11905.953069491223\n",
      "Radius: 600, max G: -11822.59765619714\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13390.879885555927\n",
      "Radius: 700, avg G: -13300.308652509158\n",
      "Radius: 700, max G: -13247.45287981882\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14769.662574502077\n",
      "Radius: 800, avg G: -14683.738090284369\n",
      "Radius: 800, max G: -14629.769722102452\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16248.521064214776\n",
      "Radius: 900, avg G: -16069.777974180677\n",
      "Radius: 900, max G: -15928.681106618924\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17515.095690895632\n",
      "Radius: 1000, avg G: -17415.80416756844\n",
      "Radius: 1000, max G: -17255.89896243961\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18800.29684734129\n",
      "Radius: 1100, avg G: -18698.663377271347\n",
      "Radius: 1100, max G: -18619.691501423487\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20172.343869621458\n",
      "Radius: 1200, avg G: -20046.74252791423\n",
      "Radius: 1200, max G: -19972.430927958947\n",
      "\n",
      "\n",
      "Mean reward: -16027.456241668848\n",
      "\n",
      "\n",
      "Elapsed time: 233.91 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 4096, 'batch_size': 2048, 'n_epochs': 512, 'gamma': 0.3, 'gae_lambda': 0.6, 'clip_range': 0.8, 'ent_coef': 0.01, 'vf_coef': 0.5, 'target_kl': 0.05, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-158058.65 +/- 3627.87\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-158382.18 +/- 4140.83\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-156302.61 +/- 1067.28\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-155476.11 +/- 1495.67\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000, episode_reward=-212444.69 +/- 4754.71\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-206796.90 +/- 4321.01\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-205756.24 +/- 4840.82\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-210833.20 +/- 3824.52\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-209569.99 +/- 3626.21\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-207870.77 +/- 4196.50\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-207731.64 +/- 5629.23\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-207284.93 +/- 4442.57\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-211182.84 +/- 3839.22\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-208014.05 +/- 1647.58\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-210055.95 +/- 3044.19\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-207692.96 +/- 7153.98\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-209719.17 +/- 2221.83\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-210344.25 +/- 6865.64\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-209184.97 +/- 6181.48\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-206418.98 +/- 3492.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11981.917434614084\n",
      "Radius: 600, avg G: -11888.93656079679\n",
      "Radius: 600, max G: -11801.212172238795\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13397.888360353429\n",
      "Radius: 700, avg G: -13314.200709067238\n",
      "Radius: 700, max G: -13205.383995557415\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14730.900203610894\n",
      "Radius: 800, avg G: -14670.52308048286\n",
      "Radius: 800, max G: -14566.546351924295\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16321.335145171659\n",
      "Radius: 900, avg G: -16071.636692648568\n",
      "Radius: 900, max G: -15970.724847443284\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17633.275774303787\n",
      "Radius: 1000, avg G: -17392.0814698776\n",
      "Radius: 1000, max G: -17326.730666820426\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18795.037570460125\n",
      "Radius: 1100, avg G: -18717.49711848837\n",
      "Radius: 1100, max G: -18619.87755854136\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20489.0991647079\n",
      "Radius: 1200, avg G: -20093.001214458094\n",
      "Radius: 1200, max G: -20010.49994289824\n",
      "\n",
      "\n",
      "Mean reward: -16047.538382593584\n",
      "\n",
      "\n",
      "Elapsed time: 234.17 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 4096, 'batch_size': 2048, 'n_epochs': 1024, 'gamma': 0.99, 'gae_lambda': 0.6, 'clip_range': 0.8, 'ent_coef': 0.1, 'vf_coef': 0.7, 'target_kl': 0.01, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-157823.88 +/- 110.26\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-157916.54 +/- 140.14\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-157985.82 +/- 151.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-158059.46 +/- 158.91\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-208593.54 +/- 3479.75\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-206311.26 +/- 4621.59\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-209178.44 +/- 4221.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-213273.08 +/- 2915.89\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-209654.06 +/- 3881.56\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-207982.79 +/- 4795.00\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-212207.41 +/- 2668.04\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-211166.19 +/- 6019.85\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-207810.62 +/- 5620.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-209927.65 +/- 4759.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-207417.30 +/- 3585.78\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-211232.46 +/- 2634.72\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-209852.12 +/- 4539.78\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-209511.15 +/- 3058.64\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-206999.01 +/- 3201.05\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-210292.45 +/- 4700.95\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11922.160222792714\n",
      "Radius: 600, avg G: -11864.110263426348\n",
      "Radius: 600, max G: -11785.683695767317\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13335.787037784514\n",
      "Radius: 700, avg G: -13267.375074298208\n",
      "Radius: 700, max G: -13214.886153700096\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14797.029572510488\n",
      "Radius: 800, avg G: -14710.327627062139\n",
      "Radius: 800, max G: -14647.670439574349\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16248.865823489501\n",
      "Radius: 900, avg G: -16055.328353255454\n",
      "Radius: 900, max G: -15962.461781519502\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17456.969671111223\n",
      "Radius: 1000, avg G: -17377.40973197177\n",
      "Radius: 1000, max G: -17282.230270582535\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18897.575066162408\n",
      "Radius: 1100, avg G: -18730.299326243814\n",
      "Radius: 1100, max G: -18656.893533672726\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20181.07270699616\n",
      "Radius: 1200, avg G: -20053.884032417503\n",
      "Radius: 1200, max G: -19993.044654171594\n",
      "\n",
      "\n",
      "Mean reward: -16021.003097071925\n",
      "\n",
      "\n",
      "Elapsed time: 233.97 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 512, 'batch_size': 1024, 'n_epochs': 128, 'gamma': 0.8, 'gae_lambda': 0.6, 'clip_range': 0.6, 'ent_coef': 0.1, 'vf_coef': 0.5, 'target_kl': 0.01, 'device': 'auto'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py:148: UserWarning: You have specified a mini-batch size of 1024, but because the `RolloutBuffer` is of size `n_steps * n_envs = 512`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 512\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=512 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-204318.24 +/- 6017.37\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-210399.99 +/- 4650.22\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-209593.59 +/- 2762.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-208059.57 +/- 5408.79\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-206637.11 +/- 3117.30\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-198220.52 +/- 7945.07\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-183546.60 +/- 4794.59\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-195506.81 +/- 4536.36\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-209540.58 +/- 1892.26\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-209805.52 +/- 4668.81\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-190589.99 +/- 5348.66\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-174640.04 +/- 6545.79\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=13000, episode_reward=-196623.71 +/- 3652.09\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-193595.26 +/- 6252.20\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-188441.13 +/- 6704.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-201608.53 +/- 5012.73\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-204943.28 +/- 2149.67\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-203088.67 +/- 10495.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-205873.52 +/- 3235.69\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-206688.19 +/- 1569.58\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -13407.308094881762\n",
      "Radius: 600, avg G: -13027.033207415841\n",
      "Radius: 600, max G: -12515.78070977681\n",
      "\n",
      "\n",
      "Radius: 700, min G: -16501.6015489121\n",
      "Radius: 700, avg G: -15881.675691289603\n",
      "Radius: 700, max G: -15015.624146464274\n",
      "\n",
      "\n",
      "Radius: 800, min G: -21195.709113726254\n",
      "Radius: 800, avg G: -19836.525198774638\n",
      "Radius: 800, max G: -18630.308289381195\n",
      "\n",
      "\n",
      "Radius: 900, min G: -25343.267392010996\n",
      "Radius: 900, avg G: -23045.735503239917\n",
      "Radius: 900, max G: -21471.413493698426\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -32892.87338255552\n",
      "Radius: 1000, avg G: -28509.326043144498\n",
      "Radius: 1000, max G: -24331.871094779628\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -38618.509449837424\n",
      "Radius: 1100, avg G: -35326.744162014176\n",
      "Radius: 1100, max G: -29903.14036609036\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -45048.56574639033\n",
      "Radius: 1200, avg G: -40374.10037948149\n",
      "Radius: 1200, max G: -37672.6921604631\n",
      "\n",
      "\n",
      "Mean reward: -25169.03834163468\n",
      "\n",
      "\n",
      "Elapsed time: 234.89 seconds\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "Training with hyperparameters: {'learning_rate': 0.003, 'n_steps': 512, 'batch_size': 1024, 'n_epochs': 128, 'gamma': 0.99, 'gae_lambda': 0.99, 'clip_range': 0.4, 'ent_coef': 0.0, 'vf_coef': 0.7, 'target_kl': 0.05, 'device': 'auto'}\n",
      "\n",
      "Eval num_timesteps=1000, episode_reward=-186428.42 +/- 6946.65\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-157887.03 +/- 180.42\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-157926.83 +/- 181.50\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-172281.39 +/- 6312.19\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-210945.22 +/- 2866.49\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-193291.50 +/- 11515.97\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-166405.88 +/- 7840.09\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-207335.23 +/- 4870.84\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-210037.83 +/- 2781.99\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-209738.88 +/- 4141.93\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-208044.93 +/- 3321.10\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-209227.73 +/- 857.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-211258.94 +/- 3771.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-206911.05 +/- 3517.84\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-209565.92 +/- 3499.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-209281.44 +/- 5164.95\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-213953.93 +/- 2120.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-209469.40 +/- 4413.65\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-207921.71 +/- 3097.05\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-210683.27 +/- 5398.67\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11948.228100134424\n",
      "Radius: 600, avg G: -11889.972489821412\n",
      "Radius: 600, max G: -11817.177609391721\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13360.256873730403\n",
      "Radius: 700, avg G: -13302.556930724615\n",
      "Radius: 700, max G: -13194.736958310574\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14845.277032953085\n",
      "Radius: 800, avg G: -14685.799048421697\n",
      "Radius: 800, max G: -14590.809270910342\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16224.988647186177\n",
      "Radius: 900, avg G: -16048.75001220892\n",
      "Radius: 900, max G: -15968.561861323598\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17528.624679848195\n",
      "Radius: 1000, avg G: -17420.32197959442\n",
      "Radius: 1000, max G: -17335.8267781674\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18912.59136606675\n",
      "Radius: 1100, avg G: -18771.171466735854\n",
      "Radius: 1100, max G: -18682.214438475505\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20104.508440012658\n",
      "Radius: 1200, avg G: -20019.48393715667\n",
      "Radius: 1200, max G: -19916.31917704009\n",
      "\n",
      "\n",
      "Mean reward: -16027.05605229593\n",
      "\n",
      "\n",
      "Elapsed time: 264.51 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define a hyperparameter grid\n",
    "hyperparameter_grid = {\n",
    "    #'policy': ['MlpPolicy', 'CnnPolicy'],  # Different policy models\n",
    "    'learning_rate': [0.03, 0.003, 0.00003],  # Learning rates\n",
    "    'n_steps': [4096, 2048, 1024, 512],  # Number of steps per update\n",
    "    'batch_size': [256, 1024, 2048],  # Minibatch size\n",
    "    'n_epochs': [128, 512, 1024], #4, 8, 16],  # Number of optimization epochs\n",
    "    'gamma': [0.99, 0.8, 0.3],  # Discount factor\n",
    "    'gae_lambda': [0.99, 0.95, 0.80, 0.6],  # GAE lambda\n",
    "    'clip_range': [0.4, 0.6, 0.8],  # Clipping parameter for policy\n",
    "    'ent_coef': [0.0, 0.01, 0.1],  # Entropy coefficient\n",
    "    'vf_coef': [0.5, 0.7],  # Value function coefficient\n",
    "    #'max_grad_norm': [0.5, 0.8],  # Maximum gradient norm\n",
    "    #'sde_sample_freq': [100, 500],  # Frequency of sampling noise matrix\n",
    "    'target_kl': [0.01, 0.05],  # Limit on KL divergence\n",
    "    #'stats_window_size': [10, 20],  # Window size for rollout logging\n",
    "    'device': ['auto'],  # Device to run on (auto or GPU)\n",
    "}  #Training with hyperparameters: {'learning_rate': 0.03, 'n_steps': 1024, 'batch_size': 256, 'n_epochs': 128, 'gamma': 0.8, 'gae_lambda': 0.95, 'clip_range': 0.4, 'device': 'auto'}\n",
    "\n",
    "# Generate all combinations of hyperparameters\n",
    "all_hyperparam_combinations = list(product(*hyperparameter_grid.values()))\n",
    "\n",
    "# Run hyperparameter tuning for random 10 combinations\n",
    "all_hyperparam_combinations = random.sample(all_hyperparam_combinations, 10)\n",
    "\n",
    "# Set random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Shuffle hyperparam combinations\n",
    "random.shuffle(all_hyperparam_combinations)\n",
    "\n",
    "best_model = None\n",
    "best_mean_reward = -float('inf')\n",
    "best_hyperparam = None\n",
    "\n",
    "for hyperparams in all_hyperparam_combinations:\n",
    "    start_time = time.time()\n",
    "    hyperparameter_dict = {key: value for key, value in zip(hyperparameter_grid.keys(), hyperparams)}\n",
    "    print(\"---------------------------------------------------------------------------------------\")\n",
    "    print(f\"\\nTraining with hyperparameters: {hyperparameter_dict}\\n\")\n",
    "    # Create the PPO model with the current hyperparameters\n",
    "    model = PPO(\"MultiInputPolicy\", CustomRaceTrack_4, verbose=0, **hyperparameter_dict)\n",
    "\n",
    "    save_to_path = './logs/5_hyperparam_tuning/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Train the model with the current hyperparameters\n",
    "    model.learn(\n",
    "        total_timesteps=20000,\n",
    "        callback=EvalCallback(\n",
    "            CustomRaceTrack_4, best_model_save_path=save_to_path, eval_freq=1000\n",
    "        )\n",
    "    )\n",
    "    # Evaluate the model's performance\n",
    "\n",
    "    model = PPO.load(save_to_path + \"/best_model.zip\")\n",
    "\n",
    "    iterations = 10\n",
    "    all_g = {\"max\": [], \"min\": [], \"avg\":[]}\n",
    "    print(f'Averaged over {iterations} iterations')\n",
    "    for radius in [600, 700, 800, 900, 1000, 1100, 1200]:\n",
    "        G_list = []\n",
    "        for iteration in range(iterations):\n",
    "            env = CustomRaceTrack_2 # Test in the env without reward shaping\n",
    "            env.radius = radius\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            G = 0\n",
    "            while not done:\n",
    "                action, _ = model.predict(state)\n",
    "                next_state, reward, done, velocity = env.step(action)\n",
    "                # added velocity for sanity check\n",
    "                state = copy.deepcopy(next_state)\n",
    "                G += reward\n",
    "            G_list.append(G)\n",
    "        print(f\"\\nRadius: {radius}, min G: {np.min(G_list)}\")\n",
    "        print(f\"Radius: {radius}, avg G: {np.mean(G_list)}\")\n",
    "        print(f\"Radius: {radius}, max G: {np.max(G_list)}\\n\")\n",
    "\n",
    "        all_g[\"max\"].append(np.max(G_list))\n",
    "        all_g[\"min\"].append(np.min(G_list))\n",
    "        all_g[\"avg\"].append(np.mean(G_list))\n",
    "    all_g\n",
    "\n",
    "    # Consider models with the best avg rewards over all radiuses\n",
    "    score = np.mean([value for sublist in all_g.values() for value in sublist])\n",
    "\n",
    "    print(f\"\\nMean reward: {score}\\n\")\n",
    "    if score > best_mean_reward:\n",
    "        best_model = model\n",
    "        best_mean_reward = score\n",
    "        best_hyperparam = hyperparameter_dict\n",
    "        best_model.save(\"best_PPO_model\")\n",
    "        print(\"\\nSaved a new model\\n\")\n",
    "\n",
    "    # Calculate and print the elapsed time\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"\\nElapsed time: {elapsed_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fIBR2SOCG6KM",
    "outputId": "6558b0d6-c9a1-4c5b-c391-e65d45600b9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 3e-05,\n",
       " 'n_steps': 2048,\n",
       " 'batch_size': 256,\n",
       " 'n_epochs': 512,\n",
       " 'gamma': 0.8,\n",
       " 'gae_lambda': 0.8,\n",
       " 'clip_range': 0.4,\n",
       " 'ent_coef': 0.1,\n",
       " 'vf_coef': 0.7,\n",
       " 'target_kl': 0.01,\n",
       " 'device': 'auto'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_hyperparam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O26epOakC-t-"
   },
   "source": [
    "### Try Tuned Hyperparameters on Test Case 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jyJvJW-P6fNG",
    "outputId": "9017b3bd-1bea-41f5-85d4-ca4e81b719bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=-19970.26 +/- 75.36\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-19908.47 +/- 51.19\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3000, episode_reward=-72701.15 +/- 155.51\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=4000, episode_reward=-72764.07 +/- 160.76\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-72750.39 +/- 139.08\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=6000, episode_reward=-72697.32 +/- 197.60\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=7000, episode_reward=-72694.60 +/- 182.68\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=8000, episode_reward=-72714.34 +/- 38.86\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=9000, episode_reward=-72722.71 +/- 129.92\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=10000, episode_reward=-72605.94 +/- 164.53\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=11000, episode_reward=-72683.22 +/- 205.48\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=12000, episode_reward=-72686.65 +/- 173.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=13000, episode_reward=-72701.71 +/- 169.22\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=14000, episode_reward=-72573.22 +/- 174.74\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=15000, episode_reward=-72654.99 +/- 143.03\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=16000, episode_reward=-72623.78 +/- 179.61\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=17000, episode_reward=-72566.61 +/- 193.89\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=18000, episode_reward=-72659.19 +/- 246.42\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=19000, episode_reward=-72645.03 +/- 217.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=20000, episode_reward=-72718.63 +/- 191.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=21000, episode_reward=-72638.79 +/- 151.17\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=22000, episode_reward=-72492.59 +/- 139.62\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=23000, episode_reward=-72656.97 +/- 134.49\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=24000, episode_reward=-72622.50 +/- 123.67\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=25000, episode_reward=-72631.24 +/- 170.85\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=26000, episode_reward=-72777.64 +/- 164.65\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=27000, episode_reward=-72635.87 +/- 84.81\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=28000, episode_reward=-72544.45 +/- 123.45\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=29000, episode_reward=-72646.35 +/- 201.55\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=30000, episode_reward=-72524.51 +/- 113.63\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=31000, episode_reward=-72677.62 +/- 82.25\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=32000, episode_reward=-72766.93 +/- 128.44\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=33000, episode_reward=-72604.83 +/- 158.59\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=34000, episode_reward=-72671.14 +/- 173.21\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=35000, episode_reward=-72633.31 +/- 172.24\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=36000, episode_reward=-72657.62 +/- 106.26\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=37000, episode_reward=-72763.86 +/- 126.58\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=38000, episode_reward=-72647.45 +/- 129.75\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=39000, episode_reward=-72739.53 +/- 115.40\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=40000, episode_reward=-72595.74 +/- 101.59\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=41000, episode_reward=-72448.27 +/- 73.87\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=42000, episode_reward=-72496.03 +/- 81.66\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=43000, episode_reward=-72648.86 +/- 141.20\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=44000, episode_reward=-72699.17 +/- 165.24\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=45000, episode_reward=-72587.69 +/- 241.97\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=46000, episode_reward=-72570.28 +/- 165.24\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=47000, episode_reward=-72589.72 +/- 126.15\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=48000, episode_reward=-72649.68 +/- 163.90\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=49000, episode_reward=-72631.23 +/- 170.94\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=50000, episode_reward=-72631.77 +/- 161.33\n",
      "Episode length: 1296.00 +/- 0.00\n",
      "Eval num_timesteps=51000, episode_reward=-72471.48 +/- 146.31\n",
      "Episode length: 1296.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x79d996615480>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CustomRaceTrack_1.radius = 1200\n",
    "model_file_path_1_hyperparam = './logs/CustomRacingEnv_1_tuned_hyperparam/'\n",
    "\n",
    "model_1 = PPO(\"MultiInputPolicy\", CustomRaceTrack_1, **best_hyperparam)\n",
    "\n",
    "model_1.learn(\n",
    "    total_timesteps=50000,\n",
    "    callback=EvalCallback(\n",
    "        CustomRaceTrack_1, best_model_save_path=model_file_path_1_hyperparam, eval_freq=1000\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xGXgrrQqnz-n",
    "outputId": "d978a4b1-7184-4e3d-ab45-5862c4cf4200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged over 10 iterations\n",
      "\n",
      "Radius: 600, min G: -11952.169083877101\n",
      "Radius: 600, avg G: -11864.01951852158\n",
      "Radius: 600, max G: -11800.513928174318\n",
      "\n",
      "\n",
      "Radius: 700, min G: -13403.42266698121\n",
      "Radius: 700, avg G: -13276.246538432\n",
      "Radius: 700, max G: -13168.530982167083\n",
      "\n",
      "\n",
      "Radius: 800, min G: -14920.894810206724\n",
      "Radius: 800, avg G: -14710.301631319137\n",
      "Radius: 800, max G: -14619.644195027884\n",
      "\n",
      "\n",
      "Radius: 900, min G: -16085.495302303849\n",
      "Radius: 900, avg G: -16038.654244326317\n",
      "Radius: 900, max G: -15971.418137296616\n",
      "\n",
      "\n",
      "Radius: 1000, min G: -17493.89409635263\n",
      "Radius: 1000, avg G: -17374.447536615662\n",
      "Radius: 1000, max G: -17238.884300005608\n",
      "\n",
      "\n",
      "Radius: 1100, min G: -18976.838446568727\n",
      "Radius: 1100, avg G: -18719.31334417736\n",
      "Radius: 1100, max G: -18616.5792565261\n",
      "\n",
      "\n",
      "Radius: 1200, min G: -20077.860821031445\n",
      "Radius: 1200, avg G: -20028.945162709253\n",
      "Radius: 1200, max G: -19971.85278740442\n"
     ]
    }
   ],
   "source": [
    "res_hyperparam = evaluate_model(model_path=model_file_path_1_hyperparam, env=CustomRaceTrack_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PXb9uUmpX5R"
   },
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "bb5DmdqolRlH",
    "outputId": "3abe31b8-60c7-4291-e8ff-a50b34f7c00c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAHHCAYAAAC2gDDfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACBL0lEQVR4nO3dd1hUx/s28HvpvSpNEVAsoNgwKhaQiILYeyGKijVgF8s3do29l4TYNcHeYuyIYsXeaywoNkCliYU67x++nJ8roKzuCob7c117XZyZOec8Z3ZZHmZn58iEEAJEREREREWAWkEHQERERET0rTD5JSIiIqIig8kvERERERUZTH6JiIiIqMhg8ktERERERQaTXyIiIiIqMpj8EhEREVGRweSXiIiIiIoMJr9EREREVGQw+SUipbC3t0f37t0LOoz/rAcPHkAmk2H16tX5ai+TyTBhwoQvOhefS6B79+6wt7cv6DAKXGF8LTRo0AANGjSQtnP73ZgwYQJkMtm3D46+C0x+C8jq1ashk8mkh46ODsqVK4egoCDExsZK7SIiIuTaaWpqonTp0ujWrRvu37+f47gvX75EcHAwypcvDx0dHZiZmcHb2xu7du1S+TXVrFkTMpkMv//+u8rP9T35+DlUV1eHhYUF2rVrh5s3bxZ0eIVWdHQ0+vXrB3t7e2hra8PCwgKtWrXCiRMnCjq0XGU/v7169cq1/pdffpHavHjxQinn3LNnzxcnuN9SgwYNUKlSJbmyqVOnYseOHQUT0P/39OlTTJgwAZcuXSrQOP4LPnyPk8lkMDIygoeHB3bv3l3QoRHloFHQARR1kyZNgoODA969e4fjx4/j999/x549e3Dt2jXo6elJ7QYOHIgffvgB6enpuHDhApYuXYrdu3fj6tWrsLGxAQDcvn0bDRs2xPPnz9GjRw/UqFEDiYmJCA0NRfPmzTF8+HDMmjVLJddx584dnD17Fvb29ggNDUX//v1Vcp7v2YfP4ZUrVxASEoKIiAhcu3YNVlZWBR1eoXLixAn4+voCAHr16gVnZ2fExMRg9erVqF+/PhYsWIABAwYUcJQ56ejoYOvWrfjtt9+gpaUlV7d+/Xro6Ojg3bt3Sjvfnj17sGTJklwT4Ldv30JDo/C+xU+dOhXt2rVDq1atCiyGp0+fYuLEibC3t0fVqlXl6pYtW4asrKyCCew71ahRI3Tr1g1CCDx8+BC///47mjdvjr1798Lb21tl57Wzs8Pbt2+hqakplY0ZMwajRo1S2Tnp+1Z43xmLiCZNmqBGjRoA3v+RNzc3x9y5c/H333+jc+fOUrv69eujXbt2AIAePXqgXLlyGDhwINasWYPRo0cjPT0d7dq1Q0JCAo4ePYpatWpJ+w4ZMgR+fn6YPXs2atSogY4dOyr9Ov766y9YWFhgzpw5aNeuHR48ePDNPzJ8/fo19PX1v+k5FfHhcwgA5cuXR//+/bF27VqMGDGiACPLn2/VvwkJCWjXrh10dXVx4sQJlClTRqobOnQovL29MXjwYLi6uqJOnToqjyfbu3fvoKWlBTW1vD8w8/Hxwc6dO7F37160bNlSKj958iSioqLQtm1bbN269VuECx0dnW9ynsIkP89Rfn2YSFH+lCtXDj/99JO03bZtWzg7O2PBggUqTX6zPz39kIaGRqH+548KFqc9FDI//vgjACAqKkqhdlu3bsW1a9cwatQoucQXANTV1fHHH3/AxMREZR+Rrlu3Du3atUOzZs1gbGyMdevWSXVbtmyBTCbDkSNHcuz3xx9/QCaT4dq1a1LZrVu30K5dO5iZmUFHRwc1atTAzp075fbLnjZy5MgR/Pzzz7CwsEDJkiUBAA8fPsTPP/+M8uXLQ1dXF+bm5mjfvj0ePHiQ4/xXrlyBh4cHdHV1UbJkSUyZMgWrVq2CTCbL0X7v3r2oX78+9PX1YWhoiKZNm+L69etf3Gf169cHANy7d0+u/MmTJ+jZsycsLS2hra2NihUrYuXKlVK9EALFihXD0KFDpbKsrCyYmJhAXV0diYmJUvmMGTOgoaGBlJQU6Xq7d++O0qVLQ0dHB1ZWVujZsydevnwpF0P2fLkbN26gS5cuMDU1Rb169aTzT5kyBSVLloSenh48PT1z7Yf09HRMnDgRZcuWhY6ODszNzVGvXj2EhYV9sl/++OMPxMTEYNasWXKJLwDo6upizZo1kMlkmDRpEgDg3LlzkMlkWLNmTY5j7d+/HzKZTG7az+f6F/i/qSobNmzAmDFjUKJECejp6SE5OfmTsZcoUQLu7u5yr38ACA0NhYuLS46P/oG851R+PK/xY927d8eSJUsAyH/knO3jOb/Zz+mtW7fQoUMHGBkZwdzcHIMGDcrXaHRiYiIGDx4MW1tbaGtrw9HRETNmzPii0VGZTIbXr19Lz6VMJpPrg699juLj4zF8+HC4uLjAwMAARkZGaNKkCS5fviy3/w8//ADg/YBCdhzZ80Zzm/P7+vVrDBs2TOqD8uXLY/bs2RBC5Li+oKAg7NixA5UqVZKuYd++fXLtXr16hcGDB8tN7WnUqBEuXLjwyf7L73tc9vvkiRMnMHToUBQvXhz6+vpo3bo1nj9/Ltc2v7/XinByckKxYsVyvMf9/fffaNq0KWxsbKCtrY0yZcpg8uTJyMzMzHGMpUuXokyZMtDV1UXNmjVx7NixHG0455cUxX+LCpnsNwlzc3OF2v3zzz8AgG7duuXa3tjYGC1btsSaNWtw9+5dODo6KitknD59Gnfv3sWqVaugpaWFNm3aIDQ0FP/73/8AAE2bNoWBgQE2bdoEDw8PuX03btyIihUrSknB9evXUbduXZQoUQKjRo2Cvr4+Nm3ahFatWmHr1q1o3bq13P4///wzihcvjnHjxuH169cAgLNnz+LkyZPo1KkTSpYsiQcPHuD3339HgwYNcOPGDWk6yZMnT+Dp6QmZTIbRo0dDX18fy5cvh7a2do5r/PPPP+Hv7w9vb2/MmDEDb968we+//4569erh4sWLXzTKnf2HytTUVCqLjY1F7dq1pT+exYsXx969exEQEIDk5GQMHjwYMpkMdevWxdGjR6X9rly5gqSkJKipqeHEiRNo2rQpAODYsWOoVq0aDAwMAABhYWG4f/8+evToASsrK1y/fh1Lly7F9evXcerUqRx/LNq3b4+yZcti6tSp0h/4cePGYcqUKfD19YWvry8uXLiAxo0bIy0tTW7fCRMmYNq0aejVqxdq1qyJ5ORknDt3DhcuXECjRo3y7Jd//vkHOjo66NChQ671Dg4OqFevHg4dOoS3b9+iRo0aKF26NDZt2gR/f3+5ths3boSpqak06pSf/v3Q5MmToaWlheHDhyM1NTXHVIbcdOnSBYMGDUJKSgoMDAyQkZGBzZs3Y+jQoUqd8tC3b188ffoUYWFh+PPPP/O9X4cOHWBvb49p06bh1KlTWLhwIRISErB27do893nz5g08PDzw5MkT9O3bF6VKlcLJkycxevRoPHv2DPPnz1co9j///FN6XfTp0wcApH90lPEc3bhxAzt27ED79u3h4OCA2NhY/PHHH/Dw8MCNGzdgY2MDJycnTJo0CePGjUOfPn2kf0bz+jRBCIEWLVrg8OHDCAgIQNWqVbF//34EBwfjyZMnmDdvnlz748ePY9u2bfj5559haGiIhQsXom3btoiOjpbet/v164ctW7YgKCgIzs7OePnyJY4fP46bN2+ievXqefZfft/jsg0YMACmpqYYP348Hjx4gPnz5yMoKAgbN26U2uT391oRSUlJSEhIyPFP7OrVq2FgYIChQ4fCwMAAhw4dwrhx45CcnCw3NW/FihXo27cv6tSpg8GDB+P+/fto0aIFzMzMYGtr+8VxEUFQgVi1apUAIA4ePCieP38uHj16JDZs2CDMzc2Frq6uePz4sRBCiMOHDwsAYuXKleL58+fi6dOnYvfu3cLe3l7IZDJx9uxZIYQQVatWFcbGxp8859y5cwUAsXPnTqVeS1BQkLC1tRVZWVlCCCEOHDggAIiLFy9KbTp37iwsLCxERkaGVPbs2TOhpqYmJk2aJJU1bNhQuLi4iHfv3kllWVlZok6dOqJs2bJSWXb/1atXT+6YQgjx5s2bHDFGRkYKAGLt2rVS2YABA4RMJpOL8+XLl8LMzEwAEFFRUUIIIV69eiVMTExE79695Y4ZExMjjI2Nc5R/LLfncN++fcLR0VHIZDJx5swZqW1AQICwtrYWL168kDtGp06dhLGxsXRts2bNEurq6iI5OVkIIcTChQuFnZ2dqFmzphg5cqQQQojMzExhYmIihgwZ8sm+Wb9+vQAgjh49KpWNHz9eABCdO3eWaxsXFye0tLRE06ZNpedbCCH+97//CQDC399fKqtSpYpo2rTpJ/smNyYmJqJKlSqfbDNw4EABQFy5ckUIIcTo0aOFpqamiI+Pl9qkpqYKExMT0bNnT6ksv/2b/ZyVLl061z7LDQARGBgo4uPjhZaWlvjzzz+FEELs3r1byGQy8eDBA6lfnz9/Lu1nZ2cn12/ZPDw8hIeHh7QdFRUlAIhVq1ZJZYGBgSKvt3EAYvz48dJ29rlbtGgh1+7nn38WAMTly5fzjGny5MlCX19f/Pvvv3L7jho1Sqirq4vo6Oi8ukW6looVK8qV6evr53rdyniO3r17JzIzM+XKoqKihLa2ttz7zdmzZ3P0aTZ/f39hZ2cnbe/YsUMAEFOmTJFr165dOyGTycTdu3elMgBCS0tLruzy5csCgFi0aJFUZmxsLAIDA3Oc+3Py+x6X/T7p5eUl9/s6ZMgQoa6uLhITE4UQiv1e5wWACAgIEM+fPxdxcXHi3LlzwsfHRwAQs2bN+mz8ffv2FXp6etJ7f1pamrCwsBBVq1YVqampUrulS5cKAJ/93ch+vRPlhtMeCpiXlxeKFy8OW1tbdOrUCQYGBti+fTtKlCgh165nz54oXrw4bGxs0LRpU+kjw+z5wq9evYKhoeEnz5Vd/7mPbhWRkZGBjRs3omPHjtKo4Y8//ggLCwuEhoZK7Tp27Ii4uDhERERIZVu2bEFWVpY0Bzk+Ph6HDh1Chw4d8OrVK7x48QIvXrzAy5cv4e3tjTt37uDJkydy5+/duzfU1dXlynR1daWf09PT8fLlSzg6OsLExETu48R9+/bBzc1N7osuZmZm8PPzkzteWFgYEhMT0blzZymmFy9eQF1dHbVq1cLhw4fz1VcfPoc+Pj5ISkrCn3/+KX30KoTA1q1b0bx5cwgh5M7l7e2NpKQkKf769esjMzMTJ0+eBPB+hLd+/fqoX7++9LHgtWvXkJiYKI1ofdw37969w4sXL1C7dm0AyPWj1n79+sltHzx4EGlpaRgwYIDcKPHHo3EAYGJiguvXr+POnTv56p9sX/Ja7tixI9LT07Ft2zapzYEDB5CYmCi9vhTp32z+/v5yfZYfpqam8PHxwfr16wG8nxJUp04d2NnZKXQcVQkMDJTbzv7i4J49e/LcZ/Pmzahfvz5MTU3l+s3LywuZmZlyn0J8DWU9R9ra2tK838zMTLx8+RIGBgYoX778Z6cU5GXPnj1QV1fHwIED5cqHDRsGIQT27t0rV+7l5SU34lm5cmUYGRnJrdJjYmKC06dP4+nTpwrFkt/3uGx9+vSR+33Nfv94+PAhAMV+rz9lxYoVKF68OCwsLFCjRg2Eh4djxIgRclO0Po4/+72+fv36ePPmDW7dugXg/XSmuLg49OvXT+4Tl+7du8PY2FihuIg+xmkPBWzJkiUoV64cNDQ0YGlpifLly+f6ZY1x48ahfv36UFdXR7FixeDk5CQ3md/Q0PCzyye9evVKapuX+Ph4uY+5dHV1P/lGc+DAATx//hw1a9bE3bt3pXJPT0+sX78eM2bMgJqaGnx8fGBsbIyNGzeiYcOGAN5/JF21alWUK1cOAHD37l0IITB27FiMHTs21/PFxcXJ/WPg4OCQo83bt28xbdo0rFq1Ck+ePJGbj5eUlCT9/PDhQ7i5ueXY/+MpIdnJW/Y8648ZGRnlWv6x7OcwJSUF27dvx4YNG+Se6+fPnyMxMRFLly7F0qVLcz1GXFwcAKB69erQ09PDsWPH4O3tjWPHjmHixImwsrLCokWL8O7dOykJzp6rC7x/fidOnIgNGzZIx8r2Yd9k+7h/s/9Yli1bVq68ePHictM3gPcrmbRs2RLlypVDpUqV4OPjg65du6Jy5cqf7CdDQ0PptZqXj1/LVapUQYUKFbBx40YEBAQAeP/6KlasmPS8KdK/2XJ7feVHly5d0LVrV0RHR2PHjh2YOXPmFx1HFT5+7sqUKQM1NbVc58Rnu3PnDq5cuYLixYvnWv9xv30pZT1HWVlZWLBgAX777TdERUXJzSX93JSyvDx8+BA2NjY53j+dnJyk+g+VKlUqxzFMTU2RkJAgbc+cORP+/v6wtbWFq6srfH190a1bN5QuXfqTseT3PS6vWLJ/V7NjUeT3+lNatmyJoKAgpKWl4ezZs5g6dSrevHmT42/a9evXMWbMGBw6dCjHYEx2/HnFlL3cJ9HXYPJbwGrWrCmN3n6Ki4sLvLy88qx3cnLCpUuXEB0dneubLvB+XigAODs753mcNm3ayH0xzd/f/5OL6meP7uY1P/PIkSPw9PSEtrY2WrVqhe3bt+O3335DbGwsTpw4galTp0pts784M3z48Dy/GfxxYprbqNyAAQOwatUqDB48GG5ubjA2NoZMJkOnTp2+6Ms52fv8+eefuS5Jlt9vFH/4HLZq1Qpv3rxB7969Ua9ePdja2krn+emnn3LMXc2WnThqamqiVq1aOHr0KO7evYuYmBjUr18flpaWSE9Px+nTp3Hs2DFUqFBBLmHp0KEDTp48ieDgYFStWhUGBgbIysqCj49Prn2j6Kjnh9zd3XHv3j38/fffOHDgAJYvX4558+YhJCQkz7Vwgfev5YsXLyI1NTXX+dfA+9eypqam3B/Gjh074tdff8WLFy9gaGiInTt3onPnztLzo0j/ZvvS62/RogW0tbXh7++P1NTUPH8/AOT5pZzMzMwcn2qoQn6+FJSVlYVGjRrluSpJ9j+wX0tZz9HUqVMxduxY9OzZE5MnT4aZmRnU1NQwePDgb7Z8WV7P3YeJaocOHVC/fn1s374dBw4cwKxZszBjxgxs27YNTZo0yfPYir7H5ScWZShZsqT0Hufr64tixYohKCgInp6eaNOmDYD3X5z08PCAkZERJk2ahDJlykBHRwcXLlzAyJEjubwcfRNMfv8jmjVrhvXr12Pt2rUYM2ZMjvrk5GT8/fffqFChwie/7DZnzhy5kYnsNYRz8/r1a/z999/o2LGj3BJe2QYOHIjQ0FB4enoCeJ+crFmzBuHh4bh58yaEEHLLrmX/N6+pqfnJRP9ztmzZAn9/f8yZM0cqe/fundwqCMD7tSE/HK3O9nFZ9keXFhYWXxXXx6ZPn47t27fj119/RUhICIoXLw5DQ0NkZmbm6zz169fHjBkzcPDgQRQrVgwVKlSATCZDxYoVcezYMRw7dgzNmjWT2ickJCA8PBwTJ07EuHHjpHJFpiVkf3R/584dudGX58+fy71uspmZmaFHjx7o0aMHUlJS4O7ujgkTJnwy+W3WrBkiIyOxefNmuWWTsj148ADHjh2Dl5eXXOLTsWNHTJw4EVu3boWlpSWSk5PRqVMnqV7R/v0aurq6aNWqFf766y80adIExYoVy7Otqalpjtcm8H7k63MjXF/ybfY7d+7IjZbevXsXWVlZn/zSZpkyZZCSkqLUfsstdmU9R1u2bIGnpydWrFghV56YmCj3XCjSf3Z2djh48GCOaTnZH9N/6bQWa2tr/Pzzz/j5558RFxeH6tWr49dff/1k8pvf97j8UvT3Or/69u2LefPmYcyYMWjdujVkMhkiIiLw8uVLbNu2De7u7lLbj1c4+jCmDz91S09PR1RUFKpUqfLFcRFxzu9/RLt27eDs7Izp06fj3LlzcnVZWVno378/EhISMH78+E8ex9XVFV5eXtLjU6PE27dvx+vXrxEYGIh27drleDRr1gxbt25FamoqgPdz4MzMzLBx40Zs3LgRNWvWlPsjbGFhgQYNGuCPP/7As2fPcpzv46V58qKurp5jRGPRokU5ltHx9vZGZGSk3N2d4uPj5eYqZ7czMjLC1KlTkZ6e/sVxfaxMmTJo27YtVq9ejZiYGKirq0vrwH649Fte56lfvz5SU1Mxf/581KtXT/pDXr9+ffz55594+vSp3Hzf7NGfj/tGkW/qe3l5QVNTE4sWLZI7Tm7H+Hj5NAMDAzg6Okqvh7z07dsXFhYWCA4OznEXw3fv3qFHjx4QQsgl8MD7EWMXFxfp9WVtbS33x1XR/v1aw4cPx/jx4/OcwpOtTJkyOHXqlNx0o127duHRo0efPUf2usuKJD3Zy6NlW7RoEQB8Mtnq0KEDIiMjsX///hx1iYmJyMjIyPf5s+nr6+eIW1nPUW7vAZs3b87xnQFF+s/X1xeZmZlYvHixXPm8efMgk8k+2X+5yczMzDFFwcLCAjY2Np/9Hcnve1x+KfJ7rQgNDQ0MGzYMN2/exN9//w0g9/ehtLQ0/Pbbb3L71qhRA8WLF0dISIjc78bq1au/OMknysaR3/8ILS0tbNmyBQ0bNkS9evXk7vC2bt06XLhwAcOGDZMbCftaoaGhMDc3z3NpoBYtWmDZsmXYvXs32rRpA01NTbRp0wYbNmzA69evMXv27Bz7LFmyBPXq1YOLiwt69+6N0qVLIzY2FpGRkXj8+LHcOp15adasGf78808YGxvD2dkZkZGROHjwYI65fiNGjMBff/2FRo0aYcCAAdJSZ6VKlUJ8fLyUTBoZGeH3339H165dUb16dXTq1AnFixdHdHQ0du/ejbp16+b4g5hfwcHB2LRpE+bPn4/p06dj+vTpOHz4MGrVqoXevXvD2dkZ8fHxuHDhAg4ePIj4+HhpXzc3N2hoaOD27dvSclHA++kG2beY/jD5NTIygru7O2bOnIn09HSUKFECBw4c+Oya0h8qXrw4hg8fjmnTpqFZs2bw9fXFxYsXsXfv3hyjm87OzmjQoAFcXV1hZmaGc+fOScs6fYq5uTm2bNmCpk2bonr16jnu8Hb37l0sWLAg19ddx44dMW7cOOjo6CAgICDHXENF+vdrValSJV+jU7169cKWLVvg4+ODDh064N69e/jrr79yLA+VG1dXVwDvP2Xx9vaGurr6Z3/Ho6Ki0KJFC/j4+CAyMhJ//fUXunTp8slYg4ODsXPnTjRr1gzdu3eHq6srXr9+jatXr2LLli148ODBJ0e384r94MGDmDt3LmxsbODg4IBatWop5Tlq1qwZJk2ahB49eqBOnTq4evUqQkNDc4yklylTBiYmJggJCYGhoSH09fVRq1atXOcRN2/eHJ6envjll1/w4MEDVKlSBQcOHMDff/+NwYMH5+v5+tCrV69QsmRJtGvXDlWqVIGBgQEOHjyIs2fPyo3o5nV9+XmPyy9Ffq8V1b17d4wbNw4zZsxAq1atUKdOHZiamsLf3x8DBw6ETCbDn3/+mSOZ19TUxJQpU9C3b1/8+OOP6NixI6KiorBq1SrO+aWv9y2XlqD/k70ETfZSZXnJXs5n8+bN+TpuXFycGDp0qHB0dBTa2trCxMREeHl5KX15s9jYWKGhoSG6du2aZ5s3b94IPT090bp1a6ksLCxMABAymUw8evQo1/3u3bsnunXrJqysrISmpqYoUaKEaNasmdiyZYvU5lP9l5CQIHr06CGKFSsmDAwMhLe3t7h161auS0pdvHhR1K9fX2hra4uSJUuKadOmiYULFwoAIiYmRq7t4cOHhbe3tzA2NhY6OjqiTJkyonv37uLcuXOf7KvPPYcNGjQQRkZG0rJDsbGxIjAwUNja2gpNTU1hZWUlGjZsKJYuXZpj3x9++EEAEKdPn5bKHj9+LAAIW1vbHO0fP34sWrduLUxMTISxsbFo3769ePr0aZ7LYn24JFe2zMxMMXHiRGFtbS10dXVFgwYNxLVr13L075QpU0TNmjWFiYmJ0NXVFRUqVBC//vqrSEtL+2R/ZYuKihK9e/cWpUqVEpqamqJYsWKiRYsW4tixY3nuc+fOHQFAABDHjx/PtU1++lfR3zsh/m+ps0/Jq1/nzJkjSpQoIbS1tUXdunXFuXPn8rXUWUZGhhgwYIAoXry4kMlkcks75fWc3rhxQ7Rr104YGhoKU1NTERQUJN6+fSsXT26/K69evRKjR48Wjo6OQktLSxQrVkzUqVNHzJ49+7PPaW5Lnd26dUu4u7sLXV3dHMtpfe1z9O7dOzFs2DDpNVq3bl0RGRmZo0+FEOLvv/8Wzs7OQkNDQ65/P17qLLsPhgwZImxsbISmpqYoW7asmDVrltzyYELk/Vr4sF9TU1NFcHCwqFKlijA0NBT6+vqiSpUq4rfffvtkXwqR//e4vN4ns/vu8OHDUll+f6/z8qnX/4QJE+TOd+LECVG7dm2hq6srbGxsxIgRI8T+/ftzxCSEEL/99ptwcHAQ2traokaNGuLo0aP5+t3gUmf0KTIhlDzjneg7N3jwYPzxxx9ISUn5Jl84IvoWJkyYgIkTJ+L58+dfPZpHRPQ945xfKtLevn0rt/3y5Uv8+eefqFevHhNfIiKi/yDO+aUizc3NDQ0aNICTkxNiY2OxYsUKJCcnf/ZLSkRERPR9YvJLRZqvry+2bNmCpUuXQiaToXr16lixYoXcKgFERET038E5v0RERERUZHDOLxEREREVGUx+iYiIiKjI4JxfJcnKysLTp09haGj4RbccJSIiom9PCIFXr17BxsYmx41x6L+Jya+SPH36FLa2tgUdBhEREX2BR48eoWTJkgUdBn0DTH6VxNDQEMD7Xx4jI6MCjoaIiIjyIzk5Gba2ttLfcfrvY/KrJNlTHYyMjJj8EhERfWc4ZbHo4OQWIiIiIioymPwSERERUZHB5JeIiIiIigzO+SUiIiL6jMzMTKSnpxd0GJQHLS2tfC9Vx+SXiIiIKA9CCMTExCAxMbGgQ6FPUFNTg4ODA7S0tD7blskvERERUR6yE18LCwvo6elxVYhCKPtGY8+ePUOpUqU++xwx+SUiIiLKRWZmppT4mpubF3Q49AnFixfH06dPkZGRAU1NzU+25RfeiIiIiHKRPcdXT0+vgCOhz8me7pCZmfnZtkx+iYiIiD6BUx0KP0WeIya/RERERFRkMPklIiIioiKDX3gjIiIiUpD9qN3f9HwPpjfNd9uQkBAEBwcjISEBGhrvU72UlBSYmpqibt26iIiIkNpGRETA09MTd+/eRZkyZZQa8+rVqzF48OBCt0wcR36JiIiI/kM8PT2RkpKCc+fOSWXHjh2DlZUVTp8+jXfv3knlhw8fRqlSpRROfIUQyMjIUFrM3xKTXyIiIqL/kPLly8Pa2jrHCG/Lli3h4OCAU6dOyZV7enoiNTUVAwcOhIWFBXR0dFCvXj2cPXtWrp1MJsPevXvh6uoKbW1tHD9+HJcvX4anpycMDQ1hZGQEV1dXnDt3DhEREejRoweSkpIgk8kgk8kwYcKEb9gLeSvQ5Pfo0aNo3rw5bGxsIJPJsGPHjhxtbt68iRYtWsDY2Bj6+vr44YcfEB0dLdW/e/cOgYGBMDc3h4GBAdq2bYvY2Fi5Y0RHR6Np06bQ09ODhYUFgoODc/y3EhERgerVq0NbWxuOjo5YvXq1Ki6ZiIiISOU8PT1x+PBhafvw4cNo0KABPDw8pPK3b9/i9OnT8PT0xIgRI7B161asWbMGFy5cgKOjI7y9vREfHy933FGjRmH69Om4efMmKleuDD8/P5QsWRJnz57F+fPnMWrUKGhqaqJOnTqYP38+jIyM8OzZMzx79gzDhw//pn2QlwKd8/v69WtUqVIFPXv2RJs2bXLU37t3D/Xq1UNAQAAmTpwIIyMjXL9+HTo6OlKbIUOGYPfu3di8eTOMjY0RFBSENm3a4MSJEwDer/fWtGlTWFlZ4eTJk3j27Bm6desGTU1NTJ06FQAQFRWFpk2bol+/fggNDUV4eDh69eoFa2treHt7f5vO+IyvnVukyFwhotzwNUhE9P3w9PTE4MGDkZGRgbdv3+LixYvw8PBAeno6QkJCAACRkZFITU1FgwYN0Lt3b6xevRpNmjQBACxbtgxhYWFYsWIFgoODpeNOmjQJjRo1krajo6MRHByMChUqAADKli0r1RkbG0Mmk8HKyupbXHK+FWjy26RJE6mTc/PLL7/A19cXM2fOlMo+nJOSlJSEFStWYN26dfjxxx8BAKtWrYKTkxNOnTqF2rVr48CBA7hx4wYOHjwIS0tLVK1aFZMnT8bIkSMxYcIEaGlpISQkBA4ODpgzZw4AwMnJCcePH8e8efMKTfJLRN83/vNABU0ZX9Di6/D70aBBA7x+/Rpnz55FQkICypUrh+LFi8PDwwM9evTAu3fvEBERgdKlSyMpKQnp6emoW7eutL+mpiZq1qyJmzdvyh23Ro0acttDhw5Fr1698Oeff8LLywvt27dX+hfnlK3QrvaQlZWF3bt3Y8SIEfD29sbFixfh4OCA0aNHo1WrVgCA8+fPIz09HV5eXtJ+FSpUQKlSpRAZGYnatWsjMjISLi4usLS0lNp4e3ujf//+uH79OqpVq4bIyEi5Y2S3GTx48Le4VPoGmHgQEVFR4ujoiJIlS+Lw4cNISEiAh4cHAMDGxga2trY4efIkDh8+LA0e5pe+vr7c9oQJE9ClSxfs3r0be/fuxfjx47Fhwwa0bt1aadeibIU2+Y2Li0NKSgqmT5+OKVOmYMaMGdi3bx/atGmDw4cPw8PDAzExMdDS0oKJiYncvpaWloiJiQEAxMTEyCW+2fXZdZ9qk5ycjLdv30JXVzdHfKmpqUhNTZW2k5OTv/qaCzOXNS5ftf9V/6tKioSKKr4Gvx77kKho8fT0REREBBISEuSmLri7u2Pv3r04c+YM+vfvjzJlykBLSwsnTpyAnZ0dgPe3dj579my+BgLLlSuHcuXKYciQIejcuTNWrVqF1q1bQ0tLK1+3G/7WCm3ym5WVBQBo2bIlhgwZAgCoWrUqTp48iZCQEOk/mIIybdo0TJw4sUBjoO8LEw8iKur4PvhteXp6IjAwEOnp6XJ5k4eHB4KCgpCWlgZPT0/o6+ujf//+CA4OhpmZGUqVKoWZM2fizZs3CAgIyPP4b9++RXBwMNq1awcHBwc8fvwYZ8+eRdu2bQEA9vb2SElJQXh4OKpUqQI9PT3o6emp/Lo/p9Amv8WKFYOGhgacnZ3lyrPn4wKAlZUV0tLSkJiYKDf6GxsbK02utrKywpkzZ+SOkb0axIdtPl4hIjY2FkZGRrmO+gLA6NGjMXToUGk7OTkZtra2X3Cl38gE46/b36GUcuL4Xn1t/wHsQ74Gvw5fg1QY8Pf4u+Lp6Ym3b9+iQoUKcp9we3h44NWrV9KSaAAwffp0ZGVloWvXrnj16hVq1KiB/fv3w9TUNM/jq6ur4+XLl+jWrRtiY2NRrFgxtGnTRhocrFOnDvr164eOHTvi5cuXGD9+fKFY7qzQJr9aWlr44YcfcPv2bbnyf//9VxqSd3V1haamJsLDw6X/Mm7fvo3o6Gi4ubkBANzc3PDrr78iLi4OFhYWAICwsDAYGRlJibWbmxv27Nkjd56wsDDpGLnR1taGtra2ci6WiIiIvivfw3dB7O3tIYTIUW5nZ5ejXEdHBwsXLsTChQtzPVaDBg1y7KOlpYX169d/Mobff/8dv//+u4KRq1aBJr8pKSm4e/eutB0VFYVLly5JQ+7BwcHo2LEj3N3d4enpiX379uGff/6RFm02NjZGQEAAhg4dCjMzMxgZGWHAgAFwc3ND7dq1AQCNGzeGs7MzunbtipkzZyImJgZjxoxBYGCglLz269cPixcvxogRI9CzZ08cOnQImzZtwu7d3/bWhURERESkWgWa/J47dw6enp7SdvY0An9/f6xevRqtW7dGSEgIpk2bhoEDB6J8+fLYunUr6tWrJ+0zb948qKmpoW3btkhNTYW3tzd+++03qV5dXR27du1C//794ebmBn19ffj7+2PSpElSGwcHB+zevRtDhgzBggULULJkSSxfvpzLnBERFSJctYWIlKFAk9/chtA/1rNnT/Ts2TPPeh0dHSxZsgRLlizJs42dnV2OaQ25xXLx4sVPB0xERERE37VCO+eXiIioMOFKBUT/DWoFHQARERER0bfC5JeIiIiIigxOeyAioqKBa9QSETjyS0RERERFCJNfIiIiIioymPwSERERUZHBOb9EREREivraOeQKny9Joebdu3fHmjVrpG0zMzP88MMPmDlzJipXrvz14UyYgB07duDSpUtffaxvjSO/RERERP9BPj4+ePbsGZ49e4bw8HBoaGigWbNmBR1WgWPyS0RERPQfpK2tDSsrK1hZWaFq1aoYNWoUHj16hOfPnwMAHj16hA4dOsDExARmZmZo2bIlHjx4IO0fERGBmjVrQl9fHyYmJqhbty4ePnyI1atXY+LEibh8+TJkMhlkMhlWr15dMBf5BZj8EhEREf3HpaSk4K+//oKjoyPMzc2Rnp4Ob29vGBoa4tixYzhx4gQMDAzg4+ODtLQ0ZGRkoFWrVvDw8MCVK1cQGRmJPn36QCaToWPHjhg2bBgqVqwojSx37NixoC8x3zjnl4iIiOg/aNeuXTAwMAAAvH79GtbW1ti1axfU1NSwbt06ZGVlYfny5ZDJZACAVatWwcTEBBEREahRowaSkpLQrFkzlClTBgDg5OQkHdvAwAAaGhqwsrL69hf2lTjyS0RERPQf5OnpiUuXLuHSpUs4c+YMvL290aRJEzx8+BCXL1/G3bt3YWhoCAMDAxgYGMDMzAzv3r3DvXv3YGZmhu7du8Pb2xvNmzfHggUL8OzZs4K+JKXgyC8RERHRf5C+vj4cHR2l7eXLl8PY2BjLli1DSkoKXF1dERoammO/4sWLA3g/Ejxw4EDs27cPGzduxJgxYxAWFobatWt/s2tQBSa/REREREWATCaDmpoa3r59i+rVq2Pjxo2wsLCAkZFRnvtUq1YN1apVw+jRo+Hm5oZ169ahdu3a0NLSQmZm5jeMXnk47YGIiIjoPyg1NRUxMTGIiYnBzZs3MWDAAKSkpKB58+bw8/NDsWLF0LJlSxw7dgxRUVGIiIjAwIED8fjxY0RFRWH06NGIjIzEw4cPceDAAdy5c0ea92tvb4+oqChcunQJL168QGpqagFfbf5x5JeIiIjoP2jfvn2wtrYGABgaGqJChQrYvHkzGjRoAAA4evQoRo4ciTZt2uDVq1coUaIEGjZsCCMjI7x9+xa3bt3CmjVr8PLlS1hbWyMwMBB9+/YFALRt2xbbtm2Dp6cnEhMTsWrVKnTv3r2ArlQxTH6JiIiIFKXgHde+tdWrV3927V0rKyu5u8B9yMjICNu3b89zX21tbWzZsuVrQiwwnPZAREREREUGk18iIiIiKjKY/BIRERFRkcHkl4iIiIiKDCa/RERERFRkMPklIiIioiKDyS8RERERFRlMfomIiIioyGDyS0RERERFBpNfIiIiIioyeHtjIiIiIgW5rHH5pue76n/1i/aLjIxEvXr14OPjg927dys5qu8TR36JiIiI/qNWrFiBAQMG4OjRo3j69KnKziOEQEZGhsqOr0xMfomIiIj+g1JSUrBx40b0798fTZs2xerVqwEAXbp0QceOHeXapqeno1ixYli7di0AICsrC9OmTYODgwN0dXVRpUoVbNmyRWofEREBmUyGvXv3wtXVFdra2jh+/Dju3buHli1bwtLSEgYGBvjhhx9w8OBBuXM9e/YMTZs2ha6uLhwcHLBu3TrY29tj/vz5UpvExET06tULxYsXh5GREX788UdcvnxZKf3C5JeIiIjoP2jTpk2oUKECypcvj59++gkrV66EEAJ+fn74559/kJKSIrXdv38/3rx5g9atWwMApk2bhrVr1yIkJATXr1/HkCFD8NNPP+HIkSNy5xg1ahSmT5+OmzdvonLlykhJSYGvry/Cw8Nx8eJF+Pj4oHnz5oiOjpb26datG54+fYqIiAhs3boVS5cuRVxcnNxx27dvj7i4OOzduxfnz59H9erV0bBhQ8THx391v3DOLxEREdF/0IoVK/DTTz8BAHx8fJCUlIQjR47A29sb+vr62L59O7p27QoAWLduHVq0aAFDQ0OkpqZi6tSpOHjwINzc3AAApUuXxvHjx/HHH3/Aw8NDOsekSZPQqFEjadvMzAxVqlSRtidPnozt27dj586dCAoKwq1bt3Dw4EGcPXsWNWrUAAAsX74cZcuWlfY5fvw4zpw5g7i4OGhrawMAZs+ejR07dmDLli3o06fPV/VLgY78Hj16FM2bN4eNjQ1kMhl27NiRZ9t+/fpBJpPJDYkDQHx8PPz8/GBkZAQTExMEBATI/ScDAFeuXEH9+vWho6MDW1tbzJw5M8fxN2/ejAoVKkBHRwcuLi7Ys2ePMi6RiIiI6Ju7ffs2zpw5g86dOwMANDQ00LFjR6xYsQIaGhro0KEDQkNDAQCvX7/G33//DT8/PwDA3bt38ebNGzRq1AgGBgbSY+3atbh3757cebIT2GwpKSkYPnw4nJycYGJiAgMDA9y8eVMa+b19+zY0NDRQvXp1aR9HR0eYmppK25cvX0ZKSgrMzc3lzh8VFZXj/F+iQEd+X79+jSpVqqBnz55o06ZNnu22b9+OU6dOwcbGJkedn58fnj17hrCwMKSnp6NHjx7o06cP1q1bBwBITk5G48aN4eXlhZCQEFy9ehU9e/aEiYmJ9J/DyZMn0blzZ0ybNg3NmjXDunXr0KpVK1y4cAGVKlVSzcUTERERqciKFSuQkZEhlzsJIaCtrY3FixfDz88PHh4eiIuLQ1hYGHR1deHj4wMA0iDi7t27UaJECbnjZo/EZtPX15fbHj58OMLCwjB79mw4OjpCV1cX7dq1Q1paWr5jT0lJgbW1NSIiInLUmZiY5Ps4eSnQ5LdJkyZo0qTJJ9s8efIEAwYMwP79+9G0aVO5ups3b2Lfvn1yQ+eLFi2Cr68vZs+eDRsbG4SGhiItLQ0rV66ElpYWKlasiEuXLmHu3LlS8rtgwQL4+PggODgYwPsh+rCwMCxevBghISEquHIiIiIi1cjIyMDatWsxZ84cNG7cWK6uVatWWL9+Pfr16wdbW1ts3LgRe/fuRfv27aGpqQkAcHZ2hra2NqKjo+WmOOTHiRMn0L17d2nucEpKCh48eCDVly9fHhkZGbh48SJcXV0BvB9pTkhIkNpUr14dMTEx0NDQgL29/Rf0wKcV6i+8ZWVloWvXrggODkbFihVz1EdGRsLExERuyN3Lywtqamo4ffq01Mbd3R1aWlpSG29vb9y+fVvq6MjISHh5eckd29vbG5GRkXnGlpqaiuTkZLkHERERUUHbtWsXEhISEBAQgEqVKsk92rZtixUrVgB4v+pDSEgIwsLCpCkPAGBoaIjhw4djyJAhWLNmDe7du4cLFy5g0aJFWLNmzSfPXbZsWWzbtg2XLl3C5cuX0aVLF2RlZUn1FSpUgJeXF/r06YMzZ87g4sWL6NOnD3R1dSGTyQC8z+Xc3NzQqlUrHDhwAA8ePMDJkyfxyy+/4Ny5c1/dP4U6+Z0xYwY0NDQwcODAXOtjYmJgYWEhV6ahoQEzMzPExMRIbSwtLeXaZG9/rk12fW6mTZsGY2Nj6WFra6vYxRERERGpwIoVK+Dl5QVjY+McdW3btsW5c+dw5coV+Pn54caNGyhRogTq1q0r127y5MkYO3Yspk2bBicnJ+kmGQ4ODp8899y5c2Fqaoo6deqgefPm8Pb2lpvfCwBr166FpaUl3N3d0bp1a/Tu3RuGhobQ0dEBAMhkMuzZswfu7u7o0aMHypUrh06dOuHhw4c58rUvUWhXezh//jwWLFiACxcuSP8JFCajR4/G0KFDpe3k5GQmwEREREXEl95x7Vv4559/8qyrWbMmhBDS9oc/f0gmk2HQoEEYNGhQrvUNGjTIdV97e3scOnRIriwwMFBu29raWm5hgcePHyMuLg6Ojo5SmaGhIRYuXIiFCxfmeS1fSqHkNysrC0eOHMGxY8fw8OFDvHnzBsWLF0e1atXg5eWl1OTv2LFjiIuLQ6lSpaSyzMxMDBs2DPPnz8eDBw9gZWWVY124jIwMxMfHw8rKCgBgZWWF2NhYuTbZ259rk12fG21t7RyTvomIiIjo0w4dOoSUlBS4uLjg2bNnGDFiBOzt7eHu7v5Nzp+vaQ9v377FlClTYGtrC19fX+zduxeJiYlQV1fH3bt3MX78eDg4OMDX1xenTp1SSmBdu3bFlStXcOnSJelhY2OD4OBg7N+/HwDg5uaGxMREnD9/Xtrv0KFDyMrKQq1ataQ2R48eRXp6utQmLCwM5cuXl5bVcHNzQ3h4uNz5w8LCpLXtiIiIiEg50tPT8b///Q8VK1ZE69atUbx4cUREREhfuFO1fI38litXDm5ubli2bBkaNWqUa3APHz7EunXr0KlTJ/zyyy/o3bv3Z4+bkpKCu3fvSttRUVG4dOkSzMzMUKpUKZibm8u119TUhJWVFcqXLw8A0hyU3r17IyQkBOnp6QgKCkKnTp2kpT26dOmCiRMnIiAgACNHjsS1a9ewYMECzJs3TzruoEGD4OHhgTlz5qBp06bYsGEDzp07h6VLl+ane4iIiIgon7y9veHt7V1g589X8nvgwAE4OTl9so2dnR1Gjx6N4cOHy93C7lPOnTsHT09PaTt7Dq2/v790/+nPCQ0NRVBQEBo2bAg1NTW0bdtWbn6IsbExDhw4gMDAQLi6uqJYsWIYN26c3N1B6tSpg3Xr1mHMmDH43//+h7Jly2LHjh1c45eIiIjoPyZfyW924puRkYGpU6eiZ8+eKFmyZK5tNTU1UaZMmXydPK/J0nn5cJ24bGZmZtINLfJSuXJlHDt27JNt2rdvj/bt2+c7FiIiIioaFMlVqGAo8hwptNSZhoYGZs2ahYyMDIWDIiIiIvqeZE/zfPPmTQFHQp+TfQc5dXX1z7ZVeKmzH3/8EUeOHFHJHTeIiIiICgt1dXWYmJhIK0vp6ekVyuVXi7qsrCw8f/4cenp60ND4fGqrcPLbpEkTjBo1ClevXoWrq2uOezq3aNFC0UMSERERFUrZy55+vLQqFS5qamooVapUvv45UTj5/fnnnwG8v4PHx2QyGTIzMxU9JBEREVGhJJPJYG1tDQsLC7llU6lw0dLSgppa/mbzKpz8fnh/ZiIiIqKiQF1dPV/zSanwU+gLbx979+6dsuIgIiIiIlI5hZPfzMxMTJ48GSVKlICBgQHu378PABg7dixWrFih9ACJiIiIiJRF4eT3119/xerVqzFz5kxoaWlJ5ZUqVcLy5cuVGhwRERERkTIpnPyuXbsWS5cuhZ+fn9zclypVquDWrVtKDY6IiIiISJkUTn6fPHkCR0fHHOVZWVn8FiQRERERFWoKJ7/Ozs653ip4y5YtqFatmlKCIiIiIiJSBYWXOhs3bhz8/f3x5MkTZGVlYdu2bbh9+zbWrl2LXbt2qSJGIiIiIiKlUHjkt2XLlvjnn39w8OBB6OvrY9y4cbh58yb++ecfNGrUSBUxEhEREREphcIjvwBQv359hIWFKTsWIiIiIiKVUnjkt3Tp0nj58mWO8sTERJQuXVopQRERERERqYLCye+DBw+QmZmZozw1NRVPnjxRSlBERERERKqQ72kPO3fulH7ev38/jI2Npe3MzEyEh4fD3t5eqcERERERESlTvpPfVq1aST/7+/vL1WlqasLe3h5z5sxRWmBERERERMqW7+Q3KysLAODg4ICzZ8+iWLFiKguKiIiIiEgVFJ7zO3HiRBgaGuYoT0tLw9q1a5USFBERERGRKiic/Pbo0QNJSUk5yl+9eoUePXooJSgiIiIiIlVQOPkVQkAmk+Uof/z4sdyX4IiIiIiICpt8z/mtVq0aZDIZZDIZGjZsCA2N/9s1MzMTUVFR8PHxUUmQRERERETKoPBqD5cuXYK3tzcMDAykOi0tLdjb26Nt27ZKD5CIiIiISFnynfyOHz8eAGBvb4+OHTtCR0dHZUEREREREamCwnN+/f398e7dOyxfvhyjR49GfHw8AODChQu8wxsRERERFWr5HvnNduXKFXh5ecHY2BgPHjxA7969YWZmhm3btiE6OprLnRERERFRoaXwyO+QIUPQvXt33LlzR27qg6+vL44eParU4IiIiIiIlEnhkd9z585h6dKlOcpLlCiBmJgYpQRFRERERKQKCo/8amtrIzk5OUf5v//+i+LFiyslKCIiIiIiVVA4+W3RogUmTZqE9PR0AIBMJkN0dDRGjhzJpc6IiIiIqFBTOPmdM2cOUlJSYGFhgbdv38LDwwOOjo4wNDTEr7/+qooYiYiIiIiUQuHk19jYGGFhYfjnn3+wcOFCBAUFYc+ePThy5Aj09fUVOtbRo0fRvHlz2NjYQCaTYceOHVJdeno6Ro4cCRcXF+jr68PGxgbdunXD06dP5Y4RHx8PPz8/GBkZwcTEBAEBAUhJSZFrc+XKFdSvXx86OjqwtbXFzJkzc8SyefNmVKhQATo6OnBxccGePXsUuhYiIiIiKvwUTn6z1atXDz///DNGjBgBLy+vLzrG69evUaVKFSxZsiRH3Zs3b3DhwgWMHTsWFy5cwLZt23D79m20aNFCrp2fnx+uX7+OsLAw7Nq1C0ePHkWfPn2k+uTkZDRu3Bh2dnY4f/48Zs2ahQkTJsh9ae/kyZPo3LkzAgICcPHiRbRq1QqtWrXCtWvXvui6iIiIiKhwUni1BwAIDw/HvHnzcPPmTQCAk5MTBg8erHAS3KRJEzRp0iTXuuwR5g8tXrwYNWvWRHR0NEqVKoWbN29i3759OHv2LGrUqAEAWLRoEXx9fTF79mzY2NggNDQUaWlpWLlyJbS0tFCxYkVcunQJc+fOlZLkBQsWwMfHB8HBwQCAyZMnIywsDIsXL0ZISIhC10REREREhZfCI7+//fYbfHx8YGhoiEGDBmHQoEEwMjKCr69vriO4ypSUlASZTAYTExMAQGRkJExMTKTEFwC8vLygpqaG06dPS23c3d2hpaUltfH29sbt27eRkJAgtfk4cff29kZkZKRKr4eIiIiIvi2FR36nTp2KefPmISgoSCobOHAg6tati6lTpyIwMFCpAWZ79+4dRo4cic6dO8PIyAgAEBMTAwsLC7l2GhoaMDMzk9YcjomJgYODg1wbS0tLqc7U1BQxMTFS2YdtPrVucWpqKlJTU6Xt3JZ/IyIiIqLCReGR38TERPj4+OQob9y4MZKSkpQS1MfS09PRoUMHCCHw+++/q+Qcipo2bRqMjY2lh62tbUGHRERERESf8UXr/G7fvj1H+d9//41mzZopJagPZSe+Dx8+RFhYmDTqCwBWVlaIi4uTa5+RkYH4+HhYWVlJbWJjY+XaZG9/rk12fW5Gjx6NpKQk6fHo0aMvv0giIiIi+ibyNe1h4cKF0s/Ozs749ddfERERATc3NwDAqVOncOLECQwbNkypwWUnvnfu3MHhw4dhbm4uV+/m5obExEScP38erq6uAIBDhw4hKysLtWrVktr88ssvSE9Ph6amJgAgLCwM5cuXh6mpqdQmPDwcgwcPlo4dFhYmXV9utLW1oa2trczLJSIiIiIVy1fyO2/ePLltU1NT3LhxAzdu3JDKTExMsHLlSowZMybfJ09JScHdu3el7aioKFy6dAlmZmawtrZGu3btcOHCBezatQuZmZnSHFwzMzNoaWnByckJPj4+6N27N0JCQpCeno6goCB06tQJNjY2AIAuXbpg4sSJCAgIwMiRI3Ht2jUsWLBA7poGDRoEDw8PzJkzB02bNsWGDRtw7tw5ueXQiIiIiOj7l6/kNyoqSiUnP3fuHDw9PaXtoUOHAgD8/f0xYcIE7Ny5EwBQtWpVuf0OHz6MBg0aAABCQ0MRFBSEhg0bQk1NDW3btpUbqTY2NsaBAwcQGBgIV1dXFCtWDOPGjZNbC7hOnTpYt24dxowZg//9738oW7YsduzYgUqVKqnkuomIiIioYHzROr/K0qBBAwgh8qz/VF02MzMzrFu37pNtKleujGPHjn2yTfv27dG+ffvPno+IiIiIvl9ffIc3IiIiIqLvDZNfIiIiIioymPwSERERUZHB5JeIiIiIigyFk999+/bh+PHj0vaSJUtQtWpVdOnSBQkJCUoNjoiIiIhImRROfoODg5GcnAwAuHr1KoYNGwZfX19ERUVJS5URERERERVGCi91FhUVBWdnZwDA1q1b0axZM0ydOhUXLlyAr6+v0gMkIiIiIlIWhUd+tbS08ObNGwDAwYMH0bhxYwDv19vNHhEmIiIiIiqMFB75rVevHoYOHYq6devizJkz2LhxIwDg33//RcmSJZUeIBERERGRsig88rt48WJoaGhgy5Yt+P3331GiRAkAwN69e+Hj46P0AImIiIiIlEXhkd9SpUph165dOcrnzZunlICIiIiIiFQlX8lvcnIyjIyMpJ8/JbsdEREREVFhk6/k19TUFM+ePYOFhQVMTEwgk8lytBFCQCaTITMzU+lBEhEREREpQ76S30OHDsHMzAwAcPjwYZUGRERERESkKvlKfj08PHL9mYiIiIjoe6Lwag9ERERERN8rJr9EREREVGQw+SUiIiKiIkOh5FcIgejoaLx7905V8RARERERqYzCya+joyMePXqkqniIiIiIiFRGoeRXTU0NZcuWxcuXL1UVDxERERGRyig853f69OkIDg7GtWvXVBEPEREREZHK5Gud3w9169YNb968QZUqVaClpQVdXV25+vj4eKUFR0RERESkTAonv/Pnz1dBGEREREREqqdw8uvv76+KOIiIiIiIVO6L1vm9d+8exowZg86dOyMuLg4AsHfvXly/fl2pwRERERERKZPCye+RI0fg4uKC06dPY9u2bUhJSQEAXL58GePHj1d6gEREREREyqJw8jtq1ChMmTIFYWFh0NLSksp//PFHnDp1SqnBEREREREpk8LJ79WrV9G6desc5RYWFnjx4oVSgiIiIiIiUgWFk18TExM8e/YsR/nFixdRokQJpQRFRERERKQKCie/nTp1wsiRIxETEwOZTIasrCycOHECw4cPR7du3VQRIxERERGRUiic/E6dOhUVKlSAra0tUlJS4OzsDHd3d9SpUwdjxoxRRYxEREREREqh8Dq/WlpaWLZsGcaOHYtr164hJSUF1apVQ9myZVURHxERERGR0ig88nv//n0AQKlSpeDr64sOHTp8ceJ79OhRNG/eHDY2NpDJZNixY4dcvRAC48aNg7W1NXR1deHl5YU7d+7ItYmPj4efnx+MjIxgYmKCgIAAafm1bFeuXEH9+vWho6MDW1tbzJw5M0csmzdvRoUKFaCjowMXFxfs2bPni66JiIiIiAovhZNfR0dHlCpVCl27dsWKFStw9+7dLz7569evUaVKFSxZsiTX+pkzZ2LhwoUICQnB6dOnoa+vD29vb7x7905q4+fnh+vXryMsLAy7du3C0aNH0adPH6k+OTkZjRs3hp2dHc6fP49Zs2ZhwoQJWLp0qdTm5MmT6Ny5MwICAnDx4kW0atUKrVq1wrVr17742oiIiIio8FE4+X306BGmTZsGXV1dzJw5E+XKlUPJkiXh5+eH5cuXK3SsJk2aYMqUKbkunSaEwPz58zFmzBi0bNkSlStXxtq1a/H06VNphPjmzZvYt28fli9fjlq1aqFevXpYtGgRNmzYgKdPnwIAQkNDkZaWhpUrV6JixYro1KkTBg4ciLlz50rnWrBgAXx8fBAcHAwnJydMnjwZ1atXx+LFixXtHiIiIiIqxBROfkuUKAE/Pz8sXboUt2/fxu3bt+Hl5YVNmzahb9++SgssKioKMTEx8PLyksqMjY1Rq1YtREZGAgAiIyNhYmKCGjVqSG28vLygpqaG06dPS23c3d3lbsjh7e2N27dvIyEhQWrz4Xmy22SfJzepqalITk6WexARERFR4abwF97evHmD48ePIyIiAhEREbh48SIqVKiAoKAgNGjQQGmBxcTEAAAsLS3lyi0tLaW6mJgYWFhYyNVraGjAzMxMro2Dg0OOY2TXmZqaIiYm5pPnyc20adMwceLEL7gyIiIiIiooCie/JiYmMDU1hZ+fH0aNGoX69evD1NRUFbEVaqNHj8bQoUOl7eTkZNja2hZgRERERET0OQonv76+vjh+/Dg2bNiAmJgYxMTEoEGDBihXrpxSA7OysgIAxMbGwtraWiqPjY1F1apVpTZxcXFy+2VkZCA+Pl7a38rKCrGxsXJtsrc/1ya7Pjfa2trQ1tb+gisjIiIiooKi8JzfHTt24MWLF9i3bx/c3Nxw4MAB1K9fX5oLrCwODg6wsrJCeHi4VJacnIzTp0/Dzc0NAODm5obExEScP39eanPo0CFkZWWhVq1aUpujR48iPT1dahMWFoby5ctLI9Zubm5y58luk30eIiIiIvpvUDj5zebi4oK6devCzc0NP/zwA+Li4rBx40aFjpGSkoJLly7h0qVLAN5/ye3SpUuIjo6GTCbD4MGDMWXKFOzcuRNXr15Ft27dYGNjg1atWgEAnJyc4OPjg969e+PMmTM4ceIEgoKC0KlTJ9jY2AAAunTpAi0tLQQEBOD69evYuHEjFixYIDdlYdCgQdi3bx/mzJmDW7duYcKECTh37hyCgoK+tHuIiIiIqBBSeNrD3LlzERERgePHj+PVq1eoUqUK3N3d0adPH9SvX1+hY507dw6enp7SdnZC6u/vj9WrV2PEiBF4/fo1+vTpg8TERNSrVw/79u2Djo6OtE9oaCiCgoLQsGFDqKmpoW3btli4cKFUb2xsjAMHDiAwMBCurq4oVqwYxo0bJ7cWcJ06dbBu3TqMGTMG//vf/1C2bFns2LEDlSpVUrR7iIiIiKgQUzj5Xb9+PTw8PKRk19jY+ItP3qBBAwgh8qyXyWSYNGkSJk2alGcbMzMzrFu37pPnqVy5Mo4dO/bJNu3bt0f79u0/HTARERERfdcUTn7Pnj2rijiIiIiIiFRO4eQXABITE7FixQrcvHkTAODs7IyAgICvGgUmIiIiIlI1hb/wdu7cOZQpUwbz5s1DfHw84uPjMW/ePJQpUwYXLlxQRYxEREREREqh8MjvkCFD0KJFCyxbtgwaGu93z8jIQK9evTB48GAcPXpU6UESERERESmDwsnvuXPn5BJf4P0thUeMGIEaNWooNTgiIiIiImVSeNqDkZERoqOjc5Q/evQIhoaGSgmKiIiIiEgVFE5+O3bsiICAAGzcuBGPHj3Co0ePsGHDBvTq1QudO3dWRYxEREREREqh8LSH2bNnQyaToVu3bsjIyAAAaGpqon///pg+fbrSAyQiIiIiUhaFk18tLS0sWLAA06ZNw7179wAAZcqUgZaWFuLi4qTbChMRERERFTZftM4vAOjp6cHFxUXavnz5MqpXr47MzEylBEZEREREpGwKz/klIiIiIvpeMfklIiIioiKDyS8RERERFRn5nvN75cqVT9bfvn37q4MhIiIiIlKlfCe/VatWhUwmgxAiR112uUwmU2pwRERERETKlO/kNyoqSpVxEBERERGpXL6TXzs7O1XGQURERESkcvn6wlt0dLRCB33y5MkXBUNEREREpEr5Sn5/+OEH9O3bF2fPns2zTVJSEpYtW4ZKlSph69atSguQiIiIiEhZ8jXt4caNG/j111/RqFEj6OjowNXVFTY2NtDR0UFCQgJu3LiB69evo3r16pg5cyZ8fX1VHTcRERERkcLyNfJrbm6OuXPn4tmzZ1i8eDHKli2LFy9e4M6dOwAAPz8/nD9/HpGRkUx8iYiIiKjQyvcX3gBAV1cX7dq1Q7t27VQVDxERERGRyvAOb0RERERUZDD5JSIiIqIig8kvERERERUZTH6JiIiIqMhg8ktERERERYbCye+aNWuwe/duaXvEiBEwMTFBnTp18PDhQ6UGR0RERESkTAonv1OnToWuri4AIDIyEkuWLMHMmTNRrFgxDBkyROkBEhEREREpi0Lr/ALAo0eP4OjoCADYsWMH2rZtiz59+qBu3bpo0KCBsuMjIiIiIlIahUd+DQwM8PLlSwDAgQMH0KhRIwCAjo4O3r59q9zoiIiIiIiUSOGR30aNGqFXr16oVq0a/v33X+l2xtevX4e9vb2y4yMiIiIiUhqFR36XLFkCNzc3PH/+HFu3boW5uTkA4Pz58+jcubNSg8vMzMTYsWPh4OAAXV1dlClTBpMnT4YQQmojhMC4ceNgbW0NXV1deHl54c6dO3LHiY+Ph5+fH4yMjGBiYoKAgACkpKTItbly5Qrq168PHR0d2NraYubMmUq9FiIiIiIqeAqP/JqYmGDx4sU5yidOnKiUgD40Y8YM/P7771izZg0qVqyIc+fOoUePHjA2NsbAgQMBADNnzsTChQuxZs0aODg4YOzYsfD29saNGzego6MDAPDz88OzZ88QFhaG9PR09OjRA3369MG6desAAMnJyWjcuDG8vLwQEhKCq1evomfPnjAxMUGfPn2Ufl1EREREVDDylfxeuXIl3wesXLnyFwfzsZMnT6Jly5Zo2rQpAMDe3h7r16/HmTNnALwf9Z0/fz7GjBmDli1bAgDWrl0LS0tL7NixA506dcLNmzexb98+nD17FjVq1AAALFq0CL6+vpg9ezZsbGwQGhqKtLQ0rFy5ElpaWqhYsSIuXbqEuXPnMvklIiIi+g/JV/JbtWpVyGQyCCEgk8k+2TYzM1MpgQFAnTp1sHTpUvz7778oV64cLl++jOPHj2Pu3LkAgKioKMTExMDLy0vax9jYGLVq1UJkZCQ6deqEyMhImJiYSIkvAHh5eUFNTQ2nT59G69atERkZCXd3d2hpaUltvL29MWPGDCQkJMDU1FRp10REREREBSdfyW9UVJT088WLFzF8+HAEBwfDzc0NwPv1fufMmaP0ebKjRo1CcnIyKlSoAHV1dWRmZuLXX3+Fn58fACAmJgYAYGlpKbefpaWlVBcTEwMLCwu5eg0NDZiZmcm1cXBwyHGM7Lrckt/U1FSkpqZK28nJyV9zqURERET0DeQr+bWzs5N+bt++PRYuXCit8gC8n+pga2uLsWPHolWrVkoLbtOmTQgNDcW6deukqQiDBw+GjY0N/P39lXaeLzFt2jSVzHMmIiIiItVReLWHq1ev5hglBQAHBwfcuHFDKUFlCw4OxqhRo9CpUye4uLiga9euGDJkCKZNmwYAsLKyAgDExsbK7RcbGyvVWVlZIS4uTq4+IyMD8fHxcm1yO8aH5/jY6NGjkZSUJD0ePXr0lVdLRERERKqmcPLr5OSEadOmIS0tTSpLS0vDtGnT4OTkpNTg3rx5AzU1+RDV1dWRlZUF4H3CbWVlhfDwcKk+OTkZp0+flqZkuLm5ITExEefPn5faHDp0CFlZWahVq5bU5ujRo0hPT5fahIWFoXz58nnO99XW1oaRkZHcg4iIiIgKN4WXOgsJCUHz5s1RsmRJaWWHK1euQCaT4Z9//lFqcM2bN8evv/6KUqVKoWLFirh48SLmzp2Lnj17AgBkMhkGDx6MKVOmoGzZstJSZzY2NtL0CycnJ/j4+KB3794ICQlBeno6goKC0KlTJ9jY2AAAunTpgokTJyIgIAAjR47EtWvXsGDBAsybN0+p10NEREREBUvh5LdmzZq4f/8+QkNDcevWLQBAx44d0aVLF+jr6ys1uEWLFmHs2LH4+eefERcXBxsbG/Tt2xfjxo2T2owYMQKvX79Gnz59kJiYiHr16mHfvn3SGr8AEBoaiqCgIDRs2BBqampo27YtFi5cKNUbGxvjwIEDCAwMhKurK4oVK4Zx48ZxmTMiIiKi/xiFkt/09HRUqFABu3bt+iaJoaGhIebPn4/58+fn2UYmk2HSpEmYNGlSnm3MzMykG1rkpXLlyjh27NiXhkpERERE3wGF5vxqamri3bt3qoqFiIiIiEilFP7CW2BgIGbMmIGMjAxVxENEREREpDIKz/k9e/YswsPDceDAAbi4uOSY57tt2zalBUdEREREpEwKJ78mJiZo27atKmIhIiIiIlIphZPfVatWqSIOIiIiIiKVU3jOLxERERHR90rhkV8A2LJlCzZt2oTo6Gi5O70BwIULF5QSGBERERGRsik88rtw4UL06NEDlpaWuHjxImrWrAlzc3Pcv38fTZo0UUWMRERERERKoXDy+9tvv2Hp0qVYtGgRtLS0MGLECISFhWHgwIFISkpSRYxEREREREqhcPIbHR2NOnXqAAB0dXXx6tUrAEDXrl2xfv165UZHRERERKRECie/VlZWiI+PBwCUKlUKp06dAgBERUVBCKHc6IiIiIiIlEjh5PfHH3/Ezp07AQA9evTAkCFD0KhRI3Ts2BGtW7dWeoBERERERMqi8GoPS5cuRVZWFoD3tzo2NzfHyZMn0aJFC/Tt21fpARIRERERKYvCya+amhrU1P5vwLhTp07o1KmTUoMiIiIiIlIFhZNfd3d3NGjQAB4eHqhbty50dHRUERcRERERkdIpPOe3cePGOHXqFFq2bAkTExPUq1cPY8aMQVhYGN68eaOKGImIiIiIlELhkd8xY8YAADIyMnD27FkcOXIEERERmDlzJtTU1PDu3TulB0lEREREpAxfdHtjALh//z6uXr2Ky5cv48qVKzA0NIS7u7syYyMiIiIiUiqFk98uXbrgyJEjSE1Nhbu7Ozw8PDBq1ChUrlwZMplMFTESERERESmFwsnvhg0bUKxYMfTq1Qs//vgj6tWrBz09PVXERkRERESkVAp/4e3ly5dYvnw50tLSMHr0aBQrVgx16tTB//73Pxw4cEAVMRIRERERKYXCya+pqSlatGiBuXPn4vz587hy5QrKlSuHWbNmoUmTJqqIkYiIiIhIKRSe9vDy5UtphYeIiAjcuHEDJiYmaN68OTw8PFQRIxERERGRUiic/FpYWKBYsWKoX78+evfujQYNGsDFxUUVsRERERERKZXCye+VK1dQsWJFVcRCRERERKRSCs/5rVixIjIyMnDw4EH88ccfePXqFQDg6dOnSElJUXqARERERETKovDI78OHD+Hj44Po6GikpqaiUaNGMDQ0xIwZM5CamoqQkBBVxElERERE9NUUHvkdNGgQatSogYSEBOjq6krlrVu3Rnh4uFKDIyIiIiJSJoVHfo8dO4aTJ09CS0tLrtze3h5PnjxRWmBERERERMqm8MhvVlYWMjMzc5Q/fvwYhoaGSgmKiIiIiEgVFE5+GzdujPnz50vbMpkMKSkpGD9+PHx9fZUZGxERERGRUik87WHOnDnw9vaGs7Mz3r17hy5duuDOnTsoVqwY1q9fr4oYiYiIiIiUQuHkt2TJkrh8+TI2btyIy5cvIyUlBQEBAfDz85P7AhwRERERUWGj8LQHANDQ0ICfnx9mzpyJ3377Db169UJiYiKCgoKUHR+ePHmCn376Cebm5tDV1YWLiwvOnTsn1QshMG7cOFhbW0NXVxdeXl64c+eO3DHi4+Ph5+cHIyMjmJiYICAgIMeaxFeuXEH9+vWho6MDW1tbzJw5U+nXQkREREQFS6Hk9/r161i8eDGWLl2KxMREAMCLFy8wZMgQlC5dGocPH1ZqcAkJCahbty40NTWxd+9e3LhxA3PmzIGpqanUZubMmVi4cCFCQkJw+vRp6Ovrw9vbG+/evZPa+Pn54fr16wgLC8OuXbtw9OhR9OnTR6pPTk5G48aNYWdnh/Pnz2PWrFmYMGECli5dqtTrISIiIqKCle9pDzt37kS7du2QkZEB4H3SuWzZMnTo0AGurq7Yvn07fHx8lBrcjBkzYGtri1WrVkllDg4O0s9CCMyfPx9jxoxBy5YtAQBr166FpaUlduzYgU6dOuHmzZvYt28fzp49ixo1agAAFi1aBF9fX8yePRs2NjYIDQ1FWloaVq5cCS0tLVSsWBGXLl3C3Llz5ZJkIiIiIvq+5Xvkd8qUKQgMDERycjLmzp2L+/fvY+DAgdizZw/27dun9MQXeJ9w16hRA+3bt4eFhQWqVauGZcuWSfVRUVGIiYmBl5eXVGZsbIxatWohMjISABAZGQkTExMp8QUALy8vqKmp4fTp01Ibd3d3ubWLvb29cfv2bSQkJOQaW2pqKpKTk+UeRERERFS45Tv5vX37NgIDA2FgYIABAwZATU0N8+bNww8//KCy4O7fv4/ff/8dZcuWxf79+9G/f38MHDgQa9asAQDExMQAACwtLeX2s7S0lOpiYmJgYWEhV6+hoQEzMzO5Nrkd48NzfGzatGkwNjaWHra2tl95tURERESkavlOfl+9egUjIyMAgLq6OnR1dVG6dGmVBQa8v6FG9erVMXXqVFSrVg19+vRB7969ERISotLz5sfo0aORlJQkPR49elTQIRERERHRZyi01Nn+/fthbGwM4H1iGh4ejmvXrsm1adGihdKCs7a2hrOzs1yZk5MTtm7dCgCwsrICAMTGxsLa2lpqExsbi6pVq0pt4uLi5I6RkZGB+Ph4aX8rKyvExsbKtcnezm7zMW1tbWhra3/hlRERERFRQVAo+fX395fb7tu3r9y2TCbL9dbHX6pu3bq4ffu2XNm///4LOzs7AO+//GZlZYXw8HAp2U1OTsbp06fRv39/AICbmxsSExNx/vx5uLq6AgAOHTqErKws1KpVS2rzyy+/ID09HZqamgCAsLAwlC9fXm5lCSIiIiL6vuV72kNWVtZnH8pMfAFgyJAhOHXqFKZOnYq7d+9i3bp1WLp0KQIDAwG8T7YHDx6MKVOmYOfOnbh69Sq6desGGxsbtGrVCsD7kWIfHx/07t0bZ86cwYkTJxAUFIROnTrBxsYGANClSxdoaWkhICAA169fx8aNG7FgwQIMHTpUqddDRERERAVL4Tu8fUs//PADtm/fjtGjR2PSpElwcHDA/Pnz4efnJ7UZMWIEXr9+jT59+iAxMRH16tXDvn37oKOjI7UJDQ1FUFAQGjZsCDU1NbRt2xYLFy6U6o2NjXHgwAEEBgbC1dUVxYoVw7hx47jMGREREdF/TKFOfgGgWbNmaNasWZ71MpkMkyZNwqRJk/JsY2ZmhnXr1n3yPJUrV8axY8e+OE4iIiIiKvy+6PbGRERERETfIya/RERERFRkMPklIiIioiLji5LfxMRELF++HKNHj0Z8fDwA4MKFC3jy5IlSgyMiIiIiUiaFv/B25coVeHl5wdjYGA8ePEDv3r1hZmaGbdu2ITo6GmvXrlVFnEREREREX03hkd+hQ4eie/fuuHPnjtxyYr6+vjh69KhSgyMiIiIiUiaFk9+zZ8/muLMbAJQoUQIxMTFKCYqIiIiISBUUTn61tbWRnJyco/zff/9F8eLFlRIUEREREZEqKJz8tmjRApMmTUJ6ejqA9zeZiI6OxsiRI9G2bVulB0hEREREpCwKJ79z5sxBSkoKLCws8PbtW3h4eMDR0RGGhob49ddfVREjEREREZFSKLzag7GxMcLCwnD8+HFcuXIFKSkpqF69Ory8vFQRHxERERGR0iic/GarV68e6tWrp8xYiIiIiIhUSuHkd+HChbmWy2Qy6OjowNHREe7u7lBXV//q4IiIiIiIlEnh5HfevHl4/vw53rx5A1NTUwBAQkIC9PT0YGBggLi4OJQuXRqHDx+Gra2t0gMmIiIiIvpSCn/hberUqfjhhx9w584dvHz5Ei9fvsS///6LWrVqYcGCBYiOjoaVlRWGDBmiiniJiIiIiL6YwiO/Y8aMwdatW1GmTBmpzNHREbNnz0bbtm1x//59zJw5k8ueEREREVGho/DI77Nnz5CRkZGjPCMjQ7rDm42NDV69evX10RERERERKZHCya+npyf69u2LixcvSmUXL15E//798eOPPwIArl69CgcHB+VFSURERESkBAonvytWrICZmRlcXV2hra0NbW1t1KhRA2ZmZlixYgUAwMDAAHPmzFF6sEREREREX0PhOb9WVlYICwvDrVu38O+//wIAypcvj/Lly0ttPD09lRchEREREZGSfPFNLipUqIAKFSooMxYiIiIiIpX6ouT38ePH2LlzJ6Kjo5GWliZXN3fuXKUERkRERESkbAonv+Hh4WjRogVKly6NW7duoVKlSnjw4AGEEKhevboqYiQiIiIiUgqFv/A2evRoDB8+HFevXoWOjg62bt2KR48ewcPDA+3bt1dFjERERERESqFw8nvz5k1069YNAKChoYG3b9/CwMAAkyZNwowZM5QeIBERERGRsiic/Orr60vzfK2trXHv3j2p7sWLF8qLjIiIiIhIyRSe81u7dm0cP34cTk5O8PX1xbBhw3D16lVs27YNtWvXVkWMRERERERKoXDyO3fuXKSkpAAAJk6ciJSUFGzcuBFly5blSg9EREREVKgplPxmZmbi8ePHqFy5MoD3UyBCQkJUEhgRERERkbIpNOdXXV0djRs3RkJCgqriISIiIiJSGYW/8FapUiXcv39fFbEQEREREamUwsnvlClTMHz4cOzatQvPnj1DcnKy3IOIiIiIqLBSOPn19fXF5cuX0aJFC5QsWRKmpqYwNTWFiYkJTE1NVRGjZPr06ZDJZBg8eLBU9u7dOwQGBsLc3BwGBgZo27YtYmNj5faLjo5G06ZNoaenBwsLCwQHByMjI0OuTUREBKpXrw5tbW04Ojpi9erVKr0WIiIiIvr2FF7t4fDhw6qI47POnj2LP/74Q/qyXbYhQ4Zg9+7d2Lx5M4yNjREUFIQ2bdrgxIkTAN5/Sa9p06awsrLCyZMn8ezZM3Tr1g2ampqYOnUqACAqKgpNmzZFv379EBoaivDwcPTq1QvW1tbw9vb+5tdKRERERKqhcPLr4eGhijg+KSUlBX5+fli2bBmmTJkilSclJWHFihVYt24dfvzxRwDAqlWr4OTkhFOnTqF27do4cOAAbty4gYMHD8LS0hJVq1bF5MmTMXLkSEyYMAFaWloICQmBg4MD5syZAwBwcnLC8ePHMW/ePCa/RERERP8hCk97AIBjx47hp59+Qp06dfDkyRMAwJ9//onjx48rNbhsgYGBaNq0Kby8vOTKz58/j/T0dLnyChUqoFSpUoiMjAQAREZGwsXFBZaWllIbb29vJCcn4/r161Kbj4/t7e0tHYOIiIiI/hsUTn63bt0Kb29v6Orq4sKFC0hNTQXwfhQ2exqBMm3YsAEXLlzAtGnTctTFxMRAS0sLJiYmcuWWlpaIiYmR2nyY+GbXZ9d9qk1ycjLevn2ba1ypqan8sh8RERHRd+aLVnsICQnBsmXLoKmpKZXXrVsXFy5cUGpwjx49wqBBgxAaGgodHR2lHvtrTZs2DcbGxtLD1ta2oEMiIiIios9QOPm9ffs23N3dc5QbGxsjMTFRGTFJzp8/j7i4OFSvXh0aGhrQ0NDAkSNHsHDhQmhoaMDS0hJpaWk5zhsbGwsrKysAgJWVVY7VH7K3P9fGyMgIurq6ucY2evRoJCUlSY9Hjx4p45KJiIiISIUUTn6trKxw9+7dHOXHjx9H6dKllRJUtoYNG+Lq1au4dOmS9KhRowb8/PyknzU1NREeHi7tc/v2bURHR8PNzQ0A4ObmhqtXryIuLk5qExYWBiMjIzg7O0ttPjxGdpvsY+RGW1sbRkZGcg8iIiIiKtwUXu2hd+/eGDRoEFauXAmZTIanT58iMjISw4cPx9ixY5UanKGhISpVqiRXpq+vD3Nzc6k8ICAAQ4cOhZmZGYyMjDBgwAC4ubmhdu3aAIDGjRvD2dkZXbt2xcyZMxETE4MxY8YgMDAQ2traAIB+/fph8eLFGDFiBHr27IlDhw5h06ZN2L17t1Kvh4iIiIgKlsLJ76hRo5CVlYWGDRvizZs3cHd3h7a2NoYPH44BAwaoIsZPmjdvHtTU1NC2bVukpqbC29sbv/32m1Svrq6OXbt2oX///nBzc4O+vj78/f0xadIkqY2DgwN2796NIUOGYMGCBShZsiSWL1/OZc6IiIiI/mMUTn5lMhl++eUXBAcH4+7du0hJSYGzszMMDAxUEV8OERERcts6OjpYsmQJlixZkuc+dnZ22LNnzyeP26BBA1y8eFEZIRIRERFRIaXwnN+//voLb968gZaWFpydnVGzZs1vlvgSEREREX0NhZPfIUOGwMLCAl26dMGePXuQmZmpiriIiIiIiJRO4eT32bNn2LBhA2QyGTp06ABra2sEBgbi5MmTqoiPiIiIiEhpFE5+NTQ00KxZM4SGhiIuLg7z5s3DgwcP4OnpiTJlyqgiRiIiIiIipVD4C28f0tPTg7e3NxISEvDw4UPcvHlTWXERERERESmdwiO/APDmzRuEhobC19cXJUqUwPz589G6dWtcv35d2fERERERESmNwiO/nTp1wq5du6Cnp4cOHTpg7Nixn7wTGhERERFRYaFw8quuro5NmzbB29sb6urqcnXXrl3LcUc2IiIiIqLCQuHkNzQ0VG771atXWL9+PZYvX47z589z6TMiIiIiKrS+aM4vABw9ehT+/v6wtrbG7Nmz8eOPP+LUqVPKjI2IiIiISKkUGvmNiYnB6tWrsWLFCiQnJ6NDhw5ITU3Fjh074OzsrKoYiYiIiIiUIt8jv82bN0f58uVx5coVzJ8/H0+fPsWiRYtUGRsRERERkVLle+R37969GDhwIPr374+yZcuqMiYiIiIiIpXI98jv8ePH8erVK7i6uqJWrVpYvHgxXrx4ocrYiIiIiIiUKt/Jb+3atbFs2TI8e/YMffv2xYYNG2BjY4OsrCyEhYXh1atXqoyTiIiIiOirKbzag76+Pnr27Injx4/j6tWrGDZsGKZPnw4LCwu0aNFCFTESERERESnFFy91BgDly5fHzJkz8fjxY6xfv15ZMRERERERqcRXJb/Z1NXV0apVK+zcuVMZhyMiIiIiUgmlJL9ERERERN8DJr9EREREVGQw+SUiIiKiIoPJLxEREREVGUx+iYiIiKjIYPJLREREREUGk18iIiIiKjKY/BIRERFRkcHkl4iIiIiKDCa/RERERFRkMPklIiIioiKDyS8RERERFRlMfomIiIioyGDyS0RERERFBpNfIiIiIioyCn3yO23aNPzwww8wNDSEhYUFWrVqhdu3b8u1effuHQIDA2Fubg4DAwO0bdsWsbGxcm2io6PRtGlT6OnpwcLCAsHBwcjIyJBrExERgerVq0NbWxuOjo5YvXq1qi+PiIiIiL6hQp/8HjlyBIGBgTh16hTCwsKQnp6Oxo0b4/Xr11KbIUOG4J9//sHmzZtx5MgRPH36FG3atJHqMzMz0bRpU6SlpeHkyZNYs2YNVq9ejXHjxkltoqKi0LRpU3h6euLSpUsYPHgwevXqhf3793/T6yUiIiIi1dEo6AA+Z9++fXLbq1evhoWFBc6fPw93d3ckJSVhxYoVWLduHX788UcAwKpVq+Dk5IRTp06hdu3aOHDgAG7cuIGDBw/C0tISVatWxeTJkzFy5EhMmDABWlpaCAkJgYODA+bMmQMAcHJywvHjxzFv3jx4e3t/8+smIiIiIuUr9CO/H0tKSgIAmJmZAQDOnz+P9PR0eHl5SW0qVKiAUqVKITIyEgAQGRkJFxcXWFpaSm28vb2RnJyM69evS20+PEZ2m+xjfCw1NRXJyclyDyIiIiIq3L6r5DcrKwuDBw9G3bp1UalSJQBATEwMtLS0YGJiItfW0tISMTExUpsPE9/s+uy6T7VJTk7G27dvc8Qybdo0GBsbSw9bW1ulXCMRERERqc53lfwGBgbi2rVr2LBhQ0GHgtGjRyMpKUl6PHr0qKBDIiIiIqLPKPRzfrMFBQVh165dOHr0KEqWLCmVW1lZIS0tDYmJiXKjv7GxsbCyspLanDlzRu542atBfNjm4xUiYmNjYWRkBF1d3RzxaGtrQ1tbWynXRkRERETfRqEf+RVCICgoCNu3b8ehQ4fg4OAgV+/q6gpNTU2Eh4dLZbdv30Z0dDTc3NwAAG5ubrh69Sri4uKkNmFhYTAyMoKzs7PU5sNjZLfJPgYRERERff8K/chvYGAg1q1bh7///huGhobSHF1jY2Po6urC2NgYAQEBGDp0KMzMzGBkZIQBAwbAzc0NtWvXBgA0btwYzs7O6Nq1K2bOnImYmBiMGTMGgYGB0uhtv379sHjxYowYMQI9e/bEoUOHsGnTJuzevbvArp2IiIiIlKvQj/z+/vvvSEpKQoMGDWBtbS09Nm7cKLWZN28emjVrhrZt28Ld3R1WVlbYtm2bVK+uro5du3ZBXV0dbm5u+Omnn9CtWzdMmjRJauPg4IDdu3cjLCwMVapUwZw5c7B8+XIuc0ZERET0H1LoR36FEJ9to6OjgyVLlmDJkiV5trGzs8OePXs+eZwGDRrg4sWLCsdIRERERN+HQj/yS0RERESkLEx+iYiIiKjIYPJLREREREUGk18iIiIiKjKY/BIRERFRkcHkl4iIiIiKDCa/RERERFRkMPklIiIioiKDyS8RERERFRlMfomIiIioyGDyS0RERERFBpNfIiIiIioymPwSERERUZHB5JeIiIiIigwmv0RERERUZDD5JSIiIqIig8kvERERERUZTH6JiIiIqMhg8ktERERERQaTXyIiIiIqMpj8EhEREVGRweSXiIiIiIoMJr9EREREVGQw+SUiIiKiIoPJLxEREREVGUx+iYiIiKjIYPJLREREREUGk18iIiIiKjKY/BIRERFRkcHkl4iIiIiKDCa/RERERFRkMPklIiIioiKDye9HlixZAnt7e+jo6KBWrVo4c+ZMQYdERERERErC5PcDGzduxNChQzF+/HhcuHABVapUgbe3N+Li4go6NCIiIiJSAia/H5g7dy569+6NHj16wNnZGSEhIdDT08PKlSsLOjQiIiIiUgImv/9fWloazp8/Dy8vL6lMTU0NXl5eiIyMLMDIiIiIiEhZNAo6gMLixYsXyMzMhKWlpVy5paUlbt26laN9amoqUlNTpe2kpCQAQHJyskriy0p981X7J8vEV+2f+Tbz686von7Jr4LuP4B9yNcgX4Nfq6D7sKj3H/Df7MPsYwrx9b9j9H1g8vuFpk2bhokTJ+Yot7W1LYBoPs/4q49w8+vO3//rIyhIyomeffh12H9fj334dYp2/wH/7T589eoVjI2//+eIPo/J7/9XrFgxqKurIzY2Vq48NjYWVlZWOdqPHj0aQ4cOlbazsrIQHx8Pc3NzyGQylcerTMnJybC1tcWjR49gZGRU0OF8d9h/X499+PXYh1+H/ff1vtc+FELg1atXsLGxKehQ6Bth8vv/aWlpwdXVFeHh4WjVqhWA9wlteHg4goKCcrTX1taGtra2XJmJick3iFR1jIyMvqs3rMKG/ff12Idfj334ddh/X+977EOO+BYtTH4/MHToUPj7+6NGjRqoWbMm5s+fj9evX6NHjx4FHRoRERERKQGT3w907NgRz58/x7hx4xATE4OqVati3759Ob4ER0RERETfJya/HwkKCsp1msN/mba2NsaPH59jGgflD/vv67EPvx778Ouw/74e+5C+FzLBtT2IiIiIqIjgTS6IiIiIqMhg8ktERERERQaTXyIiIiIqMpj8EhFRkTZhwgRUrVr1P3MeIvo0Jr/fmZiYGAwYMAClS5eGtrY2bG1t0bx5c4SHhxdYTO/evUP37t3h4uICDQ0N6SYhhVVh7MOIiAi0bNkS1tbW0NfXR9WqVREaGlpg8XxKYey/27dvw9PTE5aWltDR0UHp0qUxZswYpKenF1hMn1IY+/BDd+/ehaGhYaG6cc+jR4/Qs2dP2NjYQEtLC3Z2dhg0aBBevnyp0HFkMhl27NghVzZ8+PBC0/e56d69e67vqxEREZDJZEhMTPzmMRF9z7jU2XfkwYMHqFu3LkxMTDBr1iy4uLggPT0d+/fvR2BgIG7dulUgcWVmZkJXVxcDBw7E1q1bCySG/CqsfXjy5ElUrlwZI0eOhKWlJXbt2oVu3brB2NgYzZo1K5CYclNY+09TUxPdunVD9erVYWJigsuXL6N3797IysrC1KlTCySmvBTWPsyWnp6Ozp07o379+jh58mSBxpLt/v37cHNzQ7ly5bB+/Xo4ODjg+vXrCA4Oxt69e3Hq1CmYmZl98fENDAxgYGCgxIiLjrS0NGhpaX13x6YiTtB3o0mTJqJEiRIiJSUlR11CQoL085w5c0SlSpWEnp6eKFmypOjfv7949eqVVP/gwQPRrFkzYWJiIvT09ISzs7PYvXu3VH/16lXh4+Mj9PX1hYWFhfjpp5/E8+fP8xWjv7+/aNmy5Rdfo6p9D32YzdfXV/To0UPxi1Sh76n/hgwZIurVq6f4RapYYe/DESNGiJ9++kmsWrVKGBsbf9W1KouPj48oWbKkePPmjVz5s2fPhJ6enujXr58QQgg7OzsxadIk0alTJ6GnpydsbGzE4sWLpfZ2dnYCgPSws7MTQggxfvx4UaVKFald9vvYr7/+KiwsLISxsbGYOHGiSE9PF8OHDxempqaiRIkSYuXKlXLxjBgxQpQtW1bo6uoKBwcHMWbMGJGWlibVf3ye/MrrffXw4cMCgHj8+LEwNDQUmzdvlqvfvn270NPTE8nJySIqKkoAEOvXrxdubm5CW1tbVKxYUURERMjt87nXjYeHhwgMDBSDBg0S5ubmokGDBkIIIQCI3377Tfj4+AgdHR3h4OCQI5789s+yZcuEvb29kMlkQggh9u7dK+rWrSuMjY2FmZmZaNq0qbh79660X/a1bdy4UdSrV0/o6OiIGjVqiNu3b4szZ84IV1dXoa+vL3x8fERcXJzC/U//PZz28J2Ij4/Hvn37EBgYCH19/Rz1H348qaamhoULF+L69etYs2YNDh06hBEjRkj1gYGBSE1NxdGjR3H16lXMmDFDGvVITEzEjz/+iGrVquHcuXPYt28fYmNj0aFDB5Vfo6p9b32YlJT0VaNZyvY99d/du3exb98+eHh4fPkFq0Bh78NDhw5h8+bNWLJkiXIuWAni4+Oxf/9+/Pzzz9DV1ZWrs7Kygp+fHzZu3Ajx/5esnzVrFqpUqYKLFy9i1KhRGDRoEMLCwgAAZ8+eBQCsWrUKz549k7Zzc+jQITx9+hRHjx7F3LlzMX78eDRr1gympqY4ffo0+vXrh759++Lx48fSPoaGhli9ejVu3LiBBQsWYNmyZZg3b56yuyQHfX19dOrUCatWrZIrX7VqFdq1awdDQ0OpLDg4GMOGDcPFixfh5uaG5s2bS1NH8vu6WbNmDbS0tHDixAmEhIRI5WPHjkXbtm1x+fJl+Pn5oVOnTrh586ZUn5/+uXv3LrZu3Ypt27bh0qVLAIDXr19j6NChOHfuHMLDw6GmpobWrVsjKytLbt/x48djzJgxuHDhAjQ0NNClSxeMGDECCxYswLFjx3D37l2MGzfuyzua/jsKOvum/Dl9+rQAILZt26bwvps3bxbm5ubStouLi5gwYUKubSdPniwaN24sV/bo0SMBQNy+ffuz5yrMI7/fSx8KIcTGjRuFlpaWuHbtmsKxqsr30H/ZI1oARJ8+fURmZqbCsapSYe7DFy9eCFtbW3HkyBEhhCg0I7+nTp0SAMT27dtzrZ87d64AIGJjY4WdnZ3w8fGRq+/YsaNo0qSJtJ3bsXIb+bWzs5N7/ZQvX17Ur19f2s7IyBD6+vpi/fr1ecY+a9Ys4erqmud58svf31+oq6sLfX19uYeOjo4AIBISEsTp06eFurq6ePr0qRBCiNjYWKGhoSGN7GaPjk6fPl06bnp6uihZsqSYMWOGECJ/rxsPDw9RrVq1HDECkEbgs9WqVUv0798/z+vKrX80NTU/Ozr7/PlzAUBcvXpV7tqWL18utVm/fr0AIMLDw6WyadOmifLly3/y2FQ0cOT3OyEUuBHfwYMH0bBhQ5QoUQKGhobo2rUrXr58iTdv3gAABg4ciClTpqBu3boYP348rly5Iu17+fJlHD58WJoDZ2BggAoVKgAA7t27p9yL+sa+lz48fPgwevTogWXLlqFixYoKXqXqfA/9t3HjRly4cAHr1q3D7t27MXv27C+4UtUpzH3Yu3dvdOnSBe7u7l9xhaqT375zc3PLsf3h6GN+VaxYEWpq//cn0tLSEi4uLtK2uro6zM3NERcXJ5Vt3LgRdevWhZWVFQwMDDBmzBhER0crfO7ceHp64tKlS3KP5cuXS/U1a9ZExYoVsWbNGgDAX3/9BTs7uxzP54f9o6GhgRo1akj9k9/Xjaura64xfq7v89M/dnZ2KF68uFzZnTt30LlzZ5QuXRpGRkawt7cHgBz7Vq5cWfrZ0tISAOSeM0tLS7nni4ouJr/fibJly0Imk332yzAPHjxAs2bNULlyZWzduhXnz5+XPsJMS0sDAPTq1Qv3799H165dcfXqVdSoUQOLFi0CAKSkpKB58+Y53mTv3LlTaP8o5tf30IdHjhxB8+bNMW/ePHTr1k0JV60830P/2drawtnZGZ07d8b06dMxYcIEZGZmKuHqlaMw9+GhQ4cwe/ZsaGhoQENDAwEBAUhKSoKGhgZWrlypxF5QjKOjI2QyWZ4J7M2bN2FqapojYfpampqactsymSzXsuyP3iMjI+Hn5wdfX1/s2rULFy9exC+//CI9X19LX18fjo6Oco8SJUrItenVqxdWr14N4P2Uhx49ekAmk+X7HPl93eQ2Zedz8ts/uR27efPmiI+Px7Jly3D69GmcPn0aAHLs++Hzk33dH5d9PFWCiiYmv98JMzMzeHt7Y8mSJXj9+nWO+uylbs6fP4+srCzMmTMHtWvXRrly5fD06dMc7W1tbdGvXz9s27YNw4YNw7JlywAA1atXx/Xr12Fvb5/jjfZL3vAKk8LehxEREWjatClmzJiBPn36KOeilaiw99/HsrKykJ6eXqj+2BXmPoyMjJRLeCZNmgRDQ0NcunQJrVu3Vl4nKMjc3ByNGjXCb7/9hrdv38rVxcTEIDQ0FB07dpSSnVOnTsm1OXXqFJycnKRtTU1NlfxDdPLkSdjZ2eGXX35BjRo1ULZsWTx8+FDp5/mUn376CQ8fPsTChQtx48YN+Pv752jzYf9kZGTg/PnzUv987e/ep/r+S/vn5cuXuH37NsaMGYOGDRvCyckJCQkJn92P6FOY/H5HlixZgszMTNSsWRNbt27FnTt3cPPmTSxcuFD6uMnR0RHp6elYtGgR7t+/jz///FPuCwkAMHjwYOzfvx9RUVG4cOECDh8+LL1BBQYGIj4+Hp07d8bZs2dx79497N+/Hz169PjkH4wbN27g0qVLiI+PR1JSkvQHtLAprH14+PBhNG3aFAMHDkTbtm0RExODmJgYxMfHq7ZDFFRY+y80NBSbNm3CzZs3cf/+fWzatAmjR49Gx44dc4zWFbTC2odOTk6oVKmS9ChRogTU1NRQqVIlmJqaqrZTPmPx4sVITU2Ft7c3jh49ikePHmHfvn1o1KgRSpQogV9//VVqe+LECcycORP//vsvlixZgs2bN2PQoEFSvb29PcLDwxETE6PUJKps2bKIjo7Ghg0bcO/ePSxcuBDbt29X2vHzw9TUFG3atEFwcDAaN26MkiVL5mizZMkSbN++Hbdu3UJgYCASEhLQs2dPAF/+/p9t8+bNWLlyJf7991+MHz8eZ86cQVBQEIAv7x9TU1OYm5tj6dKluHv3Lg4dOoShQ4cq2DNEHynQGceksKdPn4rAwEBhZ2cntLS0RIkSJUSLFi3E4cOHpTZz584V1tbWQldXV3h7e4u1a9dKX4oQQoigoCBRpkwZoa2tLYoXLy66du0qXrx4Ie3/77//itatWwsTExOhq6srKlSoIAYPHiyysrLyjOvjJYSyH4VRYexDf3//XPvPw8NDhT3xZQpj/23YsEFUr15dGBgYCH19feHs7CymTp0q3r59q8qu+GKFsQ8/Vli+8JbtwYMHwt/fX1haWgpNTU1ha2srBgwYIHfNdnZ2YuLEiaJ9+/ZCT09PWFlZiQULFsgdZ+fOncLR0VFoaGh8dqmzD3l4eIhBgwbJldnZ2Yl58+ZJ28HBwcLc3FwYGBiIjh07innz5sn1oaqWOvtwibzw8HABQGzatEmubfaXwtatWydq1qwptLS0hLOzszh06JBcu8+9bnLrByHef+FtyZIlolGjRkJbW1vY29uLjRs3yrX50v4JCwsTTk5OQltbW1SuXFlERETIfXEx+9ouXrz4yb4pbK9pKjgyIRT4BgYREVEhZW9vj8GDB2Pw4MEFHUqB+fPPPzFkyBA8ffpU7gYRDx48gIODAy5evKiSWyzLZDJs37690N/hkwjgHd6IiIi+e2/evMGzZ88wffp09O3bl3dGI/oEzvklIiL6zs2cORMVKlSAlZUVRo8eXdDhEBVqnPZAREREREUGR36JiIiIqMhg8ktERERERQaTXyIiIiIqMpj8EhEREVGRweSXiIiIiIoMJr9EVCjJZLJPPiZMmPBVx96xY0e+2h4+fBi+vr4wNzeHnp4enJ2dMWzYMDx58uSLz09ERAWHyS8RFUrPnj2THvPnz4eRkZFc2fDhw1Uewx9//AEvLy9YWVlh69atuHHjBkJCQpCUlIQ5c+ao/PxERKR8TH6JqFCysrKSHsbGxpDJZHJlGzZsgJOTE3R0dFChQgX89ttv0r5paWkICgqCtbU1dHR0YGdnh2nTpgF4fwtcAGjdujVkMpm0/bHHjx9j4MCBGDhwIFauXIkGDRrA3t4e7u7uWL58OcaNGwcAePnyJTp37owSJUpAT08PLi4uWL9+vdyxtmzZAhcXF+jq6sLc3BxeXl54/fq1VL98+fIvuhYiIlIcb29MRN+d0NBQjBs3DosXL0a1atVw8eJF9O7dG/r6+vD398fChQuxc+dObNq0CaVKlcKjR4/w6NEjAMDZs2dhYWGBVatWwcfHB+rq6rmeY/PmzUhLS8OIESNyrTcxMQEAvHv3Dq6urhg5ciSMjIywe/dudO3aFWXKlEHNmjXx7NkzdO7cGTNnzkTr1q3x6tUrHDt2DNn3F/qaayEiIsUx+SWi78748eMxZ84ctGnTBgDg4OCAGzdu4I8//oC/vz+io6NRtmxZ1KtXDzKZDHZ2dtK+xYsXB/A+ebWyssrzHHfu3IGRkRGsra0/GUuJEiXkpmAMGDAA+/fvx6ZNm6TkNyMjA23atJHicHFxUcq1EBGR4pj8EtF35fXr17h37x4CAgLQu3dvqTwjIwPGxsYAgO7du6NRo0YoX748fHx80KxZMzRu3Fih8wghIJPJPtsuMzMTU6dOxaZNm/DkyROkpaUhNTUVenp6AIAqVaqgYcOGcHFxgbe3Nxo3box27drB1NT0m10LERH9Hya/RPRdSUlJAQAsW7YMtWrVkqvLnsJQvXp1REVFYe/evTh48CA6dOgALy8vbNmyJd/nKVeuHJKSkvDs2bNPjv7OmjULCxYswPz58+Hi4gJ9fX0MHjwYaWlpUkxhYWE4efIkDhw4gEWLFuGXX37B6dOnpQRZ1ddCREQfEEREhdyqVauEsbGxtG1jYyMmTZqU7/337dsnAIiXL18KIYTQ1NQUW7Zs+eQ+0dHRQktLSwwePDjX+oSEBCGEEM2aNRM9e/aUyjMzM0XZsmVFy5Ytc90vIyNDlChRQsyZM0cp10JERIrhyC8RfXcmTpyIgQMHwtjYGD4+PkhNTcW5c+eQkJCAoUOHYu7cubC2tka1atWgpqaGzZs3w8rKSvqSmr29PcLDw1G3bl1oa2vD1NQ0xzlsbW0xb948BAUFITk5Gd26dYO9vT0eP36MtWvXwsDAAHPmzEHZsmWxZcsWnDx5Eqamppg7dy5iY2Ph7OwMADh9+jTCw8PRuHFjWFhY4PTp03j+/DmcnJyUci1ERKSggs6+iYg+5+ORXyGECA0NFVWrVhVaWlrC1NRUuLu7i23btgkhhFi6dKmoWrWq0NfXF0ZGRqJhw4biwoUL0r47d+4Ujo6OQkNDQ9jZ2X3y3GFhYcLb21uYmpoKHR0dUaFCBTF8+HDx9OlTIYQQL1++FC1bthQGBgbCwsJCjBkzRnTr1k0a+b1x44bw9vYWxYsXF9ra2qJcuXJi0aJFSrsWIiJSjEyI/7/eDhERERHRfxxvckFERERERQaTXyIiIiIqMpj8EhEREVGRweSXiIiIiIoMJr9EREREVGQw+SUiIiKiIoPJLxEREREVGUx+iYiIiKjIYPJLREREREUGk18iIiIiKjKY/BIRERFRkcHkl4iIiIiKjP8HqxX8BE2U7mIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_results = [result_case1, result_case2, result_case3, result_case4, result_opt, res_hyperparam]\n",
    "\n",
    "# Initialize empty lists to store the averaged values\n",
    "avg_min = []\n",
    "avg_max = []\n",
    "avg_avg = []\n",
    "\n",
    "# Calculate the average for 'min,' 'max,' and 'avg' separately\n",
    "for result in all_results:\n",
    "    avg_min.append(abs(np.mean(result['min'])))\n",
    "    avg_max.append(abs(np.mean(result['max'])))\n",
    "    avg_avg.append(abs(np.mean(result['avg'])))\n",
    "\n",
    "# Create a list of labels for the bars\n",
    "labels = ['Case 1', 'Case 2', 'Case 3', 'Case 4', 'Optimal', 'Hyperparam']\n",
    "\n",
    "# Create a list of x-positions for the bars\n",
    "x = np.arange(len(all_results))\n",
    "\n",
    "# Set the width of the bars\n",
    "width = 0.2\n",
    "\n",
    "# Create the figure and axis objects\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(x - width, avg_min, width, label='Worst')\n",
    "ax.bar(x, avg_max, width, label='Best')\n",
    "ax.bar(x + width, avg_avg, width, label='Average')\n",
    "\n",
    "# Set the labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_xlabel('Test Cases')\n",
    "ax.set_ylabel('Average Rewards (Lower is better)')\n",
    "ax.set_title('PPO - Average Rewards Over Multiple Iterations and Radii')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-RdARHrrA-i"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
