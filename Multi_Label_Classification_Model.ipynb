{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten, LSTM\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Input\n",
    "from keras.layers.merge import Concatenate\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import wget\n",
    "import zipfile\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "import pydotplus\n",
    "import graphviz\n",
    "from io import StringIO\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from IPython.display import display\n",
    "from sklearn import metrics\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers        \n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class initialize_data:\n",
    "    \n",
    "    def __init__(self, file_date, file_path, file_tab, file_values, target_col_name, features):\n",
    "        \"\"\"pass\"\"\"\n",
    "        self.file_date = file_date\n",
    "        self.file_path = file_path\n",
    "        self.file_tab = file_tab\n",
    "        self.file_values = file_values\n",
    "        self.target_col_name = target_col_name\n",
    "        self.features = features\n",
    "\n",
    "class import_data(initialize_data):\n",
    "    \n",
    "    def __init__(self, file_name, initialize_data, pred_for):\n",
    "        super().__init__(initialize_data.file_date, initialize_data.file_path, initialize_data.file_tab, initialize_data.file_values, initialize_data.target_col_name, initialize_data.features)\n",
    "        self.file_name = file_name\n",
    "        self.pred_for = pred_for\n",
    "    \n",
    "    def import_file(self):\n",
    "        \"\"\"pass\"\"\"\n",
    "        df = pd.read_excel(self.file_path + self.file_name + self.file_date + '.xlsx', sheet_name = self.file_tab)\n",
    "        \n",
    "        if self.pred_for.lower() == 'market':\n",
    "            df[self.target_col_name] = self.file_name\n",
    "        \n",
    "        return df\n",
    "    \n",
    "class process_data(initialize_data):\n",
    "    \n",
    "    def __init__(self, target_labels, initialize_data):\n",
    "        super().__init__(initialize_data.file_date, initialize_data.file_path, initialize_data.file_tab, initialize_data.file_values, initialize_data.target_col_name, initialize_data.features)\n",
    "        self.target_labels = target_labels\n",
    "        \n",
    "    def one_hot_label(self, df):\n",
    "        \"\"\"pass\"\"\"\n",
    "        df_dummies = pd.get_dummies(df[self.target_col_name]) # create onehot labels for target values\n",
    "        df = pd.concat([df, df_dummies], axis = 1) # column bind dummies with df\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def column_reduce(self, df):\n",
    "        \"\"\"pass\"\"\"\n",
    "        col_to_keep = self.features + self.target_labels\n",
    "        df_reduced = df[col_to_keep] # keeping relevant features only\n",
    "        \n",
    "        return df_reduced\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"pass\"\"\"\n",
    "        #convert input to string\n",
    "        text = str(text)\n",
    "\n",
    "        # Remove punctuations and numbers\n",
    "        cleaner_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "        # Single character removal\n",
    "        cleaner_text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', cleaner_text)\n",
    "\n",
    "        # Removing multiple spaces\n",
    "        cleaner_text = re.sub(r'\\s+', ' ', cleaner_text)\n",
    "\n",
    "        return cleaner_text\n",
    "\n",
    "    \n",
    "    def update_text(self, df, feature):\n",
    "        \"\"\"pass\"\"\"\n",
    "        X = []\n",
    "        text = list(df[feature])\n",
    "\n",
    "        for t in text:\n",
    "            X.append(process_data.preprocess_text(self, text = t))\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n",
    "class visualize_data(initialize_data):\n",
    "    \n",
    "    def __init__(self, initialize_data):\n",
    "        super().__init__(initialize_data.file_date, initialize_data.file_path, initialize_data.file_tab, initialize_data.file_values, initialize_data.target_col_name, initialize_data.features)\n",
    "    \n",
    "    def plot_data(self, df_labels):\n",
    "        \"\"\"pass\"\"\"\n",
    "        fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "        fig_size[0] = 6\n",
    "        fig_size[1] = 5\n",
    "        plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "        df_labels.sum(axis=0).plot.bar()\n",
    "    \n",
    "class model_data(initialize_data):\n",
    "    \n",
    "    def __init__(self, initialize_data, split_ratio, max_tokens, random_state, output_sequence_length, batch_size, embed_dim, activation_function, activation_function_final, loss_function, optimizer, eval_metric, epoch, embedding_path, model_file_name):\n",
    "        super().__init__(initialize_data.file_date, initialize_data.file_path, initialize_data.file_tab, initialize_data.file_values, initialize_data.target_col_name, initialize_data.features)\n",
    "        self.split_ratio = split_ratio\n",
    "        self.max_tokens = max_tokens\n",
    "        self.random_state = random_state\n",
    "        self.output_sequence_length = output_sequence_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.activation_function = activation_function\n",
    "        self.activation_function_final = activation_function_final\n",
    "        self.loss_function = loss_function\n",
    "        self.optimizer = optimizer\n",
    "        self.eval_metric = eval_metric\n",
    "        self.epoch = epoch\n",
    "        self.embedding_path = embedding_path\n",
    "        self.model_file_name = model_file_name\n",
    "        \n",
    "    def train_val_split(self, X, y):\n",
    "        \"\"\"pass\"\"\"\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=self.split_ratio, random_state=self.random_state)\n",
    "        \n",
    "        return X_train, X_val, y_train, y_val\n",
    "        \n",
    "    def modelling(self, X_train, X_val, y_train, y_val, import_embeddings = \"yes\"):\n",
    "        \"\"\"pass\"\"\"\n",
    "        embeddings_index = {}\n",
    "        \n",
    "        if import_embeddings.lower() == \"yes\":\n",
    "            path_to_glove_file = self.embedding_path\n",
    "            \n",
    "            with open(path_to_glove_file, encoding=\"utf8\") as f:\n",
    "                for line in f:\n",
    "                    word, coefs = line.split(maxsplit=1)\n",
    "                    coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "                    embeddings_index[word] = coefs\n",
    "    \n",
    "        vectorizer = TextVectorization(max_tokens=self.max_tokens, output_sequence_length=self.output_sequence_length)\n",
    "        text_ds = tf.data.Dataset.from_tensor_slices(X_train).batch(self.batch_size)\n",
    "        vectorizer.adapt(text_ds)\n",
    "        \n",
    "        voc = vectorizer.get_vocabulary()\n",
    "        word_index = dict(zip(voc, range(len(voc))))\n",
    "        \n",
    "        num_tokens = len(voc) + 2\n",
    "        embedding_dim = self.embed_dim\n",
    "        hits = 0\n",
    "        misses = 0\n",
    "\n",
    "        # Prepare embedding matrix\n",
    "        embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in embedding index will be all-zeros.\n",
    "                # This includes the representation for \"padding\" and \"OOV\"\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                hits += 1\n",
    "            else:\n",
    "                misses += 1\n",
    "        print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "        embedding_layer = Embedding(\n",
    "            num_tokens,\n",
    "            embedding_dim,\n",
    "            embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        int_sequences_input = tf.keras.Input(shape=(None,), dtype=\"int64\")\n",
    "        embedded_sequences = embedding_layer(int_sequences_input)\n",
    "        x = layers.Conv1D(self.batch_size, 5, activation=self.activation_function)(embedded_sequences)\n",
    "        x = layers.MaxPooling1D(5)(x)\n",
    "        x = layers.Conv1D(self.batch_size, 5, activation=self.activation_function)(x)\n",
    "        x = layers.MaxPooling1D(5)(x)\n",
    "        x = layers.Conv1D(self.batch_size, 5, activation=self.activation_function)(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        x = layers.Dense(self.batch_size, activation=self.activation_function)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "        preds = layers.Dense(len(target_labels), activation=self.activation_function_final)(x)\n",
    "        model = tf.keras.Model(int_sequences_input, preds)\n",
    "        model.summary()\n",
    "\n",
    "        x_train = vectorizer(np.array([[s] for s in X_train])).numpy()\n",
    "        x_val = vectorizer(np.array([[s] for s in X_val])).numpy()\n",
    "\n",
    "        model.compile(\n",
    "            loss=self.loss_function, optimizer=self.optimizer, metrics=[self.eval_metric]\n",
    "        )\n",
    "        model.fit(x_train, y_train, batch_size=self.batch_size, epochs=self.epoch, validation_data=(x_val, y_val))\n",
    "\n",
    "        # Export the Model\n",
    "        string_input = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "        x = vectorizer(string_input)\n",
    "        preds = model(x)\n",
    "        end_to_end_model = tf.keras.Model(string_input, preds)\n",
    "        \n",
    "        return end_to_end_model, model, vectorizer\n",
    "\n",
    "        \n",
    "    def save_model(self, model):\n",
    "        \"\"\"pass\"\"\"\n",
    "        model.save(self.model_file_name + datetime.datetime.now().strftime(\"%Y_%m_%d\") + '.h5') # save model\n",
    "\n",
    "\n",
    "    def load_saved_model(self, model_file_date):\n",
    "        \"\"\"pass\"\"\"\n",
    "        new_model = tf.keras.models.load_model(self.model_file_name + model_file_date + '.h5') # load model\n",
    "\n",
    "        # set up loaded model\n",
    "        string_input = tf.keras.Input(shape=(1,), dtype=\"string\")\n",
    "        x = vectorizer(string_input)\n",
    "        preds = new_model(x)\n",
    "        end_to_end_model_new = tf.keras.Model(string_input, preds)\n",
    "        \n",
    "        return end_to_end_model_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_exe_1():\n",
    "    \"\"\"pass\"\"\"\n",
    "    # execute initializer\n",
    "    init1 = initialize_data(file_date, file_path, file_tab, file_values, target_col_name, features)\n",
    "    \n",
    "    # import files\n",
    "    x = pd.DataFrame()\n",
    "\n",
    "    for file in file_name:\n",
    "        x = x.append(import_data(file_name = file, initialize_data = init1, pred_for=pred_for).import_file())\n",
    "        \n",
    "    pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "    # preprocess the data\n",
    "\n",
    "    # extract unique target values\n",
    "    target_labels = list(x[target_col_name].dropna().drop_duplicates())\n",
    "\n",
    "    # add target labels\n",
    "    process1 = process_data(target_labels = target_labels, initialize_data = init1)\n",
    "\n",
    "    # run one hot encoder on target labels\n",
    "    x2 = process1.one_hot_label(df = x)\n",
    "\n",
    "    # reduce to filtered columns\n",
    "    x3 = process1.column_reduce(df = x2)\n",
    "\n",
    "    # apply regex to clean up features\n",
    "    cleaned_desc = process1.update_text(df = x3, feature = 'feature_var')\n",
    "    cleaned_manuf_desc = process1.update_text(df = x3, feature = 'feature_val2')\n",
    "\n",
    "    x3.loc[:, 'feature_var'] = cleaned_desc # replace with cleaned product desc\n",
    "    x3.loc[:, 'feature_val2'] = cleaned_manuf_desc # replace with cleaned manufacturer desc\n",
    "\n",
    "    notnull = x3[\"feature_var\"] != \"\"\n",
    "    x4 = x3[notnull] # remove null records\n",
    "    \n",
    "    # initialize visualization\n",
    "    visual1 = visualize_data(initialize_data = init1)\n",
    "    \n",
    "    # visualize data\n",
    "    visual1.plot_data(df_labels = x4[target_labels])\n",
    "    \n",
    "    return init1, target_labels, process1, x4, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_exe_2():\n",
    "    \"\"\"pass\"\"\"\n",
    "    # execute model\n",
    "    model1 = model_data(initialize_data=init1, \n",
    "                        split_ratio=split_ratio, \n",
    "                        max_tokens=max_tokens, \n",
    "                        random_state=random_state, \n",
    "                        output_sequence_length=output_sequence_length, \n",
    "                        batch_size=batch_size, \n",
    "                        embed_dim=embed_dim, \n",
    "                        activation_function=activation_function, \n",
    "                        activation_function_final=activation_function_final, \n",
    "                        loss_function=loss_function, \n",
    "                        optimizer=optimizer, \n",
    "                        eval_metric=eval_metric, \n",
    "                        epoch=epoch,\n",
    "                        embedding_path=embedding_path,\n",
    "                        model_file_name=model_file_name)\n",
    "    \n",
    "    # train validation split\n",
    "    X_train, X_val, y_train, y_val = model1.train_val_split(X = x4['feature_var'], y = x4[target_labels])\n",
    "\n",
    "    print(np.shape(X_train))\n",
    "    print(np.shape(X_val))\n",
    "    print(np.shape(y_train))\n",
    "    print(np.shape(y_val))\n",
    "    \n",
    "    # run model\n",
    "    final_model, model, vectorizer = model1.modelling(X_train=X_train, X_val=X_val, y_train=y_train, y_val=y_val, import_embeddings = \"yes\")\n",
    "    \n",
    "    return model1, final_model, model, vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
