{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run import_libraries.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run connect_snowflake.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "import os\n",
    "import h5py\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = 'experts_pubmed' #'bert_multi_cased_L-12_H-768_A-12' #'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class process_data:\n",
    "    \n",
    "    def __init__(self, topword_limit, feature_filter):\n",
    "        self.topword_limit = topword_limit\n",
    "        self.feature_filter = feature_filter\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the payer and insurance descriptions\n",
    "        \n",
    "        Inputs:\n",
    "        \n",
    "            text(str): a sentence of strings\n",
    "        \n",
    "        Outputs:\n",
    "        \n",
    "            cleaner_text(str): a cleaned sentence of strings\n",
    "        \n",
    "        Example:\n",
    "        \n",
    "            TRICARE/CHAMPUS ---> tricare champus\n",
    "        \n",
    "        \"\"\"\n",
    "        # Convert input to string\n",
    "        text = str(text).lower()\n",
    "\n",
    "        # Remove unwanted texts\n",
    "        cleaner_text = re.sub(r\"payer_desc_is_null\", ' ', text)\n",
    "        cleaner_text = re.sub(r\"ins_plan_desc_is_null\", ' ', cleaner_text)\n",
    "        cleaner_text = re.sub(r\"not provided\", ' ', cleaner_text)\n",
    "                \n",
    "        # Remove words that start or end with numbers without punctuations\n",
    "        cleaner_text = re.sub('\\w*\\d\\w*', ' ', cleaner_text)\n",
    "        \n",
    "        # Remove punctuations and numbers\n",
    "        cleaner_text = re.sub('[^a-zA-Z]', ' ', cleaner_text)\n",
    "\n",
    "        # Single character removal\n",
    "        cleaner_text = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', cleaner_text)\n",
    "\n",
    "        # Removing multiple spaces\n",
    "        cleaner_text = re.sub(r'\\s+', ' ', cleaner_text)\n",
    "\n",
    "        # Remove single character\n",
    "        cleaner_text = re.sub('(\\b[A-Za-z] \\b|\\b [A-Za-z]\\b)', '', cleaner_text)\n",
    "\n",
    "        # Remove stop words\n",
    "        pattern = re.compile(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*')\n",
    "        cleaner_text = pattern.sub(' ', cleaner_text)\n",
    "\n",
    "        return cleaner_text\n",
    "\n",
    "    def top_words_dict(self, df):\n",
    "        \"\"\"\"\"\"\n",
    "        top_dict = {}\n",
    "        for c in df.columns:\n",
    "            top = df[c].sort_values(ascending=False).head(self.topword_limit)\n",
    "            top_dict[c]= list(zip(top.index, top.values))\n",
    "\n",
    "        return top_dict\n",
    "\n",
    "    def print_top_words(self, df):\n",
    "        \"\"\"\"\"\"\n",
    "        for attribute_val, top_words in df.items():\n",
    "            print(attribute_val)\n",
    "            print(', '.join([word for word, count in top_words[0:self.topword_limit]]) + '\\n')\n",
    "\n",
    "    def freq_matrix_NA(self, df):\n",
    "        \"\"\"\"\"\"\n",
    "        cv = CountVectorizer(stop_words='english')\n",
    "        data_cv = cv.fit_transform(df[self.feature_filter])\n",
    "        freq_matr = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()).reset_index(drop=True).join(df[self.target_filter].reset_index(drop=True)).groupby([self.target_filter]).sum()\n",
    "       \n",
    "        return freq_matr\n",
    "    \n",
    "    def freq_matrix(self, df, group_by):\n",
    "        \"\"\"\"\"\"\n",
    "        df['WORD_FREQ'] = df[self.feature_filter].str.lower().str.split(' ')\n",
    "        df = df.explode('WORD_FREQ')\n",
    "        df['WORD'] = df['WORD_FREQ']\n",
    "        df = df.groupby([group_by,'WORD'], as_index=False)['WORD_FREQ'].count()\n",
    "        df = df.loc[df['WORD'] != '', :]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def update_text(self, df):\n",
    "        \"\"\"\"\"\"\n",
    "        lambda_clean = lambda x: process_data.preprocess_text(self, text = x)  \n",
    "        df_cleaned = df[self.feature_filter].apply(lambda_clean)\n",
    "        \n",
    "        return df_cleaned\n",
    "\n",
    "        \n",
    "class visualize_data:\n",
    "    \n",
    "    def __init__(self, initialize_data):\n",
    "        super().__init__(initialize_data.file_date, initialize_data.file_path, initialize_data.file_tab, initialize_data.file_values, initialize_data.target_col_name, initialize_data.features)\n",
    "    \n",
    "    def plot_data(self, df_labels):\n",
    "        \"\"\"\"\"\"\n",
    "        fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "        fig_size[0] = 6\n",
    "        fig_size[1] = 5\n",
    "        plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "        df_labels.sum(axis=0).plot.bar()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    Return BERT embeddings for the input description\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        sentences(list): sentences of strings\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "        tf.tensor: Bert embeddings\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "        >> bert_embeddings(['hello', 'world'])\n",
    "\n",
    "        <tf.Tensor: shape=(2, 512), dtype=float32, numpy=\n",
    "        array([[ 0.3973088 ,  0.92228746,  0.18193923, ...,  0.40281808, -0.51239616, -0.67238265],\n",
    "               [ 0.9519767 ,  0.6982696 , -0.09281926, ..., -0.38502616, -0.28820902, -0.79655826]], dtype=float32)>\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
    "    bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
    "\n",
    "    preprocessed_text = bert_preprocess_model(sentences)\n",
    "    \n",
    "    return bert_model(preprocessed_text)['pooled_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnet_pipeline(drop_out):\n",
    "    \"\"\"\n",
    "    Execute the neuralnet pipeline. Preprocess the inputs, encode the preprocessed inputs, and feed into neural net layers\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        drop_out(float): A probablity value representating the percentage of random sample to drop out\n",
    "    \n",
    "    Outputs:\n",
    "    \n",
    "        Model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessor = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessor(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    nnet = outputs['pooled_output']\n",
    "    nnet = tf.keras.layers.Dropout(drop_out, name=\"dropout\")(nnet)\n",
    "    nnet = tf.keras.layers.Dense(len(labels), activation='softmax', name=\"output\")(nnet)\n",
    "    model = tf.keras.Model(text_input, nnet)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_model(x_train, y_train, x_test, y_test, n_epochs = 3, optimizer = \"adam\", loss_function = \"categorical_crossentropy\", patience = 3, drop_out = 0.1):\n",
    "    \"\"\"\n",
    "    Execute the model and return the trained model\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        x_train(tf): tensorflow train features\n",
    "        y_train(tf): tensorflow train labels\n",
    "        x_test(tf): tensorflow test features\n",
    "        y_test(tf): tensorflow train labels\n",
    "        n_epochs(int): the number of train data points over the batch size\n",
    "        optimizer(str): optimization algorithm\n",
    "        loss_function(str): evaluation metric for the predictions\n",
    "        patience(int): early exit for inference to improve efficiency\n",
    "        drop_out(float): A probablity value representating the percentage of random sample to drop out\n",
    "        \n",
    "    Outputs:\n",
    "    \n",
    "        model_fit: trained model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    model = nnet_pipeline(drop_out)\n",
    "        \n",
    "    earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor = \"val_loss\", \n",
    "                                                          patience = patience,\n",
    "                                                          restore_best_weights = True)\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = loss_function,\n",
    "                  metrics = [tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\")])\n",
    "        \n",
    "    model_fit = model.fit(x_train, \n",
    "                          y_train, \n",
    "                          epochs = n_epochs,\n",
    "                          validation_data = (x_test, y_test),\n",
    "                          callbacks = [earlystop_callback])\n",
    "    \n",
    "    return model_fit, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(description):\n",
    "    \"\"\"\n",
    "    Return the predictions\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        description(list): sentences of strings\n",
    "        \n",
    "    Outputs:\n",
    "    \n",
    "        predictions(list)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return [np.argmax(pred) for pred in model.predict(description)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = import_my_data(limit = 10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(my_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "topword_limit = 50\n",
    "feature_filter = 'description'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = process_data(topword_limit=topword_limit, feature_filter=feature_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process and clean the insurance and payer descriptions\n",
    "my_data_cleaned = pd.DataFrame(text_processor.update_text(df = my_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(my_data['labels'].unique())\n",
    "labels.sort(reverse=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "labels_dict = {}\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels_dict[labels[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {'Commercial': 0,\n",
    "                 'Medicaid': 1,\n",
    "                 'Medicare Advantage': 2,\n",
    "                 'Medicare Traditional': 3,\n",
    "                 'Self-pay': 4,\n",
    "                 'VA': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_cleaned['labels'] = my_data['labels'].map(labels_dict) # encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.keras.utils.to_categorical(my_data_cleaned[\"labels\"].values, num_classes=len(labels))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(my_data_cleaned['description'], y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit, model = execute_model(x_train, y_train, x_test, y_test, n_epochs = 2, optimizer = \"adam\", loss_function = \"categorical_crossentropy\", patience = 3, drop_out = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dict = {v: k for k, v in labels_dict.items()} # flip the labels dictionary\n",
    "\n",
    "y_test_actual = []\n",
    "\n",
    "for i in range(np.shape(y_test)[0]):\n",
    "    y_test_actual.append(list(y_test[i]).index(1))\n",
    "\n",
    "check_pred = pd.DataFrame()\n",
    "check_pred['description'] = list(x_test)\n",
    "check_pred['prediction'] = get_prediction(list(x_test))\n",
    "check_pred['prediction_label'] = check_pred['prediction'].map(pred_dict)\n",
    "check_pred['actual_value'] = y_test_actual\n",
    "check_pred['actual_label'] = check_pred['actual_value'].map(pred_dict)\n",
    "check_pred['prediction_correct'] = check_pred['prediction_label'] == check_pred['actual_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy: \" + str(sum(check_pred['prediction_correct']) / np.shape(check_pred)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on out-of-sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = import_my_data(limit=1000000, data_type='test')['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pred_test = pd.DataFrame()\n",
    "check_pred_test['description'] = list(test_set)\n",
    "check_pred_test['prediction'] = get_prediction(list(test_set))\n",
    "check_pred_test['prediction_label'] = check_pred_test['prediction'].map(pred_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_pred_test.to_excel('C:/Users/' + my_username + '/Downloads/' + 'my_test_set' + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_username = 'user_name'\n",
    "save_filename = 'my_test_data_pred'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input1 = input(\"Save predictions? [y/n]: \")\n",
    "\n",
    "if user_input1 == 'y':\n",
    "    check_pred.to_excel('C:/Users/' + my_username + '/Downloads/' + save_filename + '.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_modelname = 'ky_my_model_v1'\n",
    "import_modelname = 'ky_my_model_v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input2 = input(\"Save model? [y/n]: \")\n",
    "\n",
    "if user_input2 == 'y':\n",
    "    model.save('C:/Users/'+ my_username + '/Downloads/saved_model/' + save_modelname + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input3 = input(\"Load model? [y/n]: \")\n",
    "\n",
    "if user_input3 == 'y':\n",
    "    import_model_path = 'c:/users/' + my_username + '/Downloads/saved_model/' + import_modelname + '.hdf5'\n",
    "    imported_model = load_model(import_model_path, custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
