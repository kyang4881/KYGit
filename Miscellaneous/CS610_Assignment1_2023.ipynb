{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92462a68-a574-447e-91ab-afccbeb1b690",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CS610 Assignment 1 [2023]\n",
    "\n",
    "***Requirement on Submission***\n",
    "1.\tSubmit ONLY your Jupyter Notebook and `requirements.txt`. \n",
    "    - Do not submit the given dataset.\n",
    "    - Do not zip your Jupyter Notebook. \n",
    "    - Your answers to all questions should be in your Jupyter Notebook.\n",
    "\n",
    "\n",
    "2.\tOrganize your code so that it clearly shows your intention and logic. To organize your code, you can either:\n",
    "    - Add comments in your code, or \n",
    "    - Add a mark-up cell to explain what you want to do in the following code cell\n",
    "    - You can use a mix of the above two throughout your Jupyter Notebook. No need to consistently use one way.\n",
    "\n",
    "\n",
    "3.\tMake sure your code can run without any adjustment on grader’s machine. \n",
    "    - Put all necessary packages in the `requirements.txt`\n",
    "    - Adjust your answer before submission if you run your code on Google Colab and use any cloud drive. \n",
    "    - Note: the dataset is supposed to be unzipped and put in the same directory with your Jupyter Notebook.\n",
    "    - Note: you can assume grader’s machine has installed the packages you use in your code.\n",
    "\n",
    "\n",
    "4.\tYou may add additional cells if needed. Ensure your response to each question are in the correct location; if answers are placed under a different question, they may be disregarded. Tabulate your numbers when a table is provided for you to fill in.\n",
    "\n",
    "5.\tLimit your lines of code in a single code cell: no more than 50 lines, including comments.\n",
    "\n",
    "***Failure to meet the requirements above will cause deduction in your grade.***\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This assignment uses two datasets: `yelp.csv` and `census_income.csv` \n",
    "\n",
    "**Description of the data:**\n",
    "\n",
    "`yelp.csv`\n",
    "- Each observation (row) in this dataset is a review of a particular business by a particular user.\n",
    "- The **stars** column is the number of stars (1 through 5) assigned by the reviewer to the business. (Higher stars are better.) In other words, it is the rating of the business by the person who wrote the review.\n",
    "- The **text** column is the text of the review.\n",
    "- **Goal:** Predict the star rating of a review using **only** the review text.\n",
    "\n",
    "`census_income.csv`\n",
    "- The income dataset was extracted from 1994 U.S. Census database. The census is a special, wide-range activity, which takes place once a decade in the entire country. The purpose is to gather information about the general population, in order to present a full and reliable picture of the population in the country - its housing conditions and demographic, social and economic characteristics. The information collected includes data on age, gender, country of origin, marital status, housing conditions, marriage, education, employment, etc.\n",
    "- **Goal:** Predict whether a person makes over 50K a year or not given their demographic variation.\n",
    "\n",
    "Use **random_state = 2023** wherever applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cab93-ca96-4c86-b69b-b1597e8dab5f",
   "metadata": {},
   "source": [
    "## Task 1: Data Exploration (6 points)\n",
    "Explore the yelp dataset and generate the word cloud for reviews with stars == 1 and stars == 5, respectively. Check the [word_cloud](https://github.com/amueller/word_cloud) library. (4 points)\n",
    "\n",
    "An example of word cloud is showed below. \n",
    "\n",
    "![word cloud](https://static.commonlounge.com/fp/600w/FxEgN5woHmXOJOLtm7oGGenV81520493685_kc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1237c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2399066c-cb11-40c3-addd-a19a15791ca9",
   "metadata": {},
   "source": [
    "What do you observe? Does the generated word cloud match your expectation? Elaborate. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff87408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c486e405-65c3-4c16-9b57-70fd33825220",
   "metadata": {},
   "source": [
    "## Task 2: Linear Regression (41 points)\n",
    "\n",
    "In this task, you are to build a linear regression model to predict the stars based solely on the text feature. You need to follow the below steps.\n",
    "1. Split the data into train (80%) and test (20%) using random_state = 2023\n",
    "2. Use [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to generate a vector representation of the text. Use ngram_range = (1, 2), min_df=0.01 and stop_words = 'english'\n",
    "3. Build a linear regression model using the default parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e10f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52d01b2c-fcce-4edd-bb17-4c49c76770ec",
   "metadata": {},
   "source": [
    "Explain how CountVectorizer works based on the documentation from scikit-learn.org. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c24419a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5699e931-3e15-4a4b-8e5a-40ec0e1bbea6",
   "metadata": {},
   "source": [
    "Explain what do ngram_range and min_df mean? (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e780cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ca256d6-4172-4f6d-99a9-a2f5e8740ae9",
   "metadata": {},
   "source": [
    "What is the RMSE score on the train and test dataset, respectively? What do you observe? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5a051c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "695302e5",
   "metadata": {},
   "source": [
    "Based on the same train/test split, \n",
    "- use [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) to generate a vector representation of the text. Use ngram_range = (1, 2), min_df=0.01 and stop_words = 'english'.  (1 point)\n",
    "- Build a linear regression model using default parameters and report RMSE score on the train and test dataset. Do you get better performances using TfidfVectorizer instead of CountVectorizer? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8af7ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01f26a84",
   "metadata": {},
   "source": [
    "Explain how TfidfVectorizer works based on the documentation from scikit-learn.org. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85121f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bfc3c10-494d-44f3-9bc4-466ce3c8c4f2",
   "metadata": {},
   "source": [
    "Based on the TfidfVectorizer, list the 10 most important features based on the magnitude of the coefficients. Explain whether these attributes make sense. (3 points) \n",
    "\n",
    "Hint \n",
    "1. check the documentation of linear regression on how to get the model coefficients. \n",
    "2. It's easier to generate a dataframe with feature names and their corresponding coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e5bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "394fd73a",
   "metadata": {},
   "source": [
    "Another way of converting a text into a vector is by using word embedding (word vector), which is a technique used in NLP that maps words or phrases from a vocabulary into continuous vectors of fixed dimensions in a latent space. You may use a pre-trained word embedding model. An example is showed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514a6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load the pre-trained Word2Vec model\n",
    "model_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    # A simple text preprocessing function that tokenizes words, removes punctuation, and converts to lowercase\n",
    "    words = text.lower().split()\n",
    "    return [word.strip('.,!?()\"') for word in words]\n",
    "\n",
    "\n",
    "def get_word_vector(word, model):\n",
    "    try:\n",
    "        return model[word]\n",
    "    except KeyError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6528dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# This gives you the word embedding of each word in the document\n",
    "embeddings = get_word_vector(preprocess(document), word2vec_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0723a8",
   "metadata": {},
   "source": [
    "Now that you have the word vectors for each word in the document, you need to combine them to create a single vector representing the entire document. There are several strategies for doing this:\n",
    "- a. Average: Compute the element-wise average of the word vectors. \n",
    "- b. Weighted Average: Compute a weighted average of the word vectors, where the weights are based on factors such as term frequency, inverse document frequency, or word importance in the document.\n",
    "- c. Max Pooling: Take the maximum value for each dimension across all word vectors. This approach can help retain some information about the most important words in the document.\n",
    "\n",
    "You task is to:\n",
    "- Implement all these approaches (9 points)\n",
    "- Train a linear regression model for each of these and report the RMSE on both train and test datasets.  (3 points)\n",
    "\n",
    "Note: For approach c, use TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa54005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eda4c15c-bef6-40be-9192-0f5f48a3e9de",
   "metadata": {},
   "source": [
    "Using TfidfVectorizer, build 4 Lasso regression models with alpha in [1e-7, 1e-6, 1e-5, 1e-4] and create a table with the below schema. Think about what is model complexity here. Comment on the observations. (8 points)\n",
    "\n",
    "| alpha  | Training_RMSE | Model Complexity | Test RMSE |\n",
    "|--------|---------------|------------------|-----------|\n",
    "| 1e-7 |               |                  |           |\n",
    "| 1e-6  |               |                  |           |\n",
    "| 1e-5   |               |                  |           |\n",
    "| 1e-4    |               |                  |           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e07ea10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1788126a-1926-4306-9faf-23ae5861b1ce",
   "metadata": {},
   "source": [
    "Buld 5 Ridge regression models with alpha in [0.01, 0.1, 1, 10, 100] and create a table with the below schema. Think about what is model complexity here. Comment on the observations. (5 points)\n",
    "\n",
    "| alpha  | Training_RMSE | Model Complexity | Test RMSE |\n",
    "|--------|---------------|------------------|-----------|\n",
    "| 0.01 |               |                  |           |\n",
    "| 0.1  |               |                  |           |\n",
    "| 1   |               |                  |           |\n",
    "| 10    |               |                  |           |\n",
    "| 100    |               |                  |           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309b7b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea19b9a5-9852-4f77-bbf2-790e34c9c4e7",
   "metadata": {},
   "source": [
    "## Task 3: Naive Bayes Classifier (13 points)\n",
    "\n",
    "\n",
    "In this task, you are to build a Naive Bayes model to predict the highest and lowest stars based on the text feature. You need to follow the below steps.\n",
    "1. Create a new DataFrame that contains only the 5-star and 1-star reviews. \n",
    "2. Split the data into train (80%) and test (20%) using random_state = 2022 \n",
    "3. Use [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)  to generate a vector representation of the text. Use ngram_range = (1, 2) and min_df=0.01\n",
    "4. Use the default parameters for the [MultinomiaNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6df48f-f5d9-4c22-883d-382d92486faf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37a59191-1a06-4ad8-a666-369b2ae468d5",
   "metadata": {},
   "source": [
    "Generate the confusion matrices on both train and test sets. What do you observe? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b25ea6-39c7-4f18-9f7b-904169c4c51b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dc81301-881f-455f-b288-da76ad252317",
   "metadata": {},
   "source": [
    "Calculate which 10 tokens are the most predictive of **5-star reviews**, and which 10 tokens are the most predictive of **1-star reviews**. (6 points)\n",
    "\n",
    "Hint\n",
    "1. Naive Bayes automatically counts the number of times each token appears in each class, as well as the number of observations in each class. You can access these counts via the `feature_count_` and `class_count_` attributes of the Naive Bayes model object.\n",
    "2. Define a metric to indicate the predictiveness of a token for 5-star or 1-star reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1e72c1-00dd-4a76-8eb7-c824744789a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "befe315e-4f62-4d4a-99d1-765e136e61e5",
   "metadata": {},
   "source": [
    "Comment on the top tokens of 5-star and 1-star reviews. Do these tokens make sense? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed0b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4aa52a3-35c9-416e-a7d7-b3dd2ad46169",
   "metadata": {},
   "source": [
    "## Task 4: Logistic Regression (31 points)\n",
    "\n",
    "In this task, you are to build a logistic regression model to predict whether a person makes over 50k a year or not given their demographicvariations. Please take note on the following data dictionary.\n",
    "\n",
    "Categorical Attributes\n",
    "\n",
    "- workclass: (categorical) Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "    - Individual work category\n",
    "- education: (categorical) Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.\n",
    "    - Individual's highest education degree\n",
    "- marital-status: (categorical) Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "    - Individual marital status\n",
    "- occupation: (categorical) Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "    - Individual's occupation\n",
    "- relationship: (categorical) Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "    - Individual's relation in a family\n",
    "- race: (categorical) White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "    - Race of Individual\n",
    "- sex: (categorical) Female, Male.\n",
    "- native-country: (categorical) United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "    - Individual's native country\n",
    "\n",
    "\n",
    "Continuous Attributes\n",
    "\n",
    "- age: continuous.\n",
    "    - Age of an individual\n",
    "- education-num: number of education year, continuous.\n",
    "    - Individual's year of receiving education\n",
    "- capital-gain: continuous.\n",
    "- capital-loss: continuous.\n",
    "- hours-per-week: continuous.\n",
    "- Individual's working hour per week"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c076ea",
   "metadata": {},
   "source": [
    "In order to train any machine learning models, you need to first generate numericl representations for all features. A popular approach to deal with categorical features is called [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html). Explain what is one-hot encoding based on the sklearn documentation. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d884114d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0a0364f",
   "metadata": {},
   "source": [
    "To avoid a very long one-hot vector due to too many unique values of a particular categorical variable, we often combine certain values together. \n",
    "\n",
    "- Use the below dictionaries to map the education, marital-status and native-country columns to new columns. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2198105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "education_mapping = {\n",
    "    'Preschool': 'dropout',\n",
    "    '10th': 'dropout',\n",
    "    '11th': 'dropout',\n",
    "    '12th': 'dropout',\n",
    "    '1st-4th': 'dropout',\n",
    "    '5th-6th': 'dropout',\n",
    "    '7th-8th': 'dropout',\n",
    "    '9th': 'dropout',\n",
    "    'HS-grad': 'HighGrad',\n",
    "    'Some-college': 'CommunityCollege',\n",
    "    'Assoc-acdm': 'CommunityCollege',\n",
    "    'Assoc-voc': 'CommunityCollege',\n",
    "    'Bachelors': 'Bachelors',\n",
    "    'Masters': 'Masters',\n",
    "    'Prof-school': 'Masters',\n",
    "    'Doctorate': 'Doctorate'\n",
    "}\n",
    "\n",
    "\n",
    "marital_status_mapping = {\n",
    "    'Never-married': 'NotMarried',\n",
    "    'Married-AF-spouse': 'Married',\n",
    "    'Married-civ-spouse': 'Married',\n",
    "    'Married-spouse-absent': 'NotMarried',\n",
    "    'Separated': 'Separated',\n",
    "    'Divorced': 'Separated',\n",
    "    'Widowed': 'Widowed'\n",
    "}\n",
    "\n",
    "native_country_mapping = {\n",
    "    'United-States': 'North America',\n",
    "    'Cuba': 'North America',\n",
    "    'Jamaica': 'North America',\n",
    "    'India': 'Asia',\n",
    "    '?': 'Unknown',\n",
    "    'Mexico': 'North America',\n",
    "    'South': 'Unknown',\n",
    "    'Puerto-Rico': 'North America',\n",
    "    'Honduras': 'North America',\n",
    "    'England': 'Europe',\n",
    "    'Canada': 'North America',\n",
    "    'Germany': 'Europe',\n",
    "    'Iran': 'Asia',\n",
    "    'Philippines': 'Asia',\n",
    "    'Italy': 'Europe',\n",
    "    'Poland': 'Europe',\n",
    "    'Columbia': 'South America',\n",
    "    'Cambodia': 'Asia',\n",
    "    'Thailand': 'Asia',\n",
    "    'Ecuador': 'South America',\n",
    "    'Laos': 'Asia',\n",
    "    'Taiwan': 'Asia',\n",
    "    'Haiti': 'North America',\n",
    "    'Portugal': 'Europe',\n",
    "    'Dominican-Republic': 'North America',\n",
    "    'El-Salvador': 'North America',\n",
    "    'France': 'Europe',\n",
    "    'Guatemala': 'North America',\n",
    "    'China': 'Asia',\n",
    "    'Japan': 'Asia',\n",
    "    'Yugoslavia': 'Europe',\n",
    "    'Peru': 'South America',\n",
    "    'Outlying-US(Guam-USVI-etc)': 'North America',\n",
    "    'Scotland': 'Europe',\n",
    "    'Trinadad&Tobago': 'North America',\n",
    "    'Greece': 'Europe',\n",
    "    'Nicaragua': 'North America',\n",
    "    'Vietnam': 'Asia',\n",
    "    'Hong': 'Asia',\n",
    "    'Ireland': 'Europe',\n",
    "    'Hungary': 'Europe',\n",
    "    'Holand-Netherlands': 'Europe'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190e19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b915f5f",
   "metadata": {},
   "source": [
    "Apply the below log transform for capital-gain and capital-loss. (1 point)\n",
    "- lambda $x$: $log_{10}(x + 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0451f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ac0270d",
   "metadata": {},
   "source": [
    "Now apply one-hot encoding to all categorical variables. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620553b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929e26d7",
   "metadata": {},
   "source": [
    "Split the data into train and test dataset (80% and 20%, respectively) and build a logistic regression using default configuration. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06708bea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9943971-a96e-4548-a038-399ceec94c7b",
   "metadata": {},
   "source": [
    "Generate the confusion matrices for train and test set. What do you observe? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105955ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4469984d-bca4-4186-a5fd-780e21d00426",
   "metadata": {},
   "source": [
    "From the test set, take a look at the two false positives with highest predicted probabilities and the two false negatives with the lowest predicted probabilities. Why do you think the model predict them as positive/negative? (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ac4554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3e9fe86-5048-46e2-af66-a3dd3348eff9",
   "metadata": {},
   "source": [
    "Build five L2 regularized logistic regression models with C in [0.01, 0.1, 1, 10, 100] and create a table with the below schema. Think about what is model complexity here. Comment on the observations.(3 points)\n",
    "\n",
    "| C  | Train AUC | Model Complexity | Test AUC |\n",
    "|--------|---------------|------------------|-----------|\n",
    "| 0.01  |               |                  |           |\n",
    "| 0.1   |               |                  |           |\n",
    "| 1    |               |                  |           |\n",
    "| 10    |               |                  |           |\n",
    "| 100   |               |                  |           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd15365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe2bf9c0-e0ba-4703-93c8-9d5fe9f62dc9",
   "metadata": {},
   "source": [
    "Plot the ROC curves for the five models you have built above. Which model has the best performance? (4 points)\n",
    "\n",
    "An example ROC plot is showed below.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/SFA9h.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49534ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b40ebb0-c768-4d97-89e8-1c5a0f571c6d",
   "metadata": {},
   "source": [
    "List the 20 most important features based on the magnitude of the coefficients of the best performing model. Do the top three features make sense? (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f37dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4263135-1409-4c73-9a32-d48c286741c8",
   "metadata": {},
   "source": [
    "Build five L1 regularized logistic regression models with C in [ 1, 10, 100, 1000, 10000] and create a table with the below schema. Think about what is model complexity here. Comment on the observations. (3 points)\n",
    "\n",
    "| C  | Train AUC | Model Complexity | Test AUC |\n",
    "|--------|---------------|------------------|-----------|\n",
    "| 1    |               |                  |           |\n",
    "| 10    |               |                  |           |\n",
    "| 100   |               |                  |           |\n",
    "| 1000 |               |                  |           |\n",
    "| 10000  |               |                  |           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6d2db4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
