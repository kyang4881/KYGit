{"cells":[{"cell_type":"markdown","metadata":{"id":"y_be73OQIIxB"},"source":["# Assignment 1\n","\n","\n","## BPE tokenizer\n","\n","You need to implement a BPE tokenizer including both **1.1)** token learning step and **1.2)** tokenizer to tokenize a sentence.\n","\n","**Grading rules** is based on algorithm correctness and tokenization quality:\n","1. Algorithm correctness: your code should be logically consistent with BPE introduced in our slides.\n","2. Token type ratio: Compare the ratio of unique tokens to the total number of tokens. A lower ratio indicates fewer out-of-vocabulary words and better generalization, but very low ratio may lose much information.\n","3. Subword length distribution: Analyze the distribution of subword lengths. Ideally, the tokenizer should generate a balance of short and long subwords, avoiding too many very short or very long subwords.\n","4. Running time.\n","\n","For example, let's consider a simple text: \"ChatGPT is an AI developed by OpenAI.\"\n","\n","If a tokenizer produces the following tokens:\n","[\"Chat\", \"G\", \"PT\", \"is\", \"an\", \"AI\", \"developed\", \"by\", \"Open\", \"AI\"]\n","\n","There are 10 tokens in total and 9 unique tokens (since \"AI\" appears twice). The token type ratio would be:\n","\n","```\n","Token type ratio = Unique tokens / Total tokens = 9 / 10 = 0.9\n","```\n","We can calculate the subword length distribution as follows:\n","\n","Length 1: 1 subwords (\"G\")\n","Length 2: 5 subwords (\"PT\", \"is\", \"an\", \"AI\", \"by\")\n","Length 3: 0 subwords ()\n","Length 4: 2 subword (\"Chat\", \"Open\")\n","Length 9: 1 subword (\"developed\")\n","\n","```\n","weighted mean/std of length : 3.11/2.28\n","```\n","\n","First, let prepare dataset! We have provided the files and code below.\n","\n","1. you need to create a folder named *assignment1* under your google drive folder */content/drive/MyDrive/SMU_MITB_NLP/*.\n","2. download train and dev files and put them into the assignment folder. File links are given below.\n","3. run each code block in turn to load dataset."]},{"cell_type":"code","source":["#@title codes to mount your google drive folder\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/SMU_MITB_NLP/assignment1/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cbMi6FmUFcgd","executionInfo":{"status":"ok","timestamp":1682257106673,"user_tz":-480,"elapsed":22146,"user":{"displayName":"Dr CAO Yixin _","userId":"02213333189428679281"}},"outputId":"5ddaf32c-8486-4025-ecd6-d3d05766c98c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/SMU_MITB_NLP/assignment1\n"]}]},{"cell_type":"code","source":["#@title import useful packages\n","import gensim\n","import pandas as pd\n","import smart_open\n","\n","import re, collections\n","from collections import defaultdict, Counter\n","\n","import copy, codecs\n","try:\n","    from tqdm import tqdm\n","except ImportError:\n","    def tqdm(iterator, *args, **kwargs):\n","        return iterator"],"metadata":{"id":"62Yx3Hnx5QSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Utility functions for reading and preprocessing data\n","\n","def read_corpus(csv_fname, tokens_only=False):\n","    data = pd.read_csv(csv_fname)\n","    for sent, tag in zip(data['review'], data['sentiment']):\n","        tokens = gensim.utils.simple_preprocess(sent)\n","        if tokens_only:\n","            yield tokens\n","        else:\n","            # For training data, add tags\n","            yield gensim.models.doc2vec.TaggedDocument(tokens, tag)\n","\n","def get_count_vocab(corpus, tokens_only=False):\n","    data = []\n","    if tokens_only:\n","        for doc in corpus:\n","            data.extend(doc)\n","    else:\n","        for taged_doc in corpus:\n","            data.extend(taged_doc.words)\n","    return collections.Counter(data)"],"metadata":{"id":"PlIGoFwRGPbk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### prepare datasets\n","\n","We provide separated files for [training](https://drive.google.com/file/d/1hP_q5jQSCQpF4SNWXli7jMLOnLKGFAEU/view?usp=sharing), [development](https://drive.google.com/file/d/189hUqCY95Kd_RGJm5DwtckvFiEnk4unh/view?usp=sharing), and testing, respectively, under your google drive folders */content/drive/MyDrive/SMU_MITB_NLP/assignment1/sa_train.csv* and */content/drive/MyDrive/SMU_MITB_NLP/assignment1/sa_dev.csv*. Training file is used to learn your model, development file is used to tune your hyperparameters (e.g., vocabulary size, not for model learning!), and testing file is not provided, which we will use for grading.\n","\n","You can use the following code to load datasets:"],"metadata":{"id":"1Tnfd3KoTVJM"}},{"cell_type":"code","source":["training_corpus = list(read_corpus('./sa_train.csv', tokens_only=False))\n","dev_corpus = list(read_corpus('./sa_dev.csv', tokens_only=False))\n","print(training_corpus[:2])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnPJen-S9Za8","executionInfo":{"status":"ok","timestamp":1682257123535,"user_tz":-480,"elapsed":4491,"user":{"displayName":"Dr CAO Yixin _","userId":"02213333189428679281"}},"outputId":"3c5d3038-0721-48ef-d244-7b4b035aaec4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[TaggedDocument(words=['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'br', 'br', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'br', 'br', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'br', 'br', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 'mess', 'around', 'the', 'first', 'episode', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'couldn', 'say', 'was', 'ready', 'for', 'it', 'but', 'as', 'watched', 'more', 'developed', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'], tags='positive'), TaggedDocument(words=['wonderful', 'little', 'production', 'br', 'br', 'the', 'filming', 'technique', 'is', 'very', 'unassuming', 'very', 'old', 'time', 'bbc', 'fashion', 'and', 'gives', 'comforting', 'and', 'sometimes', 'discomforting', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', 'br', 'br', 'the', 'actors', 'are', 'extremely', 'well', 'chosen', 'michael', 'sheen', 'not', 'only', 'has', 'got', 'all', 'the', 'polari', 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', 'you', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'williams', 'diary', 'entries', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'terrificly', 'written', 'and', 'performed', 'piece', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', 'of', 'comedy', 'and', 'his', 'life', 'br', 'br', 'the', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', 'the', 'fantasy', 'of', 'the', 'guard', 'which', 'rather', 'than', 'use', 'the', 'traditional', 'dream', 'techniques', 'remains', 'solid', 'then', 'disappears', 'it', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', 'particularly', 'with', 'the', 'scenes', 'concerning', 'orton', 'and', 'halliwell', 'and', 'the', 'sets', 'particularly', 'of', 'their', 'flat', 'with', 'halliwell', 'murals', 'decorating', 'every', 'surface', 'are', 'terribly', 'well', 'done'], tags='positive')]\n"]}]},{"cell_type":"markdown","source":["training_corpus is a list of [TaggedDocument](https://radimrehurek.com/gensim/models/doc2vec.html#gensim.models.doc2vec.TaggedDocument). You can also access the tokenized document and its corresponding tag as follows:"],"metadata":{"id":"ykayCRmWUfWU"}},{"cell_type":"code","source":["training_corpus[0].words[:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_UAqsOVVb_T","executionInfo":{"status":"ok","timestamp":1682257126227,"user_tz":-480,"elapsed":369,"user":{"displayName":"Dr CAO Yixin _","userId":"02213333189428679281"}},"outputId":"1799ce0c-1783-4b06-b066-5b96fbc9c41e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['one', 'of', 'the', 'other', 'reviewers']"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# this time won't use the tag information though:)\n","training_corpus[0].tags"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"hTWE4vUWV0lF","executionInfo":{"status":"ok","timestamp":1682257128286,"user_tz":-480,"elapsed":7,"user":{"displayName":"Dr CAO Yixin _","userId":"02213333189428679281"}},"outputId":"ab390379-6d06-4037-8b4f-4e87e374301f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'positive'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["get_count_vocab(training_corpus[:2])"],"metadata":{"id":"qcis3Q4tx1bo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","### 1.1 token learning\n","\n","To implement the token learner for BPE tokenizer. We take the following corpus as an example to illustrate the algorithm briefly.\n","\n","Corpus:\n","> “low lower newest newest”, \n","> “low lower newest widest”, \n","> “low newest widest”, \n","> “low newest widest”, \n","> “low newest longer”.\n","\n","Algorithm:\n","1.   Initialize a vocabulary with all characters in the corpus except the end-of-word character, which will be concatenated with a special token (e.g., underscore '_'  here) and then added to the initial vocab, e.g., {'d', 'e', 'g', 'i', 'l', 'n', 'o', 's', 'w’, 't_', 'w_’, 'r_’}.\n","2.   Find the 2 tokens most frequently adjacent to each other in the corpus, e.g., 'e', 's'.\n","3.   Add a new merged token to vocabulary, e.g., 'es'.\n","4.   Replace every adjacent 2 tokens in corpus (step 2) with the merged token in step 3, e.g., ‘e’ ‘s’ -> ‘es’.\n","5.   Repeated steps 2-4 until a predefined vocabulary size is reached.\n","\n","### Task\n","You are to finish a function *learn_bpe* with the following inputs and outputs:\n","\n","Inputs: \n","*   *vocab*: {'token_str': count_int, ...}, the word count dictionary of the text corpus.\n","*   *num_vocab*: integer, the predefined vocabulary size including the initial vocabulary.\n","\n","Outputs:\n","*   *result_vocab*: ['token_str', ...], the list of learned vocabulary\n","*   *result_merge*: ['token1 token2', ...], the list of learned merge operation, where two tokens separated by a whitespace. **Keeping the merge order is important!**\n","\n","A toy example inputs and outputs are given below for your testing.\n","\n","Here are some suggestions.\n","\n","*   You can tune your vocab size *num_vocab* on *dev_corpus*.\n","*   For adjacent 2 tokens with the same frequency, we merge the token following alphabet order. When using the toy example, both ('e', 's') and ('s', 't_') occur 9 times in the corpus, we merge ('e', 's') to 'es' first, as 'es' is in front of 'st_'. (This is not always required in practice.)\n","*   You are only allowed to use some neccessary 3rd party packages, such as re, collection, copy. **No tokenizer is allowed**."],"metadata":{"id":"gSwO-c7BY99l"}},{"cell_type":"code","source":["#@title toy vocabulary\n","\n","toy_vocab = {'newest':6, 'low':5, 'widest':3, 'lower':2, 'longer':1}\n","toy_result_vocab = ['longer_', 'newest_', 'widest_', 'ewest_', 'idest_', 'lower_', 'dest_', 'est_', 'ger_', 'low_', 'er_', 'lon', 'low', 'es', 'ew', 'lo', 'r_', 't_', 'w_',\n","                    'd', 'e', 'g', 'i', 'l', 'n', 'o', 's', 'w']\n","toy_result_merge = ['e s', 'es t_', 'l o', 'e w', 'ew est_', 'n ewest_', 'lo w_', 'd est_', 'e r_', 'i dest_', 'w idest_',\n","                    'lo w', 'low er_', 'g er_', 'lo n', 'lon ger_']"],"metadata":{"id":"ArI4WmoLbUPj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykwzmP5BIIxH"},"outputs":[],"source":["# do not change the code above\n","# write your code here, you may add more helper functions!\n","\n","# do not change the code below\n","\n","def sort_bpe_vocab(vocab):\n","    sorted_vocab = sorted(list(vocab))\n","    sorted_vocab = sorted(list(sorted_vocab), key=len, reverse=True)\n","    return sorted_vocab\n","\n","# total_symbols: true, the number of merges count the initial character; false otherwise. If you don't understand, keep it true.\n","def learn_bpe(vocab, num_vocab, total_symbols=True):\n","    result_vocab = set()\n","    result_merge = []\n","    # do not change the code above\n","    # write your code here\n","\n","    # do not change the code below\n","    return sort_bpe_vocab(result_vocab), result_merge"]},{"cell_type":"code","source":["result_vocab, result_merge = learn_bpe(toy_vocab, 50)\n","print(result_vocab)\n","print(result_merge)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p56q53kKy7yA","executionInfo":{"status":"ok","timestamp":1682257150085,"user_tz":-480,"elapsed":473,"user":{"displayName":"Dr CAO Yixin _","userId":"02213333189428679281"}},"outputId":"7124949f-2ca8-42e1-f601-070c0da78f86"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['longer_', 'newest_', 'widest_', 'ewest_', 'idest_', 'lower_', 'dest_', 'est_', 'ger_', 'low_', 'er_', 'lon', 'low', 'es', 'ew', 'lo', 'r_', 't_', 'w_', 'd', 'e', 'g', 'i', 'l', 'n', 'o', 's', 'w']\n","['e s', 'es t_', 'l o', 'e w', 'ew est_', 'n ewest_', 'lo w_', 'd est_', 'e r_', 'i dest_', 'w idest_', 'lo w', 'low er_', 'g er_', 'lo n', 'lon ger_']\n"]}]},{"cell_type":"markdown","source":["### 1.2 BPE Tokenizer (tokenize a sentence)\n","\n","Congratulations! Now, you have learned a *result_vocab* and *result_merge* as token learner for a BPE tokenizer. This question is to use them to tokenize corpus.\n","\n","The basic algorithm is briefly given below (suppose we use the toy example above and tokenize the sentence ['newest', 'makes', 'me', 'happy']):\n","\n","1.   For each word (e.g., 'newest'), initialize tokenized word including the end-of-word token, e.g., 'n e w e s t_'. Repeated the following steps 2-3 until there is no merge operation found or the word is merged back again (e.g., 'newest_').\n","2.   Get first (most frequent) merge operation in the list of BPE merges that can match any adjacent 2 tokens, e.g., 'e s'.\n","3.   Update the tokenized word, e.g., 'n e w es t_'.\n","4.   Check tokenized results to ensure all tokens of a word is in the learned BPE vocabulary, otherwise, replace it with a 'UNK' (adjacent 'UNK' tokens can be replaced with only one 'UNK').\n","5.   Combine all word token results as the tokenized results of the sentence (e.g., ['newest_', 'UNK', 'e', 'UNK', 'UNK', 'UNK']).\n","\n","###Task\n","You are to finish a function *bpe_tokenize* with the following inputs and outputs:\n","\n","Inputs: \n","*   *words_in_sentences*: a list of tokens ['token1', 'token2', ...], same format as *dev_corpus[0].words*\n","*   *bpe_merges*: *result_merge* as obtained above, ['token1 token2', ...], the list of learned merge operation, where two tokens separated by a whitespace. **The merge order reflect the frequency!**\n","*   *bpe_vocab*: *result_vocab* as obtained above, ['token_str', ...], the list of learned vocabulary.\n","\n","Outputs:\n","*   *result_tokens*: ['token1', ...], the list of tokenized results."],"metadata":{"id":"8CJHjNehK2Gf"}},{"cell_type":"code","source":["#@title bpe tokenizer based on learned vocab\n","# do not change the code above\n","# write your code here, you may add more helper functions!\n","\n","# do not change the code below\n","\n","def bpe_tokenize(words_in_sentence, bpe_merges, bpe_vocab):\n","    result_tokens = []\n","    # do not change the code above\n","    # write your code here\n","\n","    # do not change the code below\n","    return result_tokens\n"],"metadata":{"id":"iIrsBrhFw9IX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title test your tokenizer!\n","print(bpe_tokenize(['newest', 'makes', 'me', 'happy'], toy_result_merge, toy_result_vocab))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdJ2MqRA-c-c","executionInfo":{"status":"ok","timestamp":1682257188084,"user_tz":-480,"elapsed":773,"user":{"displayName":"Dr CAO Yixin _","userId":"02213333189428679281"}},"outputId":"fe1ff1aa-bee4-4238-938c-a43e58f82e69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['newest_', 'UNK', 'e', 'UNK', 'UNK', 'UNK']\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}